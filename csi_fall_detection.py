import pandas as pd
import numpy as np
import glob
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

class CSIFallDetection:
    def __init__(self, window_size=50, stride=1):
        self.window_size = window_size
        self.stride = stride
        self.scaler = StandardScaler()
        self.model = None
        
    def load_csv_files(self, data_directory, recursive=True):
        """
        ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•©ì¹˜ê¸°
        
        Args:
            data_directory: CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
            recursive: Trueë©´ í•˜ìœ„ í´ë”ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
        
        Returns:
            X: íŠ¹ì§• ë°ì´í„° (n_samples, n_features)
            y: ë¼ë²¨ ë°ì´í„° (n_samples,)
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ (n_samples,)
        """
        all_data = []
        
        if recursive:
            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í•˜ìœ„ í´ë”ì˜ CSV íŒŒì¼ ì°¾ê¸°
            csv_files = glob.glob(os.path.join(data_directory, "**", "*.csv"), recursive=True)
            print(f"ì¬ê·€ ê²€ìƒ‰ìœ¼ë¡œ ë°œê²¬ëœ CSV íŒŒì¼ ìˆ˜: {len(csv_files)}")
        else:
            # í˜„ì¬ í´ë”ì˜ CSV íŒŒì¼ë§Œ ì°¾ê¸°
            csv_files = glob.glob(os.path.join(data_directory, "*.csv"))
            print(f"ë°œê²¬ëœ CSV íŒŒì¼ ìˆ˜: {len(csv_files)}")
        
        # ë°œê²¬ëœ íŒŒì¼ ê²½ë¡œë“¤ ì¶œë ¥
        if csv_files:
            print("ë°œê²¬ëœ íŒŒì¼ë“¤:")
            for i, file_path in enumerate(csv_files):
                relative_path = os.path.relpath(file_path, data_directory)
                print(f"  {i+1:2d}. {relative_path}")
                if i >= 9:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                    remaining = len(csv_files) - 10
                    if remaining > 0:
                        print(f"      ... ê·¸ ì™¸ {remaining}ê°œ íŒŒì¼")
                    break
        
        for file_path in csv_files:
            print(f"\në¡œë”© ì¤‘: {os.path.basename(file_path)}")
            
            try:
                # CSV íŒŒì¼ ì½ê¸° (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)
                df = None
                encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1', 'utf-8-sig']
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding)
                        print(f"     âœ… ì¸ì½”ë”© ì„±ê³µ: {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue
                    except Exception as e:
                        print(f"     âš ï¸  {encoding} ì¸ì½”ë”© ì‹¤íŒ¨: {str(e)[:50]}...")
                        continue
                
                if df is None:
                    print(f"  âŒ ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨: {file_path}")
                    continue
                
                # ì»¬ëŸ¼ í™•ì¸
                if 'timestamp' not in df.columns or 'label' not in df.columns:
                    print(f"  âŒ ê²½ê³ : {file_path}ì— í•„ìˆ˜ ì»¬ëŸ¼(timestamp, label)ì´ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"     í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)[:5]}...")
                    continue
                
                # íŠ¹ì§• ì»¬ëŸ¼ ì¶”ì¶œ (feat_0 ~ feat_63 ë˜ëŠ” feat_255)
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                
                if len(feature_cols) == 0:
                    print(f"  âŒ ê²½ê³ : {file_path}ì— íŠ¹ì§• ì»¬ëŸ¼(feat_*)ì´ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"     í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)[:10]}...")
                    continue
                
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                if len(df) == 0:
                    print(f"  âŒ ê²½ê³ : {file_path}ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ë°ì´í„° ì¶”ê°€
                file_data = {
                    'features': df[feature_cols].values,
                    'labels': df['label'].values,
                    'timestamps': pd.to_datetime(df['timestamp']).values,
                    'filename': os.path.basename(file_path)
                }
                all_data.append(file_data)
                
                print(f"  âœ… ë¡œë“œ ì„±ê³µ!")
                print(f"     - ë°ì´í„° í˜•íƒœ: {file_data['features'].shape}")
                print(f"     - íŠ¹ì§• ìˆ˜: {len(feature_cols)}ê°œ")
                print(f"     - ë¼ë²¨ ë¶„í¬: {dict(zip(*np.unique(file_data['labels'], return_counts=True)))}")
                
            except FileNotFoundError:
                print(f"  âŒ ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except pd.errors.EmptyDataError:
                print(f"  âŒ ì˜¤ë¥˜: {file_path}ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {file_path} ë¡œë”© ì‹¤íŒ¨ - {str(e)}")
        
        if not all_data:
            raise ValueError("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! CSV íŒŒì¼ì˜ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        print(f"\nğŸ“Š ë°ì´í„° ê²°í•© ì¤‘...")
        X = np.vstack([data['features'] for data in all_data])
        y = np.hstack([data['labels'] for data in all_data])
        timestamps = np.hstack([data['timestamps'] for data in all_data])
        
        print(f"âœ… ì „ì²´ ë°ì´í„° í†µê³„:")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {X.shape[0]:,}ê°œ")
        print(f"   - íŠ¹ì§• ìˆ˜: {X.shape[1]}ê°œ")
        print(f"   - ì‚¬ìš©ëœ íŒŒì¼ ìˆ˜: {len(all_data)}ê°œ")
        
        # ë¼ë²¨ ë¶„í¬ ìƒì„¸ ì¶œë ¥
        unique_labels, counts = np.unique(y, return_counts=True)
        print(f"   - ë¼ë²¨ ë¶„í¬:")
        for label, count in zip(unique_labels, counts):
            label_name = "ë‚™ìƒ" if label == 1 else "ì •ìƒ"
            percentage = (count / len(y)) * 100
            print(f"     â€¢ {label_name}(label={label}): {count:,}ê°œ ({percentage:.1f}%)")
        
        return X, y, timestamps
    
    def load_csv_files_by_label(self, fall_dir, normal_dir, recursive=True):
        """
        ë¼ë²¨ë³„ë¡œ ë¶„ë¦¬ëœ í´ë”ì—ì„œ CSV íŒŒì¼ ë¡œë“œ
        
        Args:
            fall_dir: ë‚™ìƒ ë°ì´í„° í´ë” ê²½ë¡œ
            normal_dir: ì •ìƒ ë°ì´í„° í´ë” ê²½ë¡œ
            recursive: Trueë©´ í•˜ìœ„ í´ë”ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
        
        Returns:
            X: íŠ¹ì§• ë°ì´í„° (n_samples, n_features)
            y: ë¼ë²¨ ë°ì´í„° (n_samples,)
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ (n_samples,)
        """
        all_data = []
        
        # ë‚™ìƒ ë°ì´í„° ë¡œë“œ (label=1)
        if recursive:
            fall_files = glob.glob(os.path.join(fall_dir, "**", "*.csv"), recursive=True)
        else:
            fall_files = glob.glob(os.path.join(fall_dir, "*.csv"))
        
        print(f"ğŸ”´ ë‚™ìƒ ë°ì´í„° íŒŒì¼ ìˆ˜: {len(fall_files)}")
        
        for file_path in fall_files:
            print(f"ë‚™ìƒ ë°ì´í„° ë¡œë”©: {os.path.basename(file_path)}")
            try:
                df = pd.read_csv(file_path)
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                
                if len(feature_cols) == 0:
                    print(f"  âŒ íŠ¹ì§• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                file_data = {
                    'features': df[feature_cols].values,
                    'labels': np.ones(len(df)),  # ëª¨ë‘ 1ë¡œ ì„¤ì •
                    'timestamps': pd.to_datetime(df['timestamp']).values,
                    'filename': os.path.basename(file_path)
                }
                all_data.append(file_data)
                print(f"  âœ… í˜•íƒœ: {file_data['features'].shape}, ë¼ë²¨: ë‚™ìƒ")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {file_path} ë¡œë”© ì‹¤íŒ¨ - {e}")
        
        # ì •ìƒ ë°ì´í„° ë¡œë“œ (label=0)
        if recursive:
            normal_files = glob.glob(os.path.join(normal_dir, "**", "*.csv"), recursive=True)
        else:
            normal_files = glob.glob(os.path.join(normal_dir, "*.csv"))
            
        print(f"ğŸŸ¢ ì •ìƒ ë°ì´í„° íŒŒì¼ ìˆ˜: {len(normal_files)}")
        
        for file_path in normal_files:
            print(f"ì •ìƒ ë°ì´í„° ë¡œë”©: {os.path.basename(file_path)}")
            try:
                df = pd.read_csv(file_path)
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                
                if len(feature_cols) == 0:
                    print(f"  âŒ íŠ¹ì§• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                file_data = {
                    'features': df[feature_cols].values,
                    'labels': np.zeros(len(df)),  # ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •
                    'timestamps': pd.to_datetime(df['timestamp']).values,
                    'filename': os.path.basename(file_path)
                }
                all_data.append(file_data)
                print(f"  âœ… í˜•íƒœ: {file_data['features'].shape}, ë¼ë²¨: ì •ìƒ")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {file_path} ë¡œë”© ì‹¤íŒ¨ - {e}")
        
        if not all_data:
            raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        X = np.vstack([data['features'] for data in all_data])
        y = np.hstack([data['labels'] for data in all_data])
        timestamps = np.hstack([data['timestamps'] for data in all_data])
        
        print(f"\nâœ… ì „ì²´ ë°ì´í„° í†µê³„:")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {X.shape[0]:,}ê°œ")
        print(f"   - íŠ¹ì§• ìˆ˜: {X.shape[1]}ê°œ")
        print(f"   - ë¼ë²¨ ë¶„í¬: ì •ìƒ={np.sum(y==0):,}ê°œ, ë‚™ìƒ={np.sum(y==1):,}ê°œ")
        
        return X, y, timestamps
    
    def create_sequences(self, X, y):
        """
        ì‹œê³„ì—´ ë°ì´í„°ë¥¼ LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
        
        Args:
            X: íŠ¹ì§• ë°ì´í„° (n_samples, n_features)
            y: ë¼ë²¨ ë°ì´í„° (n_samples,)
        
        Returns:
            X_seq: ì‹œí€€ìŠ¤ ë°ì´í„° (n_sequences, window_size, n_features)
            y_seq: ì‹œí€€ìŠ¤ ë¼ë²¨ (n_sequences,)
        """
        X_sequences = []
        y_sequences = []
        
        print(f"ğŸ”„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©:")
        print(f"   - Window Size: {self.window_size}")
        print(f"   - Stride: {self.stride}")
        print(f"   - ì›ë³¸ ë°ì´í„°: {X.shape}")
        
        for i in range(0, len(X) - self.window_size + 1, self.stride):
            # ìœˆë„ìš° ì¶”ì¶œ
            window_X = X[i:i + self.window_size]
            window_y = y[i:i + self.window_size]
            
            # ìœˆë„ìš° ë‚´ ë¼ë²¨ ê²°ì • (ë‹¤ìˆ˜ê²° ë˜ëŠ” ìµœëŒ€ê°’)
            # ë‚™ìƒ ê°ì§€ì—ì„œëŠ” ë³´í†µ í•˜ë‚˜ë¼ë„ ë‚™ìƒì´ë©´ ë‚™ìƒìœ¼ë¡œ ë¶„ë¥˜
            sequence_label = 1 if np.any(window_y == 1) else 0
            
            X_sequences.append(window_X)
            y_sequences.append(sequence_label)
        
        X_seq = np.array(X_sequences)
        y_seq = np.array(y_sequences)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
        print(f"   - ìƒì„±ëœ ì‹œí€€ìŠ¤ ìˆ˜: {X_seq.shape[0]:,}ê°œ")
        print(f"   - ì‹œí€€ìŠ¤ í˜•íƒœ: {X_seq.shape}")
        
        # ì‹œí€€ìŠ¤ ë¼ë²¨ ë¶„í¬
        unique_labels, counts = np.unique(y_seq, return_counts=True)
        print(f"   - ì‹œí€€ìŠ¤ ë¼ë²¨ ë¶„í¬:")
        for label, count in zip(unique_labels, counts):
            label_name = "ë‚™ìƒ" if label == 1 else "ì •ìƒ"
            percentage = (count / len(y_seq)) * 100
            print(f"     â€¢ {label_name}: {count:,}ê°œ ({percentage:.1f}%)")
        
        return X_seq, y_seq
    
    def preprocess_data(self, X, feature_selection=True):
        """ë°ì´í„° ì „ì²˜ë¦¬ (ì •ê·œí™” + íŠ¹ì§• ì„ íƒ)"""
        print(f"ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ì›ë³¸ í˜•íƒœ ì €ì¥
        original_shape = X.shape
        print(f"   - ì›ë³¸ í˜•íƒœ: {original_shape}")
        
        # 2Dë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
        X_2d = X.reshape(-1, X.shape[-1])
        
        # íŠ¹ì§• ì„ íƒ (256ê°œê°€ ë„ˆë¬´ ë§ì„ ê²½ìš°)
        if feature_selection and X.shape[-1] > 128:
            print(f"ğŸ“Š íŠ¹ì§• ì„ íƒ ìˆ˜í–‰: {X.shape[-1]}ê°œ â†’ 128ê°œ")
            
            from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
            
            # 1. ë¶„ì‚°ì´ ë‚®ì€ íŠ¹ì§• ì œê±°
            print("   - ë¶„ì‚° ê¸°ë°˜ íŠ¹ì§• ì„ íƒ...")
            var_selector = VarianceThreshold(threshold=0.01)
            X_var = var_selector.fit_transform(X_2d)
            print(f"     ì œê±°ëœ íŠ¹ì§•: {X_2d.shape[1] - X_var.shape[1]}ê°œ")
            
            # 2. ìƒìœ„ 128ê°œ íŠ¹ì§• ì„ íƒ
            if X_var.shape[1] > 128:
                print("   - F-í†µê³„ëŸ‰ ê¸°ë°˜ íŠ¹ì§• ì„ íƒ...")
                # ì„ì‹œ ë¼ë²¨ ìƒì„± (ì‹¤ì œë¡œëŠ” y ë¼ë²¨ ì‚¬ìš©í•´ì•¼ í•¨)
                temp_y = np.random.randint(0, 2, size=X_var.shape[0])
                k_selector = SelectKBest(f_classif, k=128)
                X_selected = k_selector.fit_transform(X_var, temp_y)
                
                self.var_selector = var_selector
                self.k_selector = k_selector
                print(f"     ìµœì¢… ì„ íƒëœ íŠ¹ì§•: {X_selected.shape[1]}ê°œ")
            else:
                X_selected = X_var
                self.var_selector = var_selector
                self.k_selector = None
        else:
            X_selected = X_2d
            self.var_selector = None
            self.k_selector = None
            print("   - íŠ¹ì§• ì„ íƒ ê±´ë„ˆë›°ê¸° (íŠ¹ì§• ìˆ˜ê°€ ì ê±°ë‚˜ ë¹„í™œì„±í™”)")
        
        # ì •ê·œí™”
        print("   - ë°ì´í„° ì •ê·œí™” (StandardScaler)...")
        X_normalized = self.scaler.fit_transform(X_selected)
        
        # ì›ë³¸ í˜•íƒœë¡œ ë³µì› (íŠ¹ì§• ìˆ˜ëŠ” ë³€ê²½ë  ìˆ˜ ìˆìŒ)
        new_shape = (original_shape[0], original_shape[1], X_normalized.shape[1])
        X_normalized = X_normalized.reshape(new_shape)
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - ìµœì¢… í˜•íƒœ: {X_normalized.shape}")
        print(f"   - ë°ì´í„° ë²”ìœ„: [{X_normalized.min():.3f}, {X_normalized.max():.3f}]")
        
        return X_normalized
    
    def build_model(self, input_shape, model_type='standard'):
        """LSTM ëª¨ë¸ êµ¬ì¶• - 256ê°œ íŠ¹ì§•ì— ìµœì í™”"""
        print(f"ğŸ—ï¸  ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        print(f"   - ì…ë ¥ í˜•íƒœ: {input_shape}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {input_shape[0]}")
        print(f"   - íŠ¹ì§• ìˆ˜: {input_shape[1]}ê°œ")
        print(f"   - ëª¨ë¸ íƒ€ì…: {model_type}")
        
        if model_type == 'lightweight':
            # ê²½ëŸ‰ ëª¨ë¸ (256ê°œ íŠ¹ì§•ì´ ë§ì„ ë•Œ)
            print("   ğŸ“¦ ê²½ëŸ‰ ëª¨ë¸ êµ¬ì„±...")
            model = Sequential([
                # íŠ¹ì§• ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ Dense ì¸µ
                Dense(64, activation='relu', input_shape=input_shape, name='feature_reduction'),
                Dropout(0.3),
                
                # LSTM ì¸µë“¤
                LSTM(32, return_sequences=True, name='lstm_1'),
                Dropout(0.3),
                
                LSTM(16, return_sequences=False, name='lstm_2'),
                Dropout(0.3),
                
                # ì¶œë ¥ì¸µ
                Dense(8, activation='relu', name='dense_1'),
                BatchNormalization(),
                Dropout(0.2),
                Dense(1, activation='sigmoid', name='output')
            ])
            
        elif model_type == 'cnn_lstm':
            # CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
            print("   ğŸ”€ CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì„±...")
            model = Sequential([
                # CNN ì¸µìœ¼ë¡œ ì§€ì—­ì  íŒ¨í„´ ì¶”ì¶œ
                Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape, name='conv1d_1'),
                Dropout(0.3),
                
                Conv1D(32, kernel_size=3, activation='relu', name='conv1d_2'),
                MaxPooling1D(pool_size=2, name='maxpool_1'),
                Dropout(0.3),
                
                # LSTM ì¸µìœ¼ë¡œ ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ
                LSTM(32, return_sequences=False, name='lstm'),
                Dropout(0.3),
                
                # ì¶œë ¥ì¸µ
                Dense(16, activation='relu', name='dense_1'),
                BatchNormalization(),
                Dropout(0.2),
                Dense(1, activation='sigmoid', name='output')
            ])
            
        else:  # standard
            # í‘œì¤€ ëª¨ë¸ (ë” í° LSTM ìœ ë‹› ì‚¬ìš©)
            print("   ğŸ›ï¸  í‘œì¤€ ëª¨ë¸ êµ¬ì„±...")
            model = Sequential([
                # ì²« ë²ˆì§¸ LSTM ì¸µ (ìœ ë‹› ìˆ˜ ì¦ê°€)
                LSTM(128, return_sequences=True, input_shape=input_shape, name='lstm_1'),
                Dropout(0.4),
                
                # ë‘ ë²ˆì§¸ LSTM ì¸µ
                LSTM(64, return_sequences=False, name='lstm_2'),
                Dropout(0.4),
                
                # ì™„ì „ì—°ê²°ì¸µ
                Dense(32, activation='relu', name='dense_1'),
                BatchNormalization(),
                Dropout(0.3),
                
                Dense(16, activation='relu', name='dense_2'),
                Dropout(0.2),
                
                # ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜)
                Dense(1, activation='sigmoid', name='output')
            ])
        
        # íŠ¹ì§• ìˆ˜ì— ë”°ë¥¸ í•™ìŠµë¥  ì¡°ì •
        learning_rate = 0.001 if input_shape[1] <= 64 else 0.0005
        print(f"   - í•™ìŠµë¥ : {learning_rate}")
        
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model.count_params():,}ê°œ")
        
        # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
        model.summary()
        
        self.model = model
        return model
    
    def train_model(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """ëª¨ë¸ í•™ìŠµ"""
        print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print(f"   - í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
        print(f"   - ì—í¬í¬: {epochs}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=7,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # ê²€ì¦ ë°ì´í„° ì„¤ì •
        if X_val is not None and y_val is not None:
            validation_data = (X_val, y_val)
            validation_split = None
            print(f"   - ê²€ì¦ ë°ì´í„°: {X_val.shape}")
        else:
            validation_data = None
            validation_split = 0.2
            print(f"   - ê²€ì¦ ë¶„í• : {validation_split}")
        
        # í•™ìŠµ
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=validation_data,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
        return history
    
    def evaluate_model(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        print(f"ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
        
        # ì˜ˆì¸¡
        y_pred_prob = self.model.predict(X_test, verbose=0)
        y_pred = (y_pred_prob > 0.5).astype(int).flatten()
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        from sklearn.metrics import classification_report, confusion_matrix
        
        print(f"\nğŸ“ˆ ë¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test, y_pred, target_names=['ì •ìƒ', 'ë‚™ìƒ']))
        
        print(f"\nğŸ”¢ í˜¼ë™ í–‰ë ¬:")
        cm = confusion_matrix(y_test, y_pred)
        print("        ì˜ˆì¸¡")
        print("ì‹¤ì œ    ì •ìƒ  ë‚™ìƒ")
        print(f"ì •ìƒ   {cm[0,0]:4d}  {cm[0,1]:4d}")
        print(f"ë‚™ìƒ   {cm[1,0]:4d}  {cm[1,1]:4d}")
        
        # ì„±ëŠ¥ ì§€í‘œ
        accuracy = np.mean(y_pred == y_test)
        
        if np.sum(y_test) > 0:  # ì‹¤ì œ ì–‘ì„± ìƒ˜í”Œì´ ìˆëŠ” ê²½ìš°
            recall = np.sum((y_pred == 1) & (y_test == 1)) / np.sum(y_test == 1)
            precision = np.sum((y_pred == 1) & (y_test == 1)) / max(np.sum(y_pred == 1), 1)
            f1 = 2 * (precision * recall) / max(precision + recall, 1e-7)
            
            print(f"\nâ­ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   - ì •í™•ë„ (Accuracy): {accuracy:.4f} ({accuracy*100:.1f}%)")
            print(f"   - ì •ë°€ë„ (Precision): {precision:.4f} ({precision*100:.1f}%)")
            print(f"   - ì¬í˜„ìœ¨ (Recall): {recall:.4f} ({recall*100:.1f}%)")
            print(f"   - F1-ì ìˆ˜: {f1:.4f}")
            
            # ë‚™ìƒ ê°ì§€ ê´€ì ì—ì„œì˜ í•´ì„
            print(f"\nğŸ¯ ë‚™ìƒ ê°ì§€ ê´€ì :")
            print(f"   - ì‹¤ì œ ë‚™ìƒì„ ë†“ì¹œ ë¹„ìœ¨: {(1-recall)*100:.1f}%")
            print(f"   - ê±°ì§“ ì•ŒëŒ ë¹„ìœ¨: {(1-precision)*100:.1f}%")
        else:
            print(f"\nâ­ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   - ì •í™•ë„ (Accuracy): {accuracy:.4f} ({accuracy*100:.1f}%)")
            print(f"   âš ï¸  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë‚™ìƒ ìƒ˜í”Œì´ ì—†ì–´ ì¼ë¶€ ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return y_pred_prob, y_pred