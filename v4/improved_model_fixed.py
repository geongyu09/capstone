"""
í–¥ìƒëœ CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸ (ìˆ˜ì •ëœ ë²„ì „)
ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì”ì°¨ ì—°ê²°ì„ í¬í•¨í•œ ê°œì„ ëœ ì•„í‚¤í…ì²˜
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, 
    BatchNormalization, GlobalAveragePooling1D, Concatenate,
    Bidirectional, Add, Lambda, Layer
)
from tensorflow.keras.optimizers import Adam
import numpy as np

from config import Config, ModelConfig
from utils import setup_logging

class AttentionLayer(Layer):
    """ê°œì„ ëœ ì…€í”„ ì–´í…ì…˜ ë ˆì´ì–´"""
    
    def __init__(self, units, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.units = units
        
    def build(self, input_shape):
        self.W = self.add_weight(
            name='attention_weight',
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            name='attention_bias',
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )
        self.u = self.add_weight(
            name='attention_context',
            shape=(self.units,),
            initializer='glorot_uniform',
            trainable=True
        )
        super(AttentionLayer, self).build(input_shape)
        
    def call(self, inputs):
        # inputs: (batch_size, sequence_length, features)
        uit = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
        ait = tf.tensordot(uit, self.u, axes=1)
        a = tf.nn.softmax(ait, axis=1)
        a = tf.expand_dims(a, -1)
        weighted_input = inputs * a
        output = tf.reduce_sum(weighted_input, axis=1)
        return output
    
    def get_config(self):
        config = super(AttentionLayer, self).get_config()
        config.update({'units': self.units})
        return config

class FocalLoss(tf.keras.losses.Loss):
    """Focal Loss í´ë˜ìŠ¤"""
    
    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):
        super(FocalLoss, self).__init__(**kwargs)
        self.alpha = alpha
        self.gamma = gamma
    
    def call(self, y_true, y_pred):
        # í´ë¦¬í•‘ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        
        # Focal Loss ê³„ì‚°
        alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_loss = -alpha_t * tf.pow((1 - p_t), self.gamma) * tf.math.log(p_t)
        
        return tf.reduce_mean(focal_loss)
    
    def get_config(self):
        config = super(FocalLoss, self).get_config()
        config.update({
            'alpha': self.alpha,
            'gamma': self.gamma
        })
        return config

def residual_block(x, filters, kernel_size=3, dropout_rate=0.3):
    """ì”ì°¨ ë¸”ë¡"""
    shortcut = x
    
    # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
    x = Conv1D(filters, kernel_size, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
    x = Conv1D(filters, kernel_size, padding='same')(x)
    x = BatchNormalization()(x)
    
    # ì°¨ì›ì´ ë‹¤ë¥´ë©´ shortcut ì¡°ì •
    if shortcut.shape[-1] != filters:
        shortcut = Conv1D(filters, 1, padding='same')(shortcut)
    
    # ì”ì°¨ ì—°ê²°
    x = Add()([x, shortcut])
    x = tf.keras.layers.ReLU()(x)
    
    return x

def multi_scale_cnn_branch(inputs):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ CNN ë¸Œëœì¹˜"""
    cnn_outputs = []
    
    # ë‹¤ì–‘í•œ ì»¤ë„ í¬ê¸°ë¡œ íŠ¹ì„± ì¶”ì¶œ
    for kernel_size in [3, 5, 7]:
        x = inputs
        
        # ì²« ë²ˆì§¸ ì”ì°¨ ë¸”ë¡
        x = residual_block(x, 64, kernel_size)
        x = MaxPooling1D(2)(x)
        
        # ë‘ ë²ˆì§¸ ì”ì°¨ ë¸”ë¡
        x = residual_block(x, 128, kernel_size)
        x = MaxPooling1D(2)(x)
        
        # ì „ì—­ í‰ê·  í’€ë§
        x = GlobalAveragePooling1D()(x)
        
        cnn_outputs.append(x)
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì„± ê²°í•©
    if len(cnn_outputs) > 1:
        combined = Concatenate()(cnn_outputs)
    else:
        combined = cnn_outputs[0]
    
    return combined

def attention_lstm_branch(inputs):
    """ì–´í…ì…˜ì´ ì ìš©ëœ LSTM ë¸Œëœì¹˜"""
    
    # Bidirectional LSTM
    lstm_out = Bidirectional(
        LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)
    )(inputs)
    
    # ì»¤ìŠ¤í…€ ì–´í…ì…˜ ì ìš©
    attention_layer = AttentionLayer(units=128)
    context_vector = attention_layer(lstm_out)
    
    return context_vector

def build_improved_model(input_shape=(50, 245)):
    """í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•"""
    
    logger = setup_logging()
    logger.info("ğŸ§  í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¶• ì‹œì‘")
    
    inputs = Input(shape=input_shape, name='input')
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ CNN ë¸Œëœì¹˜
    cnn_features = multi_scale_cnn_branch(inputs)
    
    # ì–´í…ì…˜ LSTM ë¸Œëœì¹˜
    lstm_features = attention_lstm_branch(inputs)
    
    # íŠ¹ì„± ê²°í•©
    combined_features = Concatenate()([cnn_features, lstm_features])
    
    # ë¶„ë¥˜ê¸°
    x = Dense(256, activation='relu')(combined_features)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    outputs = Dense(1, activation='sigmoid', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='ImprovedCSIFallDetection')
    
    logger.info("âœ… í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¶• ì™„ë£Œ")
    
    return model

def compile_improved_model(model, learning_rate=0.001, use_focal_loss=True):
    """í–¥ìƒëœ ëª¨ë¸ ì»´íŒŒì¼"""
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = Adam(
        learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    if use_focal_loss:
        loss = FocalLoss(alpha=0.25, gamma=2.0)
        loss_name = "focal_loss"
    else:
        # ê°€ì¤‘ì¹˜ ì ìš© binary crossentropy
        def weighted_bce(y_true, y_pred):
            class_weight_1 = 2.0
            class_weight_0 = 1.0
            weights = y_true * class_weight_1 + (1 - y_true) * class_weight_0
            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
            return bce * weights
        
        loss = weighted_bce
        loss_name = "weighted_binary_crossentropy"
    
    # ë©”íŠ¸ë¦­
    metrics = [
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
    
    model.compile(
        optimizer=optimizer,
        loss=loss,
        metrics=metrics
    )
    
    print(f"âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ (ì†ì‹¤ í•¨ìˆ˜: {loss_name})")
    
    return model

def create_lightweight_model(input_shape=(50, 245)):
    """ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ìœ„í•œ ê²½ëŸ‰í™” ëª¨ë¸"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # ê¹Šì´ë³„ ë¶„ë¦¬ ê°€ëŠ¥í•œ ì»¨ë³¼ë£¨ì…˜
    x = tf.keras.layers.SeparableConv1D(64, 3, activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(2)(x)
    x = Dropout(0.2)(x)
    
    x = tf.keras.layers.SeparableConv1D(128, 3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling1D(2)(x)
    x = Dropout(0.2)(x)
    
    # ë‹¨ìˆœí•œ LSTM
    x = LSTM(64, dropout=0.2)(x)
    
    # ì••ì¶•ëœ ë¶„ë¥˜ê¸°
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    
    outputs = Dense(1, activation='sigmoid', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='LightweightCSIFallDetection')
    
    return model

def create_baseline_model(input_shape=(50, 245)):
    """ê¸°ë³¸ ë¹„êµ ëª¨ë¸"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # ê°„ë‹¨í•œ LSTM
    x = LSTM(64, dropout=0.3, recurrent_dropout=0.3)(inputs)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='BaselineCSIModel')
    
    return model

def model_comparison_test():
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    input_shape = (Config.WINDOW_SIZE, Config.TOTAL_FEATURES)
    
    models = {
        'Baseline': create_baseline_model(input_shape),
        'Improved': build_improved_model(input_shape),
        'Lightweight': create_lightweight_model(input_shape)
    }
    
    # ëª¨ë¸ë³„ ì •ë³´ ì¶œë ¥
    for name, model in models.items():
        print(f"\nğŸ“Š {name} ëª¨ë¸:")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì¶”ë¡  ì‹œê°„ í…ŒìŠ¤íŠ¸
        dummy_input = np.random.randn(1, *input_shape)
        
        import time
        start_time = time.time()
        for _ in range(10):  # 10ë²ˆ ì¶”ë¡ 
            _ = model.predict(dummy_input, verbose=0)
        inference_time = (time.time() - start_time) / 10
        
        print(f"   ì¶”ë¡  ì‹œê°„: {inference_time*1000:.2f}ms")
        print(f"   ëª¨ë¸ í¬ê¸° (ì¶”ì •): {model.count_params() * 4 / 1024 / 1024:.1f}MB")
        
        # ëª¨ë¸ ìš”ì•½ (ê°„ë‹¨íˆ)
        print(f"   ë ˆì´ì–´ ìˆ˜: {len(model.layers)}")
    
    return models

def save_improved_model(model, model_name="improved_csi_model"):
    """í–¥ìƒëœ ëª¨ë¸ ì €ì¥"""
    
    from utils import create_timestamp, save_model_artifacts
    
    experiment_name = f"{model_name}_{create_timestamp()}"
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        'experiment_name': experiment_name,
        'model_type': 'improved_hybrid',
        'timestamp': create_timestamp(),
        'architecture': 'CNN+LSTM with Attention',
        'config': {
            'window_size': Config.WINDOW_SIZE,
            'total_features': Config.TOTAL_FEATURES,
            'batch_size': Config.BATCH_SIZE
        },
        'improvements': [
            'Multi-scale CNN',
            'Bidirectional LSTM with Attention',
            'Residual connections',
            'Focal Loss'
        ],
        'created_by': 'improved_model.py'
    }
    
    try:
        # ëª¨ë¸ ì €ì¥
        model_path = f"./models/{experiment_name}.keras"
        model.save(model_path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        import json
        metadata_path = f"./models/{experiment_name}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
        print(f"   ëª¨ë¸: {model_path}")
        print(f"   ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        return experiment_name
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def train_improved_model():
    """í–¥ìƒëœ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„"""
    print("ğŸš€ í–¥ìƒëœ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„")
    
    # ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
    model = build_improved_model()
    model = compile_improved_model(model, use_focal_loss=True)
    
    # ëª¨ë¸ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“‹ ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    model.summary()
    
    # ëª¨ë¸ ì €ì¥
    experiment_name = save_improved_model(model)
    
    if experiment_name:
        print(f"\nâœ… í–¥ìƒëœ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {experiment_name}")
        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ê¸°ì¡´ trainer.pyë¥¼ ìˆ˜ì •í•˜ì—¬ ìƒˆ ëª¨ë¸ ì‚¬ìš©")
        print("2. ë˜ëŠ” ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰:")
        print(f"   # model = load_model('./models/{experiment_name}.keras')")
        
    return model, experiment_name

if __name__ == "__main__":
    print("ğŸ§  í–¥ìƒëœ CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸")
    print("=" * 50)
    
    try:
        # ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
        models = model_comparison_test()
        
        # ì‚¬ìš©ì ì„ íƒ
        print("\nğŸ¤” ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print("1. í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ë§Œ í™•ì¸")
        print("2. í–¥ìƒëœ ëª¨ë¸ ì €ì¥ ë° í•™ìŠµ ì¤€ë¹„")
        print("3. ê²½ëŸ‰í™” ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("4. ëª¨ë“  ëª¨ë¸ ë¹„êµ")
        
        choice = input("ì„ íƒ (1-4): ").strip()
        
        if choice == "1":
            improved_model = build_improved_model()
            print("\nğŸ“‹ í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜:")
            improved_model.summary()
            
        elif choice == "2":
            model, experiment_name = train_improved_model()
            
        elif choice == "3":
            lightweight_model = create_lightweight_model()
            print("\nğŸ“‹ ê²½ëŸ‰í™” ëª¨ë¸ ì•„í‚¤í…ì²˜:")
            lightweight_model.summary()
            
        elif choice == "4":
            print("\nğŸ“Š ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
            for name in models.keys():
                print(f"   âœ… {name} ëª¨ë¸")
            
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì´ ì•„ë‹™ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
