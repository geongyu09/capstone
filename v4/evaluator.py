"""
CSI ë‚™ìƒ ê°ì§€ v4 - ëª¨ë¸ í‰ê°€ê¸°
"""

import os
import glob
from typing import Dict, Any, List, Optional
import numpy as np
import tensorflow as tf

from config import Config
from utils import (
    setup_logging, load_model_artifacts, evaluate_model,
    print_evaluation_results, plot_confusion_matrix, plot_roc_curve
)
from data_generator import CSIDataGenerator, create_data_generators


class CSIModelEvaluator:
    """CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(self, model_path: str, logger=None):
        """
        Args:
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì‹¤í—˜ ì´ë¦„
            logger: ë¡œê±° ê°ì²´
        """
        self.model_path = model_path
        self.logger = logger or setup_logging()
        
        self.model = None
        self.scaler = None
        self.metadata = None
        self.test_gen = None
        
        self.logger.info(f"ğŸ“Š ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”")
        self.logger.info(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    def load_model(self) -> None:
        """ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ë“¤ ë¡œë“œ"""
        self.logger.info("ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ê²°ì •
            if self.model_path.endswith('.keras'):
                # ì§ì ‘ íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                model_file = self.model_path
                model_name = os.path.splitext(os.path.basename(model_file))[0]
            else:
                # ì‹¤í—˜ ì´ë¦„ì¸ ê²½ìš°
                model_name = self.model_path
                model_file = os.path.join(Config.MODEL_DIR, f"{model_name}.keras")
            
            if not os.path.exists(model_file):
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_file}")
            
            # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œë“œ
            self.model, self.scaler, self.metadata = load_model_artifacts(
                Config.MODEL_DIR, model_name
            )
            
            self.logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            self.logger.info(f"   ì‹¤í—˜ ì´ë¦„: {self.metadata.get('experiment_name', 'Unknown')}")
            self.logger.info(f"   ëª¨ë¸ íƒ€ì…: {self.metadata.get('model_type', 'Unknown')}")
            self.logger.info(f"   ì…ë ¥ í˜•íƒœ: {self.metadata.get('input_shape', 'Unknown')}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def prepare_test_data(self) -> None:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        self.logger.info("ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        try:
            # ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
            _, _, self.test_gen = create_data_generators()
            
            self.logger.info("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            self.logger.info(f"   í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {self.test_gen.total_sequences:,}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise
    
    def evaluate(self, detailed: bool = True) -> Dict[str, Any]:
        """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if self.test_gen is None:
            raise ValueError("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. prepare_test_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        self.logger.info("ğŸ“ˆ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        # ê¸°ë³¸ í‰ê°€
        test_results = self.model.evaluate(self.test_gen, verbose=1)
        metric_names = self.model.metrics_names
        basic_results = dict(zip(metric_names, test_results))
        
        results = {
            'basic_metrics': basic_results,
            'model_info': {
                'experiment_name': self.metadata.get('experiment_name', 'Unknown'),
                'model_type': self.metadata.get('model_type', 'Unknown'),
                'total_params': self.model.count_params()
            }
        }
        
        # ìƒì„¸ í‰ê°€
        if detailed:
            self.logger.info("ğŸ”¬ ìƒì„¸ í‰ê°€ ì‹¤í–‰ ì¤‘...")
            detailed_results = self._detailed_evaluation()
            results.update(detailed_results)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(results)
        
        return results
    
    def _detailed_evaluation(self) -> Dict[str, Any]:
        """ìƒì„¸ í‰ê°€ ì‹¤í–‰"""
        
        # ì˜ˆì¸¡ ìˆ˜ì§‘
        y_true = []
        y_scores = []
        
        self.logger.info("ğŸ”„ ì˜ˆì¸¡ ìˆ˜ì§‘ ì¤‘...")
        
        for i in range(len(self.test_gen)):
            X_batch, y_batch = self.test_gen[i]
            scores_batch = self.model.predict(X_batch, verbose=0)
            
            y_true.extend(y_batch)
            y_scores.extend(scores_batch.flatten())
            
            if (i + 1) % 10 == 0:
                self.logger.info(f"   ì§„í–‰ë¥ : {i+1}/{len(self.test_gen)} ë°°ì¹˜")
        
        y_true = np.array(y_true)
        y_scores = np.array(y_scores)
        
        # ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ í‰ê°€
        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
        threshold_results = {}
        
        for threshold in thresholds:
            eval_result = evaluate_model(
                model=self.model,
                X_test=None,  # ì´ë¯¸ ì˜ˆì¸¡ì´ ì™„ë£Œë¨
                y_test=y_true,
                threshold=threshold
            )
            
            # ì˜ˆì¸¡ê°’ ì§ì ‘ ì„¤ì •
            eval_result['predictions']['y_scores'] = y_scores.tolist()
            eval_result['predictions']['y_pred'] = (y_scores > threshold).astype(int).tolist()
            
            threshold_results[f'threshold_{threshold}'] = eval_result
        
        # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
        best_threshold = 0.5
        best_f1 = 0.0
        
        for threshold in thresholds:
            f1 = threshold_results[f'threshold_{threshold}']['f1_score']
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        # ì‹œê°í™”
        self._create_visualizations(y_true, y_scores, best_threshold)
        
        return {
            'detailed_metrics': threshold_results,
            'best_threshold': best_threshold,
            'best_f1_score': best_f1,
            'predictions': {
                'y_true': y_true.tolist(),
                'y_scores': y_scores.tolist()
            }
        }
    
    def _create_visualizations(self, y_true: np.ndarray, y_scores: np.ndarray, threshold: float) -> None:
        """ì‹œê°í™” ìƒì„±"""
        
        try:
            experiment_name = self.metadata.get('experiment_name', 'unknown')
            
            # í˜¼ë™ í–‰ë ¬
            y_pred = (y_scores > threshold).astype(int)
            cm_path = os.path.join(Config.LOG_DIR, f"{experiment_name}_confusion_matrix.png")
            plot_confusion_matrix(y_true, y_pred, save_path=cm_path)
            
            # ROC ì»¤ë¸Œ
            roc_path = os.path.join(Config.LOG_DIR, f"{experiment_name}_roc_curve.png")
            roc_auc = plot_roc_curve(y_true, y_scores, save_path=roc_path)
            
            self.logger.info(f"ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ")
            self.logger.info(f"   í˜¼ë™ í–‰ë ¬: {cm_path}")
            self.logger.info(f"   ROC ì»¤ë¸Œ: {roc_path} (AUC: {roc_auc:.3f})")
            
        except Exception as e:
            self.logger.warning(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _print_results(self, results: Dict[str, Any]) -> None:
        """ê²°ê³¼ ì¶œë ¥"""
        
        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("=" * 50)
        
        # ëª¨ë¸ ì •ë³´
        model_info = results['model_info']
        print(f"ğŸ¤– ëª¨ë¸ ì •ë³´:")
        print(f"   ì‹¤í—˜ ì´ë¦„: {model_info['experiment_name']}")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_info['model_type']}")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {model_info['total_params']:,}")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        basic_metrics = results['basic_metrics']
        print(f"\nğŸ“ˆ ê¸°ë³¸ ì„±ëŠ¥:")
        for metric, value in basic_metrics.items():
            print(f"   {metric}: {value:.4f}")
        
        # ìƒì„¸ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
        if 'best_threshold' in results:
            print(f"\nğŸ¯ ìµœì  ì„ê³„ê°’: {results['best_threshold']}")
            print(f"ğŸ† ìµœê³  F1 ì ìˆ˜: {results['best_f1_score']:.4f}")
            
            # ìµœì  ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
            best_key = f"threshold_{results['best_threshold']}"
            if best_key in results['detailed_metrics']:
                best_result = results['detailed_metrics'][best_key]
                print_evaluation_results(best_result)


def evaluate_saved_model(model_name: str, detailed: bool = True) -> Dict[str, Any]:
    """ì €ì¥ëœ ëª¨ë¸ í‰ê°€ í—¬í¼ í•¨ìˆ˜"""
    
    evaluator = CSIModelEvaluator(model_name)
    evaluator.load_model()
    evaluator.prepare_test_data()
    
    return evaluator.evaluate(detailed=detailed)


def list_available_models() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    
    model_files = glob.glob(os.path.join(Config.MODEL_DIR, "*.keras"))
    model_names = [os.path.splitext(os.path.basename(f))[0] for f in model_files]
    
    return sorted(model_names)


if __name__ == "__main__":
    # í‰ê°€ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª CSI ëª¨ë¸ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = list_available_models()
    
    if not available_models:
        print("âŒ í‰ê°€í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”: python main.py --mode train")
    else:
        print(f"ğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(available_models)}ê°œ")
        for i, model_name in enumerate(available_models, 1):
            print(f"   {i}. {model_name}")
        
        # ì‚¬ìš©ì ëª¨ë¸ ì„ íƒ
        try:
            choice = input(f"\ní‰ê°€í•  ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-{len(available_models)}): ").strip()
            model_idx = int(choice) - 1
            
            if 0 <= model_idx < len(available_models):
                selected_model = available_models[model_idx]
                print(f"\nğŸ¯ ì„ íƒëœ ëª¨ë¸: {selected_model}")
                
                # í‰ê°€ ì‹¤í–‰
                results = evaluate_saved_model(selected_model, detailed=True)
                
                print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
                
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                
        except (ValueError, KeyboardInterrupt):
            print("âŒ í‰ê°€ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
