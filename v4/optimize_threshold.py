"""
ì„ê³„ê°’ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ ëª¨ë¸ì˜ ìµœì  ì„ê³„ê°’ì„ ì°¾ì•„ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, accuracy_score

from evaluator import evaluate_saved_model
from utils import setup_logging

def find_optimal_threshold(model_name="hybrid_20250607_143159_best"):
    """ìµœì  ì„ê³„ê°’ ì°¾ê¸°"""
    
    logger = setup_logging()
    logger.info("ğŸ¯ ì„ê³„ê°’ ìµœì í™” ì‹œì‘")
    
    # ëª¨ë¸ í‰ê°€ ì‹¤í–‰í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ ì–»ê¸°
    results = evaluate_saved_model(model_name, detailed=True)
    
    if 'predictions' not in results:
        logger.error("ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    y_true = np.array(results['predictions']['y_true'])
    y_scores = np.array(results['predictions']['y_scores'])
    
    logger.info(f"ë°ì´í„° í¬ê¸°: {len(y_true)}ê°œ")
    logger.info(f"ë‚™ìƒ ë¹„ìœ¨: {np.mean(y_true):.3f}")
    
    # ë‹¤ì–‘í•œ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
    thresholds = np.arange(0.1, 0.9, 0.05)
    
    metrics = {
        'threshold': [],
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'tn': [],
        'fp': [],
        'fn': [],
        'tp': []
    }
    
    for threshold in thresholds:
        y_pred = (y_scores >= threshold).astype(int)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        
        # í˜¼ë™ í–‰ë ¬
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        # ê²°ê³¼ ì €ì¥
        metrics['threshold'].append(threshold)
        metrics['accuracy'].append(accuracy)
        metrics['precision'].append(precision)
        metrics['recall'].append(recall)
        metrics['f1'].append(f1)
        metrics['tn'].append(tn)
        metrics['fp'].append(fp)
        metrics['fn'].append(fn)
        metrics['tp'].append(tp)
        
        logger.info(f"ì„ê³„ê°’ {threshold:.2f}: F1={f1:.3f}, Acc={accuracy:.3f}, "
                   f"Prec={precision:.3f}, Rec={recall:.3f}")
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    best_f1_idx = np.argmax(metrics['f1'])
    best_threshold = metrics['threshold'][best_f1_idx]
    best_f1 = metrics['f1'][best_f1_idx]
    
    logger.info(f"ğŸ† ìµœì  ì„ê³„ê°’: {best_threshold:.2f} (F1: {best_f1:.3f})")
    
    # ì‹œê°í™”
    plot_threshold_analysis(metrics)
    
    # ìµœì  ì„ê³„ê°’ì—ì„œì˜ ìƒì„¸ ê²°ê³¼
    print_detailed_results(metrics, best_f1_idx)
    
    return best_threshold, metrics

def plot_threshold_analysis(metrics):
    """ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    plt.figure(figsize=(15, 10))
    
    # 1. ë©”íŠ¸ë¦­ ë³€í™”
    plt.subplot(2, 3, 1)
    plt.plot(metrics['threshold'], metrics['accuracy'], 'b-o', label='Accuracy')
    plt.plot(metrics['threshold'], metrics['precision'], 'r-s', label='Precision')
    plt.plot(metrics['threshold'], metrics['recall'], 'g-^', label='Recall')
    plt.plot(metrics['threshold'], metrics['f1'], 'k-d', label='F1 Score')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title('Performance vs Threshold')
    plt.legend()
    plt.grid(True)
    
    # 2. F1 ì ìˆ˜ ìƒì„¸
    plt.subplot(2, 3, 2)
    plt.plot(metrics['threshold'], metrics['f1'], 'k-o', linewidth=2)
    best_idx = np.argmax(metrics['f1'])
    plt.plot(metrics['threshold'][best_idx], metrics['f1'][best_idx], 
             'ro', markersize=10, label=f'Best: {metrics["threshold"][best_idx]:.2f}')
    plt.xlabel('Threshold')
    plt.ylabel('F1 Score')
    plt.title('F1 Score Optimization')
    plt.legend()
    plt.grid(True)
    
    # 3. ì •ë°€ë„-ì¬í˜„ìœ¨ íŠ¸ë ˆì´ë“œì˜¤í”„
    plt.subplot(2, 3, 3)
    plt.plot(metrics['recall'], metrics['precision'], 'b-o')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.grid(True)
    
    # 4. í˜¼ë™ í–‰ë ¬ ìš”ì†Œ
    plt.subplot(2, 3, 4)
    plt.plot(metrics['threshold'], metrics['tp'], 'g-o', label='True Positive')
    plt.plot(metrics['threshold'], metrics['fp'], 'r-s', label='False Positive')
    plt.plot(metrics['threshold'], metrics['fn'], 'orange', marker='^', label='False Negative')
    plt.plot(metrics['threshold'], metrics['tn'], 'b-d', label='True Negative')
    plt.xlabel('Threshold')
    plt.ylabel('Count')
    plt.title('Confusion Matrix Elements')
    plt.legend()
    plt.grid(True)
    
    # 5. ê±°ì§“ ì–‘ì„±ë¥  vs ì¬í˜„ìœ¨
    plt.subplot(2, 3, 5)
    fpr = np.array(metrics['fp']) / (np.array(metrics['fp']) + np.array(metrics['tn']))
    plt.plot(fpr, metrics['recall'], 'purple', marker='o')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate (Recall)')
    plt.title('ROC Space')
    plt.grid(True)
    
    # 6. ì„ê³„ê°’ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ
    plt.subplot(2, 3, 6)
    performance_matrix = np.array([
        metrics['accuracy'],
        metrics['precision'], 
        metrics['recall'],
        metrics['f1']
    ])
    
    im = plt.imshow(performance_matrix, cmap='viridis', aspect='auto')
    plt.colorbar(im)
    plt.yticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1'])
    plt.xlabel('Threshold Index')
    plt.title('Performance Heatmap')
    
    plt.tight_layout()
    plt.savefig('./logs/threshold_optimization.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_detailed_results(metrics, best_idx):
    """ìµœì  ì„ê³„ê°’ì—ì„œì˜ ìƒì„¸ ê²°ê³¼ ì¶œë ¥"""
    
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì  ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    threshold = metrics['threshold'][best_idx]
    accuracy = metrics['accuracy'][best_idx]
    precision = metrics['precision'][best_idx]
    recall = metrics['recall'][best_idx]
    f1 = metrics['f1'][best_idx]
    
    tn = metrics['tn'][best_idx]
    fp = metrics['fp'][best_idx]
    fn = metrics['fn'][best_idx]
    tp = metrics['tp'][best_idx]
    
    print(f"ğŸ“Š ìµœì  ì„ê³„ê°’: {threshold:.3f}")
    print(f"")
    print(f"ğŸ¯ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   ì •í™•ë„ (Accuracy): {accuracy:.3f}")
    print(f"   ì •ë°€ë„ (Precision): {precision:.3f}")
    print(f"   ì¬í˜„ìœ¨ (Recall): {recall:.3f}")
    print(f"   F1 ì ìˆ˜: {f1:.3f}")
    print(f"")
    print(f"ğŸ“‹ í˜¼ë™ í–‰ë ¬:")
    print(f"                  ì˜ˆì¸¡")
    print(f"              ì •ìƒ    ë‚™ìƒ")
    print(f"   ì‹¤ì œ ì •ìƒ   {tn:4d}   {fp:4d}")
    print(f"        ë‚™ìƒ   {fn:4d}   {tp:4d}")
    print(f"")
    print(f"ğŸ’¡ í•´ì„:")
    print(f"   - ì •ìƒì„ ë‚™ìƒìœ¼ë¡œ ì˜¤ë¶„ë¥˜: {fp}ê±´ ({fp/(tn+fp)*100:.1f}%)")
    print(f"   - ë‚™ìƒì„ ì •ìƒìœ¼ë¡œ ì˜¤ë¶„ë¥˜: {fn}ê±´ ({fn/(fn+tp)*100:.1f}%)")
    print(f"   - ë‚™ìƒ ê°ì§€ ì„±ê³µë¥ : {tp}ê±´ ({tp/(fn+tp)*100:.1f}%)")
    
    # ê°œì„  ë°©í–¥ ì œì‹œ
    print(f"")
    print(f"ğŸš€ ê°œì„  ë°©í–¥:")
    if precision < 0.5:
        print(f"   âš ï¸  ì •ë°€ë„ê°€ ë‚®ìŒ â†’ ê±°ì§“ ì–‘ì„± ê°ì†Œ í•„ìš”")
    if recall < 0.7:
        print(f"   âš ï¸  ì¬í˜„ìœ¨ì´ ë‚®ìŒ â†’ ë†“ì¹˜ëŠ” ë‚™ìƒ ê°ì†Œ í•„ìš”")
    if f1 < 0.6:
        print(f"   âš ï¸  ì „ì²´ì ì¸ ì„±ëŠ¥ ê°œì„  í•„ìš”")
    
    print("="*60)

def apply_optimal_threshold(optimal_threshold):
    """ìµœì  ì„ê³„ê°’ì„ ì„¤ì • íŒŒì¼ì— ì €ì¥"""
    
    try:
        # config.py ì—…ë°ì´íŠ¸
        config_path = "./config.py"
        
        with open(config_path, 'r') as f:
            content = f.read()
        
        # FALL_THRESHOLD ê°’ ì—…ë°ì´íŠ¸
        updated_content = content.replace(
            'FALL_THRESHOLD = 0.5',
            f'FALL_THRESHOLD = {optimal_threshold:.3f}'
        )
        
        with open(config_path, 'w') as f:
            f.write(updated_content)
        
        print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: FALL_THRESHOLD = {optimal_threshold:.3f}")
        
        # ì‹¤ì‹œê°„ ê°ì§€ê¸°ìš© ì„¤ì • íŒŒì¼ë„ ìƒì„±
        threshold_config = {
            'optimal_threshold': optimal_threshold,
            'model_name': 'hybrid_20250607_143159_best',
            'updated_time': str(np.datetime64('now'))
        }
        
        import json
        with open('./models/optimal_threshold.json', 'w') as f:
            json.dump(threshold_config, f, indent=2)
        
        print("âœ… ì„ê³„ê°’ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: ./models/optimal_threshold.json")
        
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def compare_thresholds():
    """í˜„ì¬ ì„ê³„ê°’ vs ìµœì  ì„ê³„ê°’ ë¹„êµ"""
    
    model_name = "hybrid_20250607_143159_best"
    current_threshold = 0.5  # í˜„ì¬ ì‚¬ìš©ì¤‘ì¸ ì„ê³„ê°’
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    optimal_threshold, metrics = find_optimal_threshold(model_name)
    
    if optimal_threshold is None:
        return
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š ì„ê³„ê°’ ë¹„êµ ë¶„ì„")
    print("="*70)
    
    # í˜„ì¬ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
    current_idx = None
    for i, t in enumerate(metrics['threshold']):
        if abs(t - current_threshold) < 0.01:
            current_idx = i
            break
    
    if current_idx is not None:
        optimal_idx = np.argmax(metrics['f1'])
        
        print(f"ğŸ”¹ í˜„ì¬ ì„ê³„ê°’ ({current_threshold:.2f}):")
        print(f"   F1: {metrics['f1'][current_idx]:.3f}")
        print(f"   ì •í™•ë„: {metrics['accuracy'][current_idx]:.3f}")
        print(f"   ì •ë°€ë„: {metrics['precision'][current_idx]:.3f}")
        print(f"   ì¬í˜„ìœ¨: {metrics['recall'][current_idx]:.3f}")
        
        print(f"\nğŸ† ìµœì  ì„ê³„ê°’ ({optimal_threshold:.2f}):")
        print(f"   F1: {metrics['f1'][optimal_idx]:.3f}")
        print(f"   ì •í™•ë„: {metrics['accuracy'][optimal_idx]:.3f}")
        print(f"   ì •ë°€ë„: {metrics['precision'][optimal_idx]:.3f}")
        print(f"   ì¬í˜„ìœ¨: {metrics['recall'][optimal_idx]:.3f}")
        
        # ê°œì„ ëŸ‰ ê³„ì‚°
        f1_improvement = metrics['f1'][optimal_idx] - metrics['f1'][current_idx]
        acc_improvement = metrics['accuracy'][optimal_idx] - metrics['accuracy'][current_idx]
        
        print(f"\nğŸ“ˆ ì˜ˆìƒ ê°œì„ ëŸ‰:")
        print(f"   F1 ì ìˆ˜: +{f1_improvement:.3f} ({f1_improvement/metrics['f1'][current_idx]*100:+.1f}%)")
        print(f"   ì •í™•ë„: +{acc_improvement:.3f} ({acc_improvement/metrics['accuracy'][current_idx]*100:+.1f}%)")
        
        # ì ìš© ì—¬ë¶€ í™•ì¸
        if f1_improvement > 0.01:  # 1% ì´ìƒ ê°œì„ ë˜ëŠ” ê²½ìš°
            response = input(f"\nğŸ¤” ìµœì  ì„ê³„ê°’ì„ ì ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() == 'y':
                apply_optimal_threshold(optimal_threshold)
            else:
                print("ì„ê³„ê°’ ë³€ê²½ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print("ğŸ’¡ í˜„ì¬ ì„ê³„ê°’ì´ ì´ë¯¸ ì¶©ë¶„íˆ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    print("="*70)

if __name__ == "__main__":
    print("ğŸ¯ CSI ë‚™ìƒ ê°ì§€ ì„ê³„ê°’ ìµœì í™”")
    print("=" * 50)
    
    try:
        # ì„ê³„ê°’ ë¹„êµ ë° ìµœì í™”
        compare_thresholds()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
