"""
CSI ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ v4
ë‚™ìƒ ê°ì§€ë¥¼ ìœ„í•œ CSI ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ê°œì„ ì‚¬í•­:
- ëª¨ë“ˆí™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
- ë‹¤ì–‘í•œ ì •ê·œí™” ì˜µì…˜
- ë°ì´í„° ê²€ì¦ ê¸°ëŠ¥
"""

import numpy as np
import pandas as pd
from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
import os
import logging
from typing import Tuple, Optional, List, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns


class CSIPreprocessor:
    """CSI ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 amplitude_start_col: int = 8,
                 amplitude_end_col: int = 253,
                 scaler_type: str = 'minmax',
                 logger: Optional[logging.Logger] = None):
        """
        Args:
            amplitude_start_col: amplitude ë°ì´í„° ì‹œì‘ ì»¬ëŸ¼ ì¸ë±ìŠ¤
            amplitude_end_col: amplitude ë°ì´í„° ì¢…ë£Œ ì»¬ëŸ¼ ì¸ë±ìŠ¤ (exclusive)
            scaler_type: ì •ê·œí™” ë°©ë²• ('minmax', 'standard', 'robust')
            logger: ë¡œê±° ê°ì²´
        """
        self.amplitude_start_col = amplitude_start_col
        self.amplitude_end_col = amplitude_end_col
        self.scaler_type = scaler_type
        self.logger = logger or self._setup_logger()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.scaler = self._init_scaler()
        self.fitted = False
        
        self.logger.info(f"CSI Preprocessor ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"  Amplitude ì»¬ëŸ¼ ë²”ìœ„: {amplitude_start_col}:{amplitude_end_col}")
        self.logger.info(f"  ìŠ¤ì¼€ì¼ëŸ¬ íƒ€ì…: {scaler_type}")
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('CSIPreprocessor')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _init_scaler(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”"""
        if self.scaler_type == 'minmax':
            return MinMaxScaler()
        elif self.scaler_type == 'standard':
            return StandardScaler()
        elif self.scaler_type == 'robust':
            return RobustScaler()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¼ëŸ¬ íƒ€ì…: {self.scaler_type}")
    
    def apply_moving_average_2d(self, data: np.ndarray, window_size: int = 5) -> np.ndarray:
        """
        2D ë°ì´í„°ì— ëŒ€í•´ ì‹œê°„ ì¶•(í–‰ ë°©í–¥)ìœ¼ë¡œ ì´ë™ í‰ê·  í•„í„°ë¥¼ ì ìš©
        
        Args:
            data: (íŒ¨í‚· ìˆ˜, í”¼ì²˜ ìˆ˜) í˜•íƒœì˜ 2D ë°ì´í„°
            window_size: ì´ë™ í‰ê·  ì°½ í¬ê¸°
            
        Returns:
            ì´ë™ í‰ê·  ì²˜ë¦¬ëœ ê²°ê³¼
        """
        self.logger.debug(f"ì´ë™ í‰ê·  í•„í„° ì ìš© (window_size={window_size})")
        
        df = pd.DataFrame(data)
        filtered = df.rolling(window=window_size, min_periods=1).mean()
        return filtered.values
    
    def remove_outliers_zscore(self, data: np.ndarray, threshold: float = 3.0) -> pd.DataFrame:
        """
        z-score ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°
        ì´ìƒì¹˜ì¸ ê°’ì€ NaNìœ¼ë¡œ ë°”ê¾¸ê³  ì„ í˜• ë³´ê°„ìœ¼ë¡œ ì±„ì›€
        
        Args:
            data: ì…ë ¥ ë°ì´í„°
            threshold: z-score ì„ê³„ê°’
            
        Returns:
            ì´ìƒì¹˜ê°€ ì œê±°ëœ DataFrame
        """
        self.logger.debug(f"Z-score ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (threshold={threshold})")
        
        df = pd.DataFrame(data)
        z_scores = np.abs(zscore(df, nan_policy='omit'))
        
        # ì´ìƒì¹˜ë¥¼ NaNìœ¼ë¡œ ì„¤ì •
        df[z_scores > threshold] = np.nan
        
        # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        df.interpolate(method='linear', axis=0, inplace=True, limit_direction='both')
        
        # ì—¬ì „íˆ NaNì´ ìˆëŠ” ê²½ìš° forward fill / backward fill
        df.fillna(method='ffill', inplace=True)
        df.fillna(method='bfill', inplace=True)
        
        return df
    
    def normalize_data(self, df: pd.DataFrame, fit: bool = True) -> np.ndarray:
        """
        ë°ì´í„° ì •ê·œí™”
        
        Args:
            df: ì…ë ¥ DataFrame
            fit: Trueì´ë©´ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµ, Falseì´ë©´ ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
            
        Returns:
            ì •ê·œí™”ëœ ë°ì´í„° ë°°ì—´
        """
        self.logger.debug(f"ë°ì´í„° ì •ê·œí™” ({self.scaler_type}, fit={fit})")
        
        if fit:
            scaled = self.scaler.fit_transform(df)
            self.fitted = True
        else:
            if not self.fitted:
                raise ValueError("ìŠ¤ì¼€ì¼ëŸ¬ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit=Trueë¡œ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            scaled = self.scaler.transform(df)
        
        return scaled
    
    def process_single_file(self, 
                          file_path: str,
                          moving_avg_window: int = 5,
                          outlier_threshold: float = 3.0,
                          fit_scaler: bool = True,
                          save_processed: bool = False,
                          output_path: Optional[str] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ë‹¨ì¼ íŒŒì¼ ì „ì²˜ë¦¬
        
        Args:
            file_path: CSV íŒŒì¼ ê²½ë¡œ
            moving_avg_window: ì´ë™ í‰ê·  ì°½ í¬ê¸°
            outlier_threshold: ì´ìƒì¹˜ ì„ê³„ê°’
            fit_scaler: ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì—¬ë¶€
            save_processed: ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì—¬ë¶€
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì „ì²˜ë¦¬ëœ DataFrameê³¼ ì²˜ë¦¬ í†µê³„
        """
        self.logger.info(f"íŒŒì¼ ì „ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        try:
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(file_path)
            original_shape = df.shape
            self.logger.debug(f"ì›ë³¸ ë°ì´í„° í˜•íƒœ: {original_shape}")
            
            # amplitude ë°ì´í„° ì¶”ì¶œ
            amplitude_cols = df.columns[self.amplitude_start_col:self.amplitude_end_col]
            amplitude_data = df.iloc[:, self.amplitude_start_col:self.amplitude_end_col]
            
            self.logger.debug(f"Amplitude ë°ì´í„° í˜•íƒœ: {amplitude_data.shape}")
            
            # 1. ì´ë™ í‰ê·  í•„í„° ì ìš©
            amplitude_filtered = self.apply_moving_average_2d(
                amplitude_data.values, 
                window_size=moving_avg_window
            )
            
            # 2. ì´ìƒì¹˜ ì œê±°
            amplitude_no_outliers = self.remove_outliers_zscore(
                amplitude_filtered, 
                threshold=outlier_threshold
            )
            
            # 3. ì •ê·œí™”
            amplitude_normalized = self.normalize_data(
                amplitude_no_outliers, 
                fit=fit_scaler
            )
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            processed_df = pd.DataFrame(
                amplitude_normalized, 
                columns=amplitude_cols,
                index=amplitude_data.index
            )
            
            # ì›ë³¸ ë°ì´í„°ì˜ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ê³¼ ê²°í•©
            result_df = df.copy()
            result_df.loc[:, amplitude_cols] = processed_df
            
            # ì²˜ë¦¬ í†µê³„
            stats = {
                'original_shape': original_shape,
                'processed_shape': result_df.shape,
                'amplitude_features': len(amplitude_cols),
                'moving_avg_window': moving_avg_window,
                'outlier_threshold': outlier_threshold,
                'scaler_type': self.scaler_type,
                'scaler_fitted': self.fitted
            }
            
            # íŒŒì¼ ì €ì¥
            if save_processed:
                if output_path is None:
                    base_name = os.path.splitext(os.path.basename(file_path))[0]
                    output_path = f"{base_name}_processed.csv"
                
                result_df.to_csv(output_path, index=False)
                self.logger.info(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")
                stats['output_path'] = output_path
            
            self.logger.info(f"íŒŒì¼ ì „ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
            return result_df, stats
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
            raise
    
    def process_multiple_files(self, 
                             file_paths: List[str],
                             output_dir: str,
                             moving_avg_window: int = 5,
                             outlier_threshold: float = 3.0,
                             fit_scaler_on_first: bool = True) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ íŒŒì¼ ë°°ì¹˜ ì „ì²˜ë¦¬
        
        Args:
            file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            moving_avg_window: ì´ë™ í‰ê·  ì°½ í¬ê¸°
            outlier_threshold: ì´ìƒì¹˜ ì„ê³„ê°’
            fit_scaler_on_first: ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        self.logger.info(f"ë°°ì¹˜ ì „ì²˜ë¦¬ ì‹œì‘: {len(file_paths)}ê°œ íŒŒì¼")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        results = {
            'processed_files': [],
            'failed_files': [],
            'total_files': len(file_paths),
            'processing_stats': []
        }
        
        for i, file_path in enumerate(file_paths):
            try:
                # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
                fit_scaler = fit_scaler_on_first and (i == 0)
                
                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±
                base_name = os.path.splitext(os.path.basename(file_path))[0]
                output_path = os.path.join(output_dir, f"{base_name}_processed.csv")
                
                # íŒŒì¼ ì²˜ë¦¬
                processed_df, stats = self.process_single_file(
                    file_path=file_path,
                    moving_avg_window=moving_avg_window,
                    outlier_threshold=outlier_threshold,
                    fit_scaler=fit_scaler,
                    save_processed=True,
                    output_path=output_path
                )
                
                results['processed_files'].append(file_path)
                results['processing_stats'].append(stats)
                
                self.logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(file_paths)} ({(i+1)/len(file_paths)*100:.1f}%)")
                
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
                results['failed_files'].append({'file': file_path, 'error': str(e)})
        
        self.logger.info(f"ë°°ì¹˜ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(results['processed_files'])}ê°œ ì„±ê³µ, {len(results['failed_files'])}ê°œ ì‹¤íŒ¨")
        return results
    
    def visualize_preprocessing_effects(self, 
                                      original_data: np.ndarray,
                                      processed_data: np.ndarray,
                                      sample_features: int = 5,
                                      sample_length: int = 100) -> None:
        """
        ì „ì²˜ë¦¬ íš¨ê³¼ ì‹œê°í™”
        
        Args:
            original_data: ì›ë³¸ ë°ì´í„°
            processed_data: ì „ì²˜ë¦¬ëœ ë°ì´í„°
            sample_features: ì‹œê°í™”í•  íŠ¹ì„± ìˆ˜
            sample_length: ì‹œê°í™”í•  ìƒ˜í”Œ ê¸¸ì´
        """
        self.logger.info("ì „ì²˜ë¦¬ íš¨ê³¼ ì‹œê°í™”")
        
        # ìƒ˜í”Œ ë°ì´í„° ì„ íƒ
        sample_data_orig = original_data[:sample_length, :sample_features]
        sample_data_proc = processed_data[:sample_length, :sample_features]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ì›ë³¸ ë°ì´í„° ì‹œê³„ì—´
        axes[0, 0].plot(sample_data_orig)
        axes[0, 0].set_title('ì›ë³¸ ë°ì´í„° (ì‹œê³„ì—´)')
        axes[0, 0].set_xlabel('ì‹œê°„')
        axes[0, 0].set_ylabel('ê°’')
        axes[0, 0].legend([f'Feature {i}' for i in range(sample_features)])
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‹œê³„ì—´
        axes[0, 1].plot(sample_data_proc)
        axes[0, 1].set_title('ì „ì²˜ë¦¬ëœ ë°ì´í„° (ì‹œê³„ì—´)')
        axes[0, 1].set_xlabel('ì‹œê°„')
        axes[0, 1].set_ylabel('ì •ê·œí™”ëœ ê°’')
        axes[0, 1].legend([f'Feature {i}' for i in range(sample_features)])
        
        # ì›ë³¸ ë°ì´í„° ë¶„í¬
        axes[1, 0].hist(original_data.flatten(), bins=50, alpha=0.7, density=True)
        axes[1, 0].set_title('ì›ë³¸ ë°ì´í„° ë¶„í¬')
        axes[1, 0].set_xlabel('ê°’')
        axes[1, 0].set_ylabel('ë°€ë„')
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¶„í¬
        axes[1, 1].hist(processed_data.flatten(), bins=50, alpha=0.7, density=True)
        axes[1, 1].set_title('ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¶„í¬')
        axes[1, 1].set_xlabel('ì •ê·œí™”ëœ ê°’')
        axes[1, 1].set_ylabel('ë°€ë„')
        
        plt.tight_layout()
        plt.show()
    
    def generate_processing_report(self, stats_list: List[Dict[str, Any]]) -> str:
        """
        ì „ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            stats_list: ì²˜ë¦¬ í†µê³„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë³´ê³ ì„œ ë¬¸ìì—´
        """
        if not stats_list:
            return "ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # í†µê³„ ê³„ì‚°
        total_files = len(stats_list)
        total_original_samples = sum(stat['original_shape'][0] for stat in stats_list)
        total_features = stats_list[0]['amplitude_features']
        
        report = f"""
        ğŸ“Š CSI ë°ì´í„° ì „ì²˜ë¦¬ ë³´ê³ ì„œ
        ===============================================
        
        ğŸ“ ì²˜ë¦¬ ê²°ê³¼:
        - ì´ íŒŒì¼ ìˆ˜: {total_files:,}ê°œ
        - ì´ ìƒ˜í”Œ ìˆ˜: {total_original_samples:,}ê°œ
        - Amplitude íŠ¹ì„± ìˆ˜: {total_features}ê°œ
        
        âš™ï¸ ì „ì²˜ë¦¬ ì„¤ì •:
        - ì´ë™ í‰ê·  ì°½ í¬ê¸°: {stats_list[0]['moving_avg_window']}
        - ì´ìƒì¹˜ ì„ê³„ê°’: {stats_list[0]['outlier_threshold']}
        - ì •ê·œí™” ë°©ë²•: {stats_list[0]['scaler_type']}
        
        ğŸ“ˆ ì²˜ë¦¬ í†µê³„:
        - í‰ê·  íŒŒì¼ í¬ê¸°: {total_original_samples/total_files:.0f}ê°œ ìƒ˜í”Œ
        - ë°ì´í„° í˜•íƒœ: (ìƒ˜í”Œìˆ˜, {total_features}ê°œ íŠ¹ì„±)
        
        âœ… ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì „ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.
        """
        
        return report


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì‚¬ìš© ì˜ˆì œ"""
    import glob
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ”§ CSI ë°ì´í„° ì „ì²˜ë¦¬ v4")
    print("=" * 50)
    
    # CSI ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    data_dir = "../csi_data"
    csv_files = []
    
    # caseë³„ë¡œ íŒŒì¼ ìˆ˜ì§‘
    for case in ['case1', 'case2', 'case3']:
        case_dir = os.path.join(data_dir, case)
        if os.path.exists(case_dir):
            case_files = glob.glob(os.path.join(case_dir, "*.csv"))
            csv_files.extend(case_files)
    
    if not csv_files:
        print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = CSIPreprocessor(
        amplitude_start_col=8,
        amplitude_end_col=253,
        scaler_type='minmax'
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "./processed_data"
    
    # ìƒ˜í”Œ íŒŒì¼ë¡œ ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ë‹¨ì¼ íŒŒì¼ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    test_file = csv_files[0]
    print(f"í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file}")
    
    try:
        processed_df, stats = preprocessor.process_single_file(
            file_path=test_file,
            moving_avg_window=5,
            outlier_threshold=3.0,
            fit_scaler=True,
            save_processed=False
        )
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì›ë³¸ í˜•íƒœ: {stats['original_shape']}")
        print(f"   ì²˜ë¦¬ í›„ í˜•íƒœ: {stats['processed_shape']}")
        print(f"   Amplitude íŠ¹ì„± ìˆ˜: {stats['amplitude_features']}")
        
        # ì „ì²˜ë¦¬ íš¨ê³¼ ì‹œê°í™” (ì²˜ìŒ 100ê°œ ìƒ˜í”Œ, 5ê°œ íŠ¹ì„±)
        original_data = pd.read_csv(test_file).iloc[:, 8:253].values
        processed_data = processed_df.iloc[:, 8:253].values
        
        print("\nğŸ“Š ì „ì²˜ë¦¬ íš¨ê³¼ ì‹œê°í™”")
        preprocessor.visualize_preprocessing_effects(
            original_data=original_data[:100, :5],
            processed_data=processed_data[:100, :5],
            sample_features=5,
            sample_length=100
        )
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return
    
    # ì‚¬ìš©ì ì„ íƒ
    print(f"\nğŸš€ ì „ì²´ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print(f"   ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(csv_files)}ê°œ")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    choice = input("ê³„ì†í•˜ë ¤ë©´ 'y'ë¥¼ ì…ë ¥í•˜ì„¸ìš” (y/n): ").lower()
    
    if choice != 'y':
        print("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    print(f"\nâš¡ ë°°ì¹˜ ì „ì²˜ë¦¬ ì‹œì‘...")
    
    try:
        results = preprocessor.process_multiple_files(
            file_paths=csv_files,
            output_dir=output_dir,
            moving_avg_window=5,
            outlier_threshold=3.0,
            fit_scaler_on_first=True
        )
        
        # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        report = preprocessor.generate_processing_report(results['processing_stats'])
        print(report)
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì¶œë ¥
        if results['failed_files']:
            print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨ íŒŒì¼ë“¤:")
            for failed in results['failed_files']:
                print(f"   {failed['file']}: {failed['error']}")
        
        print(f"\nâœ… ë°°ì¹˜ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì„±ê³µ: {len(results['processed_files'])}ê°œ")
        print(f"   ì‹¤íŒ¨: {len(results['failed_files'])}ê°œ")
        print(f"   ì¶œë ¥ ìœ„ì¹˜: {output_dir}")
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()
