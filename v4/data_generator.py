"""
CSI ë‚™ìƒ ê°ì§€ v4 - ë°ì´í„° ì œë„ˆë ˆì´í„°
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
"""

import numpy as np
import pandas as pd
from tensorflow.keras.utils import Sequence
import os
import glob
from typing import List, Tuple, Optional
from config import Config
from utils import setup_logging


class CSIDataGenerator(Sequence):
    """CSI ë°ì´í„°ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ì œë„ˆë ˆì´í„°"""
    
    def __init__(self, 
                 file_paths: List[str],
                 batch_size: int = Config.BATCH_SIZE,
                 window_size: int = Config.WINDOW_SIZE,
                 stride: int = Config.STRIDE,
                 shuffle: bool = True,
                 scaler=None,
                 logger=None):
        """
        Args:
            file_paths: ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            window_size: ì‹œí€€ìŠ¤ ìœˆë„ìš° í¬ê¸°
            stride: ìœˆë„ìš° ì´ë™ ê°„ê²©
            shuffle: ì—í¬í¬ë§ˆë‹¤ ì…”í”Œ ì—¬ë¶€
            scaler: ì „ì²˜ë¦¬ì— ì‚¬ìš©ëœ ìŠ¤ì¼€ì¼ëŸ¬ (ì´ë¯¸ ì ìš©ëœ ê²½ìš° None)
            logger: ë¡œê±° ê°ì²´
        """
        self.file_paths = file_paths
        self.batch_size = batch_size
        self.window_size = window_size
        self.stride = stride
        self.shuffle = shuffle
        self.scaler = scaler
        self.logger = logger or setup_logging()
        
        # ê° íŒŒì¼ì˜ ì‹œí€€ìŠ¤ ì •ë³´ ê³„ì‚°
        self.sequences = self._calculate_sequences()
        self.total_sequences = len(self.sequences)
        
        # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.indices = np.arange(self.total_sequences)
        if self.shuffle:
            np.random.shuffle(self.indices)
        
        self.logger.info(f"ğŸ“Š ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   íŒŒì¼ ìˆ˜: {len(self.file_paths)}")
        self.logger.info(f"   ì´ ì‹œí€€ìŠ¤: {self.total_sequences:,}ê°œ")
        self.logger.info(f"   ë°°ì¹˜ ìˆ˜: {len(self)}ê°œ")
    
    def _calculate_sequences(self) -> List[Tuple[str, int, int]]:
        """ê° íŒŒì¼ì—ì„œ ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ë“¤ì„ ë¯¸ë¦¬ ê³„ì‚°"""
        sequences = []
        
        for file_path in self.file_paths:
            try:
                # íŒŒì¼ í¬ê¸°ë§Œ í™•ì¸ (ì „ì²´ ë¡œë“œ ì—†ì´)
                df_info = pd.read_csv(file_path, nrows=0)  # í—¤ë”ë§Œ ì½ê¸°
                
                # ì‹¤ì œ ë°ì´í„° í–‰ ìˆ˜ í™•ì¸ì„ ìœ„í•´ í•œ ë²ˆì€ ì½ì–´ì•¼ í•¨
                with open(file_path, 'r') as f:
                    n_rows = sum(1 for line in f) - 1  # í—¤ë” ì œì™¸
                
                if n_rows < self.window_size:
                    self.logger.warning(f"íŒŒì¼ í¬ê¸° ë¶€ì¡±ìœ¼ë¡œ ìŠ¤í‚µ: {file_path} ({n_rows} < {self.window_size})")
                    continue
                
                # ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ì‹œì‘ì ë“¤ ê³„ì‚°
                max_start = n_rows - self.window_size
                for start_idx in range(0, max_start + 1, self.stride):
                    end_idx = start_idx + self.window_size
                    sequences.append((file_path, start_idx, end_idx))
                
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
        
        return sequences
    
    def __len__(self) -> int:
        """ë°°ì¹˜ ìˆ˜ ë°˜í™˜"""
        return int(np.ceil(self.total_sequences / self.batch_size))
    
    def __getitem__(self, batch_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ ë°ì´í„° ìƒì„±"""
        # í˜„ì¬ ë°°ì¹˜ì˜ ì¸ë±ìŠ¤ë“¤
        start_idx = batch_idx * self.batch_size
        end_idx = min(start_idx + self.batch_size, self.total_sequences)
        batch_indices = self.indices[start_idx:end_idx]
        
        # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        X_batch = []
        y_batch = []
        
        for idx in batch_indices:
            file_path, seq_start, seq_end = self.sequences[idx]
            
            try:
                X_seq, y_seq = self._load_sequence(file_path, seq_start, seq_end)
                X_batch.append(X_seq)
                y_batch.append(y_seq)
            except Exception as e:
                self.logger.warning(f"ì‹œí€€ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {file_path}[{seq_start}:{seq_end}] - {e}")
                # ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´
                X_batch.append(np.zeros((self.window_size, Config.TOTAL_FEATURES)))
                y_batch.append(0)
        
        return np.array(X_batch), np.array(y_batch)
    
    def _load_sequence(self, file_path: str, start_idx: int, end_idx: int) -> Tuple[np.ndarray, int]:
        """íŠ¹ì • ì‹œí€€ìŠ¤ ë¡œë“œ"""
        # í•„ìš”í•œ í–‰ë§Œ ë¡œë“œ
        df = pd.read_csv(file_path, skiprows=range(1, start_idx + 1), nrows=self.window_size)
        
        # Amplitude ë°ì´í„° ì¶”ì¶œ
        amplitude_cols = df.columns[Config.AMPLITUDE_START_COL:Config.AMPLITUDE_END_COL]
        X = df[amplitude_cols].values
        
        # ë¼ë²¨ ì²˜ë¦¬
        if 'label' in df.columns:
            labels = df['label'].values
            # ì‹œí€€ìŠ¤ ë¼ë²¨ë§: ë‚™ìƒì´ ì¼ì • ë¹„ìœ¨ ì´ìƒ í¬í•¨ë˜ë©´ ë‚™ìƒ ì‹œí€€ìŠ¤ë¡œ ë¶„ë¥˜
            y = 1 if np.mean(labels) >= Config.OVERLAP_THRESHOLD else 0
        else:
            y = 0  # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš° ì •ìƒìœ¼ë¡œ ë¶„ë¥˜
        
        # ë°ì´í„° ê²€ì¦
        if X.shape[0] != self.window_size:
            raise ValueError(f"ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶ˆì¼ì¹˜: {X.shape[0]} != {self.window_size}")
        
        if X.shape[1] != Config.TOTAL_FEATURES:
            raise ValueError(f"íŠ¹ì„± ìˆ˜ ë¶ˆì¼ì¹˜: {X.shape[1]} != {Config.TOTAL_FEATURES}")
        
        return X, y
    
    def on_epoch_end(self):
        """ì—í¬í¬ ì¢…ë£Œ ì‹œ ì¸ë±ìŠ¤ ì…”í”Œ"""
        if self.shuffle:
            np.random.shuffle(self.indices)
    
    def get_class_distribution(self) -> dict:
        """í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°"""
        labels = []
        
        # ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ë¼ë²¨ ìˆ˜ì§‘ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        self.logger.info("í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚° ì¤‘...")
        
        for i, (file_path, start_idx, end_idx) in enumerate(self.sequences):
            try:
                _, y = self._load_sequence(file_path, start_idx, end_idx)
                labels.append(y)
                
                if (i + 1) % 1000 == 0:
                    self.logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(self.sequences)}")
                    
            except Exception as e:
                self.logger.warning(f"ë¼ë²¨ ë¡œë“œ ì‹¤íŒ¨: {file_path}[{start_idx}:{end_idx}] - {e}")
                labels.append(0)
        
        labels = np.array(labels)
        distribution = {
            'normal': np.sum(labels == 0),
            'fall': np.sum(labels == 1),
            'total': len(labels)
        }
        
        if distribution['total'] > 0:
            distribution['fall_ratio'] = distribution['fall'] / distribution['total']
        else:
            distribution['fall_ratio'] = 0.0
        
        return distribution
    
    def print_statistics(self):
        """í†µê³„ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ“Š ë°ì´í„° ì œë„ˆë ˆì´í„° í†µê³„:")
        print(f"   íŒŒì¼ ìˆ˜: {len(self.file_paths)}")
        print(f"   ì´ ì‹œí€€ìŠ¤: {self.total_sequences:,}ê°œ")
        print(f"   ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"   ë°°ì¹˜ ìˆ˜: {len(self)}ê°œ")
        print(f"   ìœˆë„ìš° í¬ê¸°: {self.window_size}")
        print(f"   ìŠ¤íŠ¸ë¼ì´ë“œ: {self.stride}")
        print(f"   íŠ¹ì„± ìˆ˜: {Config.TOTAL_FEATURES}")


def create_data_generators(processed_data_dir: str = Config.PROCESSED_DATA_DIR,
                          train_ratio: float = Config.TRAIN_RATIO,
                          val_ratio: float = Config.VAL_RATIO,
                          test_ratio: float = Config.TEST_RATIO,
                          random_seed: int = 42) -> Tuple[CSIDataGenerator, CSIDataGenerator, CSIDataGenerator]:
    """í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
    
    logger = setup_logging()
    
    # ì „ì²˜ë¦¬ëœ íŒŒì¼ë“¤ ìˆ˜ì§‘
    processed_files = glob.glob(os.path.join(processed_data_dir, "*_processed.csv"))
    
    if not processed_files:
        raise ValueError(f"ì „ì²˜ë¦¬ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {processed_data_dir}")
    
    logger.info(f"ì „ì²˜ë¦¬ëœ íŒŒì¼ {len(processed_files)}ê°œ ë°œê²¬")
    
    # íŒŒì¼ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¶„í• 
    from utils import split_data_by_files
    train_files, val_files, test_files = split_data_by_files(
        processed_files, train_ratio, val_ratio, test_ratio, random_seed
    )
    
    logger.info(f"ë°ì´í„° ë¶„í• : í›ˆë ¨ {len(train_files)}ê°œ, ê²€ì¦ {len(val_files)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test_files)}ê°œ")
    
    # ì œë„ˆë ˆì´í„° ìƒì„±
    train_gen = CSIDataGenerator(
        file_paths=train_files,
        batch_size=Config.BATCH_SIZE,
        window_size=Config.WINDOW_SIZE,
        stride=Config.STRIDE,
        shuffle=True,
        logger=logger
    )
    
    val_gen = CSIDataGenerator(
        file_paths=val_files,
        batch_size=Config.BATCH_SIZE,
        window_size=Config.WINDOW_SIZE,
        stride=Config.STRIDE,
        shuffle=False,  # ê²€ì¦ ë°ì´í„°ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ
        logger=logger
    )
    
    test_gen = CSIDataGenerator(
        file_paths=test_files,
        batch_size=Config.BATCH_SIZE,
        window_size=Config.WINDOW_SIZE,
        stride=Config.STRIDE,
        shuffle=False,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ
        logger=logger
    )
    
    return train_gen, val_gen, test_gen


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª CSI ë°ì´í„° ì œë„ˆë ˆì´í„° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        # ì œë„ˆë ˆì´í„° ìƒì„±
        train_gen, val_gen, test_gen = create_data_generators()
        
        # í†µê³„ ì¶œë ¥
        print("\nğŸ“Š í›ˆë ¨ ë°ì´í„°:")
        train_gen.print_statistics()
        
        print("\nğŸ“Š ê²€ì¦ ë°ì´í„°:")
        val_gen.print_statistics()
        
        print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°:")
        test_gen.print_statistics()
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:")
        X_batch, y_batch = train_gen[0]
        print(f"   X í˜•íƒœ: {X_batch.shape}")
        print(f"   y í˜•íƒœ: {y_batch.shape}")
        print(f"   y ë¶„í¬: {np.bincount(y_batch)}")
        
        print("\nâœ… ë°ì´í„° ì œë„ˆë ˆì´í„° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
