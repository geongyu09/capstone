"""
CSI ë‚™ìƒ ê°ì§€ v4 - ëª¨ë¸ ì•„í‚¤í…ì²˜
CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, 
    BatchNormalization, GlobalAveragePooling1D, Concatenate,
    TimeDistributed, Bidirectional
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from typing import Tuple, Dict, Any
import numpy as np

from config import Config, ModelConfig
from utils import setup_logging


# TensorFlow 2.x í˜¸í™˜ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë“¤
def weighted_binary_crossentropy_factory(class_weights):
    """
    í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ binary crossentropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        class_weights: {0: weight_for_class_0, 1: weight_for_class_1} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜
    """
    def loss_function(y_true, y_pred):
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        weight_0 = class_weights.get(0, 1.0)
        weight_1 = class_weights.get(1, 1.0)
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weights = y_true * weight_1 + (1 - y_true) * weight_0
        
        # Binary crossentropy with weights
        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
        return bce * weights
    
    # í•¨ìˆ˜ì— ì´ë¦„ ì†ì„± ì¶”ê°€ (ì €ì¥/ë¡œë“œë¥¼ ìœ„í•´)
    loss_function.__name__ = 'weighted_binary_crossentropy'
    return loss_function


def simple_weighted_binary_crossentropy(y_true, y_pred):
    """
    ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ì ìš© binary crossentropy
    ê¸°ë³¸ì ìœ¼ë¡œ í´ë˜ìŠ¤ 1ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì ìš© (ë‚™ìƒ ê°ì§€ë¥¼ ìœ„í•´)
    """
    # ë‚™ìƒ(1)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš©
    class_weight_1 = 2.0  # ë‚™ìƒ í´ë˜ìŠ¤ì— 2ë°° ê°€ì¤‘ì¹˜
    class_weight_0 = 1.0  # ì •ìƒ í´ë˜ìŠ¤
    
    weights = y_true * class_weight_1 + (1 - y_true) * class_weight_0
    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    return bce * weights


# ì „ì—­ ì»¤ìŠ¤í…€ ê°ì²´ ë”•ì…”ë„ˆë¦¬ (ëª¨ë¸ ë¡œë“œì‹œ ì‚¬ìš©)
CUSTOM_OBJECTS = {
    'weighted_binary_crossentropy': simple_weighted_binary_crossentropy,
    'simple_weighted_binary_crossentropy': simple_weighted_binary_crossentropy
}


class CSIFallDetectionModel:
    """CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸ ë¹Œë”"""
    
    def __init__(self, 
                 input_shape: Tuple[int, int],
                 model_config: Dict[str, Any] = None,
                 logger=None):
        """
        Args:
            input_shape: (window_size, n_features) í˜•íƒœ
            model_config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            logger: ë¡œê±° ê°ì²´
        """
        self.input_shape = input_shape
        self.model_config = model_config or self._get_default_config()
        self.logger = logger or setup_logging()
        self.model = None
        
        self.logger.info(f"ğŸ§  ëª¨ë¸ ë¹Œë” ì´ˆê¸°í™”")
        self.logger.info(f"   ì…ë ¥ í˜•íƒœ: {input_shape}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë¸ ì„¤ì •"""
        return {
            'cnn_filters': ModelConfig.CNN_FILTERS,
            'cnn_kernel_size': ModelConfig.CNN_KERNEL_SIZE,
            'cnn_dropout': ModelConfig.CNN_DROPOUT,
            'lstm_units': ModelConfig.LSTM_UNITS,
            'lstm_dropout': ModelConfig.LSTM_DROPOUT,
            'lstm_recurrent_dropout': ModelConfig.LSTM_RECURRENT_DROPOUT,
            'dense_units': ModelConfig.DENSE_UNITS,
            'dense_dropout': ModelConfig.DENSE_DROPOUT,
            'output_activation': ModelConfig.OUTPUT_ACTIVATION
        }
    
    def build_cnn_lstm_model(self) -> Model:
        """CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•"""
        
        # ì…ë ¥ ë ˆì´ì–´
        inputs = Input(shape=self.input_shape, name='input')
        
        # CNN ë¸Œëœì¹˜
        cnn_branch = self._build_cnn_branch(inputs)
        
        # LSTM ë¸Œëœì¹˜
        lstm_branch = self._build_lstm_branch(inputs)
        
        # ë¸Œëœì¹˜ ê²°í•©
        if cnn_branch is not None and lstm_branch is not None:
            combined = Concatenate(name='concat')([cnn_branch, lstm_branch])
        elif cnn_branch is not None:
            combined = cnn_branch
        else:
            combined = lstm_branch
        
        # ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´
        outputs = self._build_classifier(combined)
        
        # ëª¨ë¸ ìƒì„±
        model = Model(inputs=inputs, outputs=outputs, name='CSI_Fall_Detection')
        
        self.model = model
        self.logger.info(f"âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        
        return model
    
    def _build_cnn_branch(self, inputs) -> tf.Tensor:
        """CNN ë¸Œëœì¹˜ êµ¬ì¶•"""
        x = inputs
        
        # CNN ë ˆì´ì–´ë“¤
        for i, filters in enumerate(self.model_config['cnn_filters']):
            x = Conv1D(
                filters=filters,
                kernel_size=self.model_config['cnn_kernel_size'],
                activation='relu',
                padding='same',
                name=f'cnn_conv_{i+1}'
            )(x)
            
            x = BatchNormalization(name=f'cnn_bn_{i+1}')(x)
            
            x = MaxPooling1D(
                pool_size=2,
                name=f'cnn_pool_{i+1}'
            )(x)
            
            x = Dropout(
                self.model_config['cnn_dropout'],
                name=f'cnn_dropout_{i+1}'
            )(x)
        
        # Global Average Pooling
        cnn_output = GlobalAveragePooling1D(name='cnn_gap')(x)
        
        return cnn_output
    
    def _build_lstm_branch(self, inputs) -> tf.Tensor:
        """LSTM ë¸Œëœì¹˜ êµ¬ì¶•"""
        x = inputs
        
        # LSTM ë ˆì´ì–´ë“¤
        for i, units in enumerate(self.model_config['lstm_units']):
            return_sequences = (i < len(self.model_config['lstm_units']) - 1)
            
            x = Bidirectional(
                LSTM(
                    units=units,
                    return_sequences=return_sequences,
                    dropout=self.model_config['lstm_dropout'],
                    recurrent_dropout=self.model_config['lstm_recurrent_dropout'],
                    name=f'lstm_{i+1}'
                ),
                name=f'bi_lstm_{i+1}'
            )(x)
            
            if return_sequences:
                x = BatchNormalization(name=f'lstm_bn_{i+1}')(x)
        
        return x
    
    def _build_classifier(self, features) -> tf.Tensor:
        """ë¶„ë¥˜ê¸° êµ¬ì¶•"""
        x = features
        
        # Dense ë ˆì´ì–´ë“¤
        for i, units in enumerate(self.model_config['dense_units']):
            x = Dense(
                units=units,
                activation='relu',
                name=f'dense_{i+1}'
            )(x)
            
            x = BatchNormalization(name=f'dense_bn_{i+1}')(x)
            
            x = Dropout(
                self.model_config['dense_dropout'],
                name=f'dense_dropout_{i+1}'
            )(x)
        
        # ì¶œë ¥ ë ˆì´ì–´
        outputs = Dense(
            units=1,
            activation=self.model_config['output_activation'],
            name='output'
        )(x)
        
        return outputs
    
    def compile_model(self, 
                     learning_rate: float = Config.LEARNING_RATE,
                     class_weights: Dict[int, float] = None) -> None:
        """ëª¨ë¸ ì»´íŒŒì¼"""
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ì•„ì§ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_model()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = Adam(learning_rate=learning_rate)
        
        # ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤)
        if class_weights:
            # íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
            loss = weighted_binary_crossentropy_factory(class_weights)
            loss_name = "weighted_binary_crossentropy"
        else:
            # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ì ìš© í•¨ìˆ˜ ì‚¬ìš©
            loss = simple_weighted_binary_crossentropy
            loss_name = "simple_weighted_binary_crossentropy"
        
        # ë©”íŠ¸ë¦­
        metrics = [
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ]
        
        self.model.compile(
            optimizer=optimizer,
            loss=loss,
            metrics=metrics
        )
        
        self.logger.info(f"âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ")
        self.logger.info(f"   ì˜µí‹°ë§ˆì´ì €: Adam (lr={learning_rate})")
        self.logger.info(f"   ì†ì‹¤ í•¨ìˆ˜: {loss_name}")
        self.logger.info(f"   ë©”íŠ¸ë¦­: {[m.name if hasattr(m, 'name') else str(m) for m in metrics]}")
    
    def get_callbacks(self, 
                     model_name: str,
                     monitor: str = 'val_loss',
                     patience: int = 10) -> list:
        """í•™ìŠµ ì½œë°± ìƒì„±"""
        
        callbacks = [
            # Early Stopping
            EarlyStopping(
                monitor=monitor,
                patience=patience,
                restore_best_weights=True,
                verbose=1
            ),
            
            # Model Checkpoint
            ModelCheckpoint(
                filepath=f"{Config.MODEL_DIR}/{model_name}_best.keras",
                monitor=monitor,
                save_best_only=True,
                verbose=1
            ),
            
            # Learning Rate Reduction
            ReduceLROnPlateau(
                monitor=monitor,
                factor=0.5,
                patience=patience//2,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        return callbacks
    
    def print_model_summary(self) -> None:
        """ëª¨ë¸ ìš”ì•½ ì¶œë ¥"""
        if self.model is None:
            self.logger.warning("ëª¨ë¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ ìš”ì•½")
        print("=" * 50)
        self.model.summary()
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = self.model.count_params()
        trainable_params = sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])
        non_trainable_params = total_params - trainable_params
        
        print(f"\nğŸ“Š íŒŒë¼ë¯¸í„° ì •ë³´:")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")
        print(f"   ê³ ì •: {non_trainable_params:,}")
    
    def create_simple_model(self) -> Model:
        """ê°„ë‹¨í•œ baseline ëª¨ë¸"""
        inputs = Input(shape=self.input_shape, name='input')
        
        # ê°„ë‹¨í•œ LSTM
        x = LSTM(64, dropout=0.3, recurrent_dropout=0.3)(inputs)
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs=inputs, outputs=outputs, name='Simple_CSI_Model')
        self.model = model
        
        return model
    
    def create_cnn_only_model(self) -> Model:
        """CNNë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸"""
        inputs = Input(shape=self.input_shape, name='input')
        
        x = inputs
        for i, filters in enumerate([64, 128, 256]):
            x = Conv1D(filters, 3, activation='relu', padding='same')(x)
            x = BatchNormalization()(x)
            x = MaxPooling1D(2)(x)
            x = Dropout(0.3)(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs=inputs, outputs=outputs, name='CNN_Only_Model')
        self.model = model
        
        return model


def create_model(model_type: str = 'hybrid',
                input_shape: Tuple[int, int] = None,
                learning_rate: float = Config.LEARNING_RATE,
                class_weights: Dict[int, float] = None) -> Tuple[Model, CSIFallDetectionModel]:
    """ëª¨ë¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    
    if input_shape is None:
        input_shape = (Config.WINDOW_SIZE, Config.TOTAL_FEATURES)
    
    logger = setup_logging()
    
    # ëª¨ë¸ ë¹Œë” ìƒì„±
    model_builder = CSIFallDetectionModel(input_shape=input_shape, logger=logger)
    
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ êµ¬ì¶•
    if model_type == 'hybrid':
        model = model_builder.build_cnn_lstm_model()
    elif model_type == 'simple':
        model = model_builder.create_simple_model()
    elif model_type == 'cnn':
        model = model_builder.create_cnn_only_model()
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model_builder.compile_model(learning_rate=learning_rate, class_weights=class_weights)
    
    logger.info(f"âœ… {model_type} ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    return model, model_builder


# ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ê°ì²´ getter í•¨ìˆ˜
def get_custom_objects():
    """ëª¨ë¸ ë¡œë“œì‹œ ì‚¬ìš©í•  ì»¤ìŠ¤í…€ ê°ì²´ë“¤ ë°˜í™˜"""
    return CUSTOM_OBJECTS.copy()


if __name__ == "__main__":
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª CSI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ì…ë ¥ í˜•íƒœ ì„¤ì •
    input_shape = (Config.WINDOW_SIZE, Config.TOTAL_FEATURES)
    print(f"ì…ë ¥ í˜•íƒœ: {input_shape}")
    
    # ë‹¤ì–‘í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model_types = ['simple', 'cnn', 'hybrid']
    
    for model_type in model_types:
        print(f"\nğŸ” {model_type.upper()} ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        
        try:
            model, builder = create_model(model_type=model_type, input_shape=input_shape)
            builder.print_model_summary()
            
            # ë”ë¯¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            dummy_input = np.random.randn(1, *input_shape)
            output = model.predict(dummy_input, verbose=0)
            print(f"âœ… ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì„±ê³µ: ì¶œë ¥ í˜•íƒœ {output.shape}")
            
        except Exception as e:
            print(f"âŒ {model_type} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nâœ… ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
