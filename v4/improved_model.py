"""
í–¥ìƒëœ CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸
ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì”ì°¨ ì—°ê²°ì„ í¬í•¨í•œ ê°œì„ ëœ ì•„í‚¤í…ì²˜
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, 
    BatchNormalization, GlobalAveragePooling1D, Concatenate,
    Bidirectional, Add, MultiHeadAttention
)
from tensorflow.keras.optimizers import Adam
import numpy as np

from config import Config, ModelConfig
from utils import setup_logging

class AttentionLayer(tf.keras.layers.Layer):
    """ì…€í”„ ì–´í…ì…˜ ë ˆì´ì–´"""
    
    def __init__(self, units, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.units = units
        self.W = tf.keras.layers.Dense(units)
        self.U = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)
        
    def build(self, input_shape):
        super(AttentionLayer, self).build(input_shape)
        
    def call(self, inputs):
        # inputs: (batch_size, sequence_length, features)
        
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        score = self.V(tf.nn.tanh(self.W(inputs)))
        attention_weights = tf.nn.softmax(score, axis=1)
        
        # ê°€ì¤‘ í•©
        context_vector = tf.reduce_sum(attention_weights * inputs, axis=1)
        
        return context_vector, attention_weights
    
    def get_config(self):
        config = super(AttentionLayer, self).get_config()
        config.update({'units': self.units})
        return config

def focal_loss(alpha=0.25, gamma=2.0):
    """
    Focal Loss - ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘
    í´ë˜ìŠ¤ ë¶ˆê· í˜•ê³¼ ì–´ë ¤ìš´ ìƒ˜í”Œ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°
    """
    def loss_function(y_true, y_pred):
        # í´ë¦¬í•‘ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        
        # Focal Loss ê³„ì‚°
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_loss = -alpha_t * tf.pow((1 - p_t), gamma) * tf.log(p_t)
        
        return tf.reduce_mean(focal_loss)
    
    return loss_function

def residual_block(x, filters, kernel_size=3, dropout_rate=0.3):
    """ì”ì°¨ ë¸”ë¡"""
    shortcut = x
    
    # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
    x = Conv1D(filters, kernel_size, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
    x = Conv1D(filters, kernel_size, padding='same')(x)
    x = BatchNormalization()(x)
    
    # ì°¨ì›ì´ ë‹¤ë¥´ë©´ shortcut ì¡°ì •
    if shortcut.shape[-1] != filters:
        shortcut = Conv1D(filters, 1, padding='same')(shortcut)
    
    # ì”ì°¨ ì—°ê²°
    x = Add()([x, shortcut])
    x = tf.keras.activations.relu(x)
    
    return x

def multi_scale_cnn_branch(inputs):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ CNN ë¸Œëœì¹˜"""
    cnn_outputs = []
    
    # ë‹¤ì–‘í•œ ì»¤ë„ í¬ê¸°ë¡œ íŠ¹ì„± ì¶”ì¶œ
    for kernel_size in [3, 5, 7]:
        x = inputs
        
        # ì²« ë²ˆì§¸ ì”ì°¨ ë¸”ë¡
        x = residual_block(x, 64, kernel_size)
        x = MaxPooling1D(2)(x)
        
        # ë‘ ë²ˆì§¸ ì”ì°¨ ë¸”ë¡
        x = residual_block(x, 128, kernel_size)
        x = MaxPooling1D(2)(x)
        
        # ì „ì—­ í‰ê·  í’€ë§
        x = GlobalAveragePooling1D()(x)
        
        cnn_outputs.append(x)
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì„± ê²°í•©
    if len(cnn_outputs) > 1:
        combined = Concatenate()(cnn_outputs)
    else:
        combined = cnn_outputs[0]
    
    return combined

def attention_lstm_branch(inputs):
    """ì–´í…ì…˜ì´ ì ìš©ëœ LSTM ë¸Œëœì¹˜"""
    
    # Bidirectional LSTM
    lstm_out = Bidirectional(
        LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)
    )(inputs)
    
    # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì ìš©
    attention_layer = MultiHeadAttention(
        num_heads=8, 
        key_dim=64,
        dropout=0.1
    )
    
    attended_output = attention_layer(lstm_out, lstm_out)
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¡œ ì‹œí€€ìŠ¤ ìš”ì•½
    attention_weights = tf.nn.softmax(
        tf.reduce_mean(attended_output, axis=-1, keepdims=True), 
        axis=1
    )
    
    context_vector = tf.reduce_sum(attention_weights * attended_output, axis=1)
    
    return context_vector

def build_improved_model(input_shape=(50, 245)):
    """í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•"""
    
    logger = setup_logging()
    logger.info("ğŸ§  í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¶• ì‹œì‘")
    
    inputs = Input(shape=input_shape, name='input')
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ CNN ë¸Œëœì¹˜
    cnn_features = multi_scale_cnn_branch(inputs)
    
    # ì–´í…ì…˜ LSTM ë¸Œëœì¹˜
    lstm_features = attention_lstm_branch(inputs)
    
    # íŠ¹ì„± ê²°í•©
    combined_features = Concatenate()([cnn_features, lstm_features])
    
    # ë¶„ë¥˜ê¸°
    x = Dense(256, activation='relu')(combined_features)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    outputs = Dense(1, activation='sigmoid', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='ImprovedCSIFallDetection')
    
    logger.info("âœ… í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¶• ì™„ë£Œ")
    
    return model

def compile_improved_model(model, learning_rate=0.001, use_focal_loss=True):
    """í–¥ìƒëœ ëª¨ë¸ ì»´íŒŒì¼"""
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = Adam(
        learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    if use_focal_loss:
        loss = focal_loss(alpha=0.25, gamma=2.0)
        loss_name = "focal_loss"
    else:
        # ê°€ì¤‘ì¹˜ ì ìš© binary crossentropy
        def weighted_bce(y_true, y_pred):
            class_weight_1 = 2.0
            class_weight_0 = 1.0
            weights = y_true * class_weight_1 + (1 - y_true) * class_weight_0
            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
            return bce * weights
        
        loss = weighted_bce
        loss_name = "weighted_binary_crossentropy"
    
    # ë©”íŠ¸ë¦­
    metrics = [
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
    
    model.compile(
        optimizer=optimizer,
        loss=loss,
        metrics=metrics
    )
    
    print(f"âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ (ì†ì‹¤ í•¨ìˆ˜: {loss_name})")
    
    return model

def create_lightweight_model(input_shape=(50, 245)):
    """ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ìœ„í•œ ê²½ëŸ‰í™” ëª¨ë¸"""
    
    inputs = Input(shape=input_shape, name='input')
    
    # ê¹Šì´ë³„ ë¶„ë¦¬ ê°€ëŠ¥í•œ ì»¨ë³¼ë£¨ì…˜
    x = tf.keras.layers.SeparableConv1D(64, 3, activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(2)(x)
    x = Dropout(0.2)(x)
    
    x = tf.keras.layers.SeparableConv1D(128, 3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling1D(2)(x)
    x = Dropout(0.2)(x)
    
    # ë‹¨ìˆœí•œ LSTM
    x = LSTM(64, dropout=0.2)(x)
    
    # ì••ì¶•ëœ ë¶„ë¥˜ê¸°
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    
    outputs = Dense(1, activation='sigmoid', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='LightweightCSIFallDetection')
    
    return model

def model_comparison_test():
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    input_shape = (Config.WINDOW_SIZE, Config.TOTAL_FEATURES)
    
    models = {
        'Improved': build_improved_model(input_shape),
        'Lightweight': create_lightweight_model(input_shape)
    }
    
    # ëª¨ë¸ë³„ ì •ë³´ ì¶œë ¥
    for name, model in models.items():
        print(f"\nğŸ“Š {name} ëª¨ë¸:")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì¶”ë¡  ì‹œê°„ í…ŒìŠ¤íŠ¸
        dummy_input = np.random.randn(1, *input_shape)
        
        import time
        start_time = time.time()
        for _ in range(100):  # 100ë²ˆ ì¶”ë¡ 
            _ = model.predict(dummy_input, verbose=0)
        inference_time = (time.time() - start_time) / 100
        
        print(f"   ì¶”ë¡  ì‹œê°„: {inference_time*1000:.2f}ms")
        print(f"   ëª¨ë¸ í¬ê¸° (ì¶”ì •): {model.count_params() * 4 / 1024 / 1024:.1f}MB")
    
    return models

def train_improved_model(model_name="improved_csi_model"):
    """í–¥ìƒëœ ëª¨ë¸ í•™ìŠµ"""
    
    from utils import create_timestamp
    
    print("ğŸš€ í–¥ìƒëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    
    # í–¥ìƒëœ ëª¨ë¸ ìƒì„±
    model = build_improved_model()
    model = compile_improved_model(model, use_focal_loss=True)
    
    # ëª¨ë¸ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“‹ ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    model.summary()
    
    # í•™ìŠµ ì„¤ì •
    experiment_name = f"{model_name}_{create_timestamp()}"
    
    # í•™ìŠµ ì‹¤í–‰ (ê¸°ì¡´ trainer ìˆ˜ì • í•„ìš”)
    print(f"\nâš ï¸ ì£¼ì˜: ê¸°ì¡´ trainer.pyë¥¼ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ê±°ë‚˜")
    print(f"ìƒˆë¡œìš´ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
    print(f"ì‹¤í—˜ ì´ë¦„: {experiment_name}")
    
    return model, experiment_name

if __name__ == "__main__":
    print("ğŸ§  í–¥ìƒëœ CSI ë‚™ìƒ ê°ì§€ ëª¨ë¸")
    print("=" * 50)
    
    try:
        # ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
        models = model_comparison_test()
        
        # ì‚¬ìš©ì ì„ íƒ
        print("\nğŸ¤” ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print("1. í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ë§Œ í™•ì¸")
        print("2. í–¥ìƒëœ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„")
        print("3. ê²½ëŸ‰í™” ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        
        choice = input("ì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            improved_model = build_improved_model()
            print("\nğŸ“‹ í–¥ìƒëœ ëª¨ë¸ ì•„í‚¤í…ì²˜:")
            improved_model.summary()
            
        elif choice == "2":
            model, experiment_name = train_improved_model()
            print(f"\nâœ… í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ: {experiment_name}")
            
        elif choice == "3":
            lightweight_model = create_lightweight_model()
            print("\nğŸ“‹ ê²½ëŸ‰í™” ëª¨ë¸ ì•„í‚¤í…ì²˜:")
            lightweight_model.summary()
            
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì´ ì•„ë‹™ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
