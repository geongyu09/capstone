"""
CSI ë‚™ìƒ ê°ì§€ v4 ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import os
import glob
import logging
import json
import pickle
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc


def setup_logging(log_dir: str = "./logs", log_level: str = "INFO") -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    os.makedirs(log_dir, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"csi_fall_detection_{timestamp}.log")
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger("CSIFallDetection")
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # í•¸ë“¤ëŸ¬ê°€ ì´ë¯¸ ìˆë‹¤ë©´ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter(
        '%(levelname)s - %(message)s'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    logger.info(f"ë¡œê¹… ì‹œì‘ - ë¡œê·¸ íŒŒì¼: {log_file}")
    return logger


def collect_csv_files(data_paths: List[str], pattern: str = "*.csv") -> List[str]:
    """ì§€ì •ëœ ê²½ë¡œë“¤ì—ì„œ CSV íŒŒì¼ ìˆ˜ì§‘"""
    csv_files = []
    
    for path in data_paths:
        if os.path.exists(path):
            files = glob.glob(os.path.join(path, pattern))
            csv_files.extend(files)
        else:
            print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    
    return csv_files


def analyze_data_distribution(csv_files: List[str]) -> Dict[str, Any]:
    """ë°ì´í„° ë¶„í¬ ë¶„ì„"""
    print("ğŸ“Š ë°ì´í„° ë¶„í¬ ë¶„ì„ ì¤‘...")
    
    stats = {
        'total_files': len(csv_files),
        'file_sizes': [],
        'sample_counts': [],
        'label_distributions': {},
        'amplitude_stats': {},
        'file_info': []
    }
    
    for file_path in csv_files:
        try:
            # íŒŒì¼ í¬ê¸°
            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
            stats['file_sizes'].append(file_size)
            
            # ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§)
            df = pd.read_csv(file_path)
            sample_count = len(df)
            stats['sample_counts'].append(sample_count)
            
            # ë¼ë²¨ ë¶„í¬ (ìˆëŠ” ê²½ìš°)
            if 'label' in df.columns:
                label_dist = df['label'].value_counts().to_dict()
                file_name = os.path.basename(file_path)
                stats['label_distributions'][file_name] = label_dist
            
            # Amplitude ë°ì´í„° í†µê³„ (ìƒ˜í”Œë§)
            if len(df.columns) > 253:
                amplitude_data = df.iloc[:100, 8:253]  # ì²˜ìŒ 100í–‰ë§Œ ìƒ˜í”Œë§
                amp_stats = {
                    'mean': amplitude_data.values.mean(),
                    'std': amplitude_data.values.std(),
                    'min': amplitude_data.values.min(),
                    'max': amplitude_data.values.max()
                }
                stats['amplitude_stats'][os.path.basename(file_path)] = amp_stats
            
            # íŒŒì¼ ì •ë³´
            stats['file_info'].append({
                'file': os.path.basename(file_path),
                'size_mb': file_size,
                'samples': sample_count,
                'columns': len(df.columns)
            })
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path} - {e}")
    
    # ì „ì²´ í†µê³„ ê³„ì‚°
    if stats['file_sizes']:
        stats['avg_file_size_mb'] = np.mean(stats['file_sizes'])
        stats['total_samples'] = sum(stats['sample_counts'])
        stats['avg_samples_per_file'] = np.mean(stats['sample_counts'])
    
    return stats


def print_data_analysis(stats: Dict[str, Any]) -> None:
    """ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
    print("\nğŸ“ˆ ë°ì´í„° ë¶„ì„ ê²°ê³¼")
    print("=" * 50)
    
    print(f"ğŸ“ íŒŒì¼ ì •ë³´:")
    print(f"   - ì´ íŒŒì¼ ìˆ˜: {stats['total_files']:,}ê°œ")
    if stats.get('avg_file_size_mb'):
        print(f"   - í‰ê·  íŒŒì¼ í¬ê¸°: {stats['avg_file_size_mb']:.1f} MB")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {stats['total_samples']:,}ê°œ")
        print(f"   - íŒŒì¼ë‹¹ í‰ê·  ìƒ˜í”Œ: {stats['avg_samples_per_file']:.0f}ê°œ")
    
    # ë¼ë²¨ ë¶„í¬
    if stats['label_distributions']:
        print(f"\nğŸ·ï¸ ë¼ë²¨ ë¶„í¬:")
        total_fall = 0
        total_normal = 0
        for file_name, dist in stats['label_distributions'].items():
            fall_count = dist.get(1, 0)
            normal_count = dist.get(0, 0)
            total_fall += fall_count
            total_normal += normal_count
        
        total_labeled = total_fall + total_normal
        if total_labeled > 0:
            print(f"   - ë‚™ìƒ ìƒ˜í”Œ: {total_fall:,}ê°œ ({total_fall/total_labeled*100:.1f}%)")
            print(f"   - ì •ìƒ ìƒ˜í”Œ: {total_normal:,}ê°œ ({total_normal/total_labeled*100:.1f}%)")
    
    # Amplitude ë°ì´í„° í†µê³„
    if stats['amplitude_stats']:
        print(f"\nğŸ“Š Amplitude ë°ì´í„° í†µê³„:")
        all_means = [stat['mean'] for stat in stats['amplitude_stats'].values()]
        all_stds = [stat['std'] for stat in stats['amplitude_stats'].values()]
        all_mins = [stat['min'] for stat in stats['amplitude_stats'].values()]
        all_maxs = [stat['max'] for stat in stats['amplitude_stats'].values()]
        
        print(f"   - í‰ê· ê°’ ë²”ìœ„: {np.min(all_means):.3f} ~ {np.max(all_means):.3f}")
        print(f"   - í‘œì¤€í¸ì°¨ ë²”ìœ„: {np.min(all_stds):.3f} ~ {np.max(all_stds):.3f}")
        print(f"   - ìµœì†Œê°’ ë²”ìœ„: {np.min(all_mins):.3f} ~ {np.max(all_mins):.3f}")
        print(f"   - ìµœëŒ€ê°’ ë²”ìœ„: {np.min(all_maxs):.3f} ~ {np.max(all_maxs):.3f}")


def save_metadata(metadata: Dict[str, Any], file_path: str) -> None:
    """ë©”íƒ€ë°ì´í„° ì €ì¥"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)


def load_metadata(file_path: str) -> Dict[str, Any]:
    """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_model_artifacts(model, scaler, metadata: Dict[str, Any], 
                        model_dir: str, model_name: str) -> Dict[str, str]:
    """ëª¨ë¸ ê´€ë ¨ íŒŒì¼ë“¤ ì €ì¥"""
    os.makedirs(model_dir, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œë“¤
    paths = {
        'model': os.path.join(model_dir, f"{model_name}.keras"),
        'scaler': os.path.join(model_dir, f"{model_name}_scaler.pkl"),
        'metadata': os.path.join(model_dir, f"{model_name}_metadata.json")
    }
    
    # ëª¨ë¸ ì €ì¥
    model.save(paths['model'])
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    with open(paths['scaler'], 'wb') as f:
        pickle.dump(scaler, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    save_metadata(metadata, paths['metadata'])
    
    return paths


def load_model_artifacts(model_dir: str, model_name: str) -> Tuple[Any, Any, Dict[str, Any]]:
    """ëª¨ë¸ ê´€ë ¨ íŒŒì¼ë“¤ ë¡œë“œ"""
    from tensorflow.keras.models import load_model
    
    paths = {
        'model': os.path.join(model_dir, f"{model_name}.keras"),
        'scaler': os.path.join(model_dir, f"{model_name}_scaler.pkl"),
        'metadata': os.path.join(model_dir, f"{model_name}_metadata.json")
    }
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_model(paths['model'])
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    with open(paths['scaler'], 'rb') as f:
        scaler = pickle.load(f)
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata = load_metadata(paths['metadata'])
    
    return model, scaler, metadata


def plot_training_history(history: Dict[str, List[float]], 
                         save_path: Optional[str] = None) -> None:
    """í•™ìŠµ íˆìŠ¤í† ë¦¬ í”Œë¡¯"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Loss
    axes[0, 0].plot(history['loss'], label='Training Loss')
    if 'val_loss' in history:
        axes[0, 0].plot(history['val_loss'], label='Validation Loss')
    axes[0, 0].set_title('Model Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Accuracy
    if 'accuracy' in history:
        axes[0, 1].plot(history['accuracy'], label='Training Accuracy')
        if 'val_accuracy' in history:
            axes[0, 1].plot(history['val_accuracy'], label='Validation Accuracy')
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
    
    # Precision
    if 'precision' in history:
        axes[1, 0].plot(history['precision'], label='Training Precision')
        if 'val_precision' in history:
            axes[1, 0].plot(history['val_precision'], label='Validation Precision')
        axes[1, 0].set_title('Model Precision')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
    
    # Recall
    if 'recall' in history:
        axes[1, 1].plot(history['recall'], label='Training Recall')
        if 'val_recall' in history:
            axes[1, 1].plot(history['val_recall'], label='Validation Recall')
        axes[1, 1].set_title('Model Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {save_path}")
    
    plt.show()


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, 
                         labels: List[str] = None,
                         save_path: Optional[str] = None) -> None:
    """í˜¼ë™ í–‰ë ¬ í”Œë¡¯"""
    if labels is None:
        labels = ['Normal', 'Fall']
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š í˜¼ë™ í–‰ë ¬ ì €ì¥: {save_path}")
    
    plt.show()


def plot_roc_curve(y_true: np.ndarray, y_scores: np.ndarray,
                  save_path: Optional[str] = None) -> float:
    """ROC ì»¤ë¸Œ í”Œë¡¯"""
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(True)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ROC ì»¤ë¸Œ ì €ì¥: {save_path}")
    
    plt.show()
    
    return roc_auc


def evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray,
                  threshold: float = 0.5) -> Dict[str, Any]:
    """ëª¨ë¸ í‰ê°€"""
    # ì˜ˆì¸¡
    y_scores = model.predict(X_test)
    y_pred = (y_scores > threshold).astype(int)
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test, y_pred)
    
    # ROC AUC
    roc_auc = auc(*roc_curve(y_test, y_scores)[:2])
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'roc_auc': roc_auc,
        'accuracy': report['accuracy'],
        'precision': report['1']['precision'],
        'recall': report['1']['recall'],
        'f1_score': report['1']['f1-score'],
        'threshold': threshold,
        'predictions': {
            'y_true': y_test.tolist(),
            'y_scores': y_scores.flatten().tolist(),
            'y_pred': y_pred.flatten().tolist()
        }
    }
    
    return results


def print_evaluation_results(results: Dict[str, Any]) -> None:
    """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼")
    print("=" * 50)
    
    print(f"ğŸ¯ ì „ì²´ ì„±ëŠ¥:")
    print(f"   - ì •í™•ë„: {results['accuracy']:.4f}")
    print(f"   - ROC AUC: {results['roc_auc']:.4f}")
    
    print(f"\nğŸ” ë‚™ìƒ ê°ì§€ ì„±ëŠ¥:")
    print(f"   - ì •ë°€ë„: {results['precision']:.4f}")
    print(f"   - ì¬í˜„ìœ¨: {results['recall']:.4f}")
    print(f"   - F1 ì ìˆ˜: {results['f1_score']:.4f}")
    
    print(f"\nğŸ“‹ í˜¼ë™ í–‰ë ¬:")
    cm = np.array(results['confusion_matrix'])
    print(f"   TN: {cm[0,0]}, FP: {cm[0,1]}")
    print(f"   FN: {cm[1,0]}, TP: {cm[1,1]}")
    
    print(f"\nâš™ï¸ ì„ê³„ê°’: {results['threshold']}")


def create_timestamp() -> str:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def calculate_class_weights(y: np.ndarray) -> Dict[int, float]:
    """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    from sklearn.utils.class_weight import compute_class_weight
    
    classes = np.unique(y)
    weights = compute_class_weight('balanced', classes=classes, y=y)
    
    return dict(zip(classes, weights))


def split_data_by_files(file_paths: List[str], 
                       train_ratio: float = 0.7,
                       val_ratio: float = 0.15,
                       test_ratio: float = 0.15,
                       random_seed: int = 42) -> Tuple[List[str], List[str], List[str]]:
    """íŒŒì¼ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¶„í• """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        "ë¹„ìœ¨ì˜ í•©ì´ 1ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    np.random.seed(random_seed)
    
    # íŒŒì¼ë“¤ì„ ì…”í”Œ
    shuffled_files = file_paths.copy()
    np.random.shuffle(shuffled_files)
    
    n_files = len(shuffled_files)
    n_train = int(n_files * train_ratio)
    n_val = int(n_files * val_ratio)
    
    train_files = shuffled_files[:n_train]
    val_files = shuffled_files[n_train:n_train + n_val]
    test_files = shuffled_files[n_train + n_val:]
    
    return train_files, val_files, test_files


def memory_usage_mb() -> float:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB) ë°˜í™˜"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def format_time(seconds: float) -> str:
    """ì´ˆë¥¼ ì‹œ:ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = int(seconds % 60)
    
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    else:
        return f"{minutes:02d}:{seconds:02d}"


if __name__ == "__main__":
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # ë¡œê¹… í…ŒìŠ¤íŠ¸
    logger = setup_logging()
    logger.info("ë¡œê¹… í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í…ŒìŠ¤íŠ¸
    timestamp = create_timestamp()
    print(f"í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
    memory = memory_usage_mb()
    print(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory:.1f} MB")
    
    # ì‹œê°„ í¬ë§· í…ŒìŠ¤íŠ¸
    test_seconds = 3661
    formatted_time = format_time(test_seconds)
    print(f"ì‹œê°„ í¬ë§· í…ŒìŠ¤íŠ¸: {test_seconds}ì´ˆ -> {formatted_time}")
    
    print("âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
