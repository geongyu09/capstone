# multi_file_trainer.py
import pandas as pd
import numpy as np
import glob
import os
import pickle
import json
import logging
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

# ì „ì—­ ì„¤ì •ê°’
DEFAULT_TEST_CSV = 'case32.csv'         # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ CSV íŒŒì¼
DEFAULT_DATA_DIR = './capstone/csi_data'         # ê¸°ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬
DEFAULT_WINDOW_SIZE = 30                # ê¸°ë³¸ ìœˆë„ìš° í¬ê¸° (í…ŒìŠ¤íŠ¸ìš©)
DEFAULT_STRIDE = 5                      # ê¸°ë³¸ ìŠ¤íŠ¸ë¼ì´ë“œ
DEFAULT_OVERLAP_THRESHOLD = 0.3         # ê¸°ë³¸ ì„ê³„ê°’
DEFAULT_TARGET_FEATURES = 32            # ê¸°ë³¸ íŠ¹ì§• ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
DEFAULT_EPOCHS = 5                      # ê¸°ë³¸ ì—í¬í¬ (í…ŒìŠ¤íŠ¸ìš©)
DEFAULT_BATCH_SIZE = 16                 # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°

# ì¶”ê°€ í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ (ìë™ íƒì§€ìš©)
POSSIBLE_TEST_FILES = [
    'case32.csv',
    '32_labeled.csv', 
    'data.csv',
    'test.csv'
]

# ì¶”ê°€ ë°ì´í„° ë””ë ‰í† ë¦¬ë“¤ (ìë™ íƒì§€ìš©)
POSSIBLE_DATA_DIRS = [
    './csi_data',
    '../csi_data',
    './data',
    '../data',
    './csv_data',
    '../csv_data'
]

class MultiFileCSITrainer:
    """100ê°œ ì´ìƒì˜ CSI íŒŒì¼ë¡œ ëŒ€ê·œëª¨ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=50, stride=5, overlap_threshold=0.3):
        self.window_size = window_size
        self.stride = stride
        self.overlap_threshold = overlap_threshold
        self.scaler = RobustScaler()
        self.feature_selector = None
        self.model = None
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ìƒíƒœ ì €ì¥ìš©
        self.training_stats = {}
    
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        log_filename = f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸš€ CSI í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘")
        self.logger.info(f"ğŸ“‹ ì„¤ì •: ìœˆë„ìš°={self.window_size}, ìŠ¤íŠ¸ë¼ì´ë“œ={self.stride}, ì„ê³„ê°’={self.overlap_threshold}")
    
    def analyze_single_file(self, file_path):
        """ë‹¨ì¼ CSV íŒŒì¼ ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)"""
        print(f"\nğŸ“ íŒŒì¼ ë¶„ì„: {os.path.basename(file_path)}")
        
        try:
            # CSV ì½ê¸°
            df = pd.read_csv(file_path)
            print(f"   ğŸ“Š íŒŒì¼ í˜•íƒœ: {df.shape}")
            print(f"   ğŸ“‹ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            
            # ì»¬ëŸ¼ í™•ì¸
            print(f"   ğŸ“ ì£¼ìš” ì»¬ëŸ¼:")
            if 'timestamp' in df.columns:
                print(f"      âœ… timestamp: {df['timestamp'].dtype}")
            else:
                print(f"      âŒ timestamp ì»¬ëŸ¼ ì—†ìŒ")
            
            if 'label' in df.columns:
                label_counts = df['label'].value_counts().sort_index()
                print(f"      âœ… label: {dict(label_counts)}")
            else:
                print(f"      âŒ label ì»¬ëŸ¼ ì—†ìŒ")
            
            # íŠ¹ì§• ì»¬ëŸ¼ í™•ì¸
            feature_cols = [col for col in df.columns if col.startswith('feat_')]
            print(f"      ğŸ“ˆ íŠ¹ì§• ì»¬ëŸ¼: {len(feature_cols)}ê°œ")
            
            if len(feature_cols) > 0:
                # íŠ¹ì§• í†µê³„
                X = df[feature_cols].values
                non_zero_features = np.sum(np.any(X != 0, axis=0))
                print(f"      ğŸ“Š í™œì„± íŠ¹ì§•: {non_zero_features}/{len(feature_cols)}ê°œ")
                
                # ì¼ë¶€ íŠ¹ì§• ê°’ í™•ì¸
                sample_features = feature_cols[:5] + feature_cols[-5:]
                print(f"      ğŸ” ìƒ˜í”Œ íŠ¹ì§•ê°’:")
                for feat in sample_features:
                    values = df[feat].values
                    print(f"         {feat}: [{values.min():.2f}, {values.max():.2f}], í‰ê·  {values.mean():.2f}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return False
    
    def load_multiple_csv_files(self, data_directory, file_pattern="*.csv", max_files=None):
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ"""
        self.logger.info(f"ğŸ“ ë‹¤ì¤‘ íŒŒì¼ ë¡œë”© ì‹œì‘: {data_directory}")
        
        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        if os.path.isfile(data_directory):
            # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°
            csv_files = [data_directory]
        else:
            # ë””ë ‰í† ë¦¬ì¸ ê²½ìš°
            file_pattern_path = os.path.join(data_directory, "**", file_pattern)
            csv_files = glob.glob(file_pattern_path, recursive=True)
        
        if max_files:
            csv_files = csv_files[:max_files]
        
        self.logger.info(f"   ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
        
        if len(csv_files) == 0:
            raise ValueError(f"âŒ {data_directory}ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        # íŒŒì¼ ëª©ë¡ ì¶œë ¥
        for i, file_path in enumerate(csv_files[:5]):  # ì²˜ìŒ 5ê°œë§Œ
            print(f"   {i+1}. {os.path.basename(file_path)}")
        if len(csv_files) > 5:
            print(f"   ... ì™¸ {len(csv_files)-5}ê°œ íŒŒì¼")
        
        all_data = []
        all_labels = []
        file_info = []
        successful_files = 0
        failed_files = 0
        
        for i, file_path in enumerate(csv_files):
            try:
                self.logger.info(f"   ë¡œë”© ì¤‘ ({i+1}/{len(csv_files)}): {os.path.basename(file_path)}")
                
                # CSV íŒŒì¼ ì½ê¸°
                df = pd.read_csv(file_path)
                
                # ê¸°ë³¸ ê²€ì¦
                required_cols = ['timestamp', 'label']
                missing_cols = [col for col in required_cols if col not in df.columns]
                if missing_cols:
                    self.logger.warning(f"   âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ {missing_cols}: {file_path}")
                    failed_files += 1
                    continue
                
                # íŠ¹ì§• ì»¬ëŸ¼ ì¶”ì¶œ
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                if len(feature_cols) == 0:
                    self.logger.warning(f"   âš ï¸ íŠ¹ì§• ì»¬ëŸ¼ ì—†ìŒ: {file_path}")
                    failed_files += 1
                    continue
                
                # ë°ì´í„° ì¶”ì¶œ
                X_file = df[feature_cols].values
                y_file = df['label'].values
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
                if len(X_file) < self.window_size:
                    self.logger.warning(f"   âš ï¸ ë°ì´í„° ë¶€ì¡± (< {self.window_size}): {file_path}")
                    failed_files += 1
                    continue
                
                # ë¼ë²¨ ë¶„í¬ í™•ì¸
                unique_labels = np.unique(y_file)
                label_counts = np.bincount(y_file, minlength=2)
                fall_ratio = label_counts[1] / len(y_file) if len(y_file) > 0 else 0
                
                all_data.append(X_file)
                all_labels.append(y_file)
                
                file_info.append({
                    'filename': os.path.basename(file_path),
                    'path': file_path,
                    'samples': len(X_file),
                    'features': len(feature_cols),
                    'fall_ratio': fall_ratio,
                    'normal_count': int(label_counts[0]),
                    'fall_count': int(label_counts[1])
                })
                
                successful_files += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if i % 10 == 0 and i > 0:
                    total_samples = sum(len(data) for data in all_data)
                    self.logger.info(f"   ğŸ“Š ì¤‘ê°„ í†µê³„: {successful_files}ê°œ íŒŒì¼, {total_samples:,}ê°œ ìƒ˜í”Œ")
                
            except Exception as e:
                self.logger.error(f"   âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                failed_files += 1
                continue
        
        if len(all_data) == 0:
            raise ValueError("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ëª¨ë“  ë°ì´í„° ê²°í•©
        self.logger.info("ğŸ”„ ë°ì´í„° ê²°í•© ì¤‘...")
        X_combined = np.vstack(all_data)
        y_combined = np.hstack(all_labels)
        
        # ìµœì¢… í†µê³„
        total_samples = len(X_combined)
        total_features = X_combined.shape[1]
        fall_samples = np.sum(y_combined == 1)
        normal_samples = np.sum(y_combined == 0)
        
        self.training_stats.update({
            'total_files': len(csv_files),
            'successful_files': successful_files,
            'failed_files': failed_files,
            'total_samples': total_samples,
            'total_features': total_features,
            'fall_samples': fall_samples,
            'normal_samples': normal_samples,
            'imbalance_ratio': normal_samples / max(fall_samples, 1)
        })
        
        self.logger.info(f"âœ… ë¡œë”© ì™„ë£Œ!")
        self.logger.info(f"   ì„±ê³µ: {successful_files}ê°œ, ì‹¤íŒ¨: {failed_files}ê°œ")
        self.logger.info(f"   ì´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
        self.logger.info(f"   ì´ íŠ¹ì§•: {total_features}ê°œ")
        self.logger.info(f"   ë¼ë²¨ ë¶„í¬: ì •ìƒ {normal_samples:,}ê°œ, ë‚™ìƒ {fall_samples:,}ê°œ")
        self.logger.info(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {normal_samples/max(fall_samples,1):.1f}:1")
        
        return X_combined, y_combined, file_info
    
    def smart_feature_selection(self, X, y, target_features=64):
        """ìŠ¤ë§ˆíŠ¸ íŠ¹ì§• ì„ íƒ"""
        self.logger.info("ğŸ¯ íŠ¹ì§• ì„ íƒ ì‹œì‘...")
        
        original_features = X.shape[1]
        self.logger.info(f"   ì›ë³¸ íŠ¹ì§• ìˆ˜: {original_features}")
        
        # 1ë‹¨ê³„: ë¶„ì‚°ì´ ê±°ì˜ 0ì¸ íŠ¹ì§• ì œê±°
        self.logger.info("   1ï¸âƒ£ ì €ë¶„ì‚° íŠ¹ì§• ì œê±°...")
        variance_threshold = VarianceThreshold(threshold=0.01)
        X_var = variance_threshold.fit_transform(X)
        
        removed_by_variance = original_features - X_var.shape[1]
        self.logger.info(f"      ì œê±°ëœ íŠ¹ì§•: {removed_by_variance}ê°œ")
        self.logger.info(f"      ë‚¨ì€ íŠ¹ì§•: {X_var.shape[1]}ê°œ")
        
        # 2ë‹¨ê³„: í†µê³„ì  íŠ¹ì§• ì„ íƒ (í•„ìš”í•œ ê²½ìš°)
        if X_var.shape[1] > target_features:
            self.logger.info(f"   2ï¸âƒ£ ìƒìœ„ {target_features}ê°œ íŠ¹ì§• ì„ íƒ...")
            
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìƒ˜í”Œë§ (ë°ì´í„°ê°€ í° ê²½ìš°)
            if len(X_var) > 30000:
                sample_size = 30000
                sample_indices = np.random.choice(len(X_var), sample_size, replace=False)
                X_sample = X_var[sample_indices]
                y_sample = y[sample_indices]
                self.logger.info(f"      ìƒ˜í”Œë§: {len(X_sample):,}ê°œë¡œ íŠ¹ì§• ì„ íƒ ìˆ˜í–‰")
            else:
                X_sample = X_var
                y_sample = y
            
            # F-í†µê³„ëŸ‰ ê¸°ë°˜ íŠ¹ì§• ì„ íƒ
            k_selector = SelectKBest(f_classif, k=target_features)
            X_selected = k_selector.fit_transform(X_sample, y_sample)
            
            # ì „ì²´ ë°ì´í„°ì— ì ìš©
            X_final = k_selector.transform(X_var)
            
            self.logger.info(f"      ìµœì¢… ì„ íƒ: {X_final.shape[1]}ê°œ")
            
            # ì„ íƒê¸° ì €ì¥
            self.feature_selector = {
                'variance_selector': variance_threshold,
                'k_selector': k_selector,
                'selected_indices': k_selector.get_support(indices=True)
            }
        else:
            X_final = X_var
            self.feature_selector = {
                'variance_selector': variance_threshold,
                'k_selector': None,
                'selected_indices': np.arange(X_var.shape[1])
            }
        
        self.logger.info(f"âœ… íŠ¹ì§• ì„ íƒ ì™„ë£Œ: {original_features} â†’ {X_final.shape[1]}ê°œ")
        return X_final
    
    def create_sequences(self, X, y):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
        self.logger.info(f"â° ì‹œí€€ìŠ¤ ìƒì„± ì‹œì‘...")
        self.logger.info(f"   ì„¤ì •: ìœˆë„ìš°={self.window_size}, ìŠ¤íŠ¸ë¼ì´ë“œ={self.stride}")
        
        sequences = []
        labels = []
        
        total_possible = (len(X) - self.window_size) // self.stride + 1
        self.logger.info(f"   ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: ìµœëŒ€ {total_possible:,}ê°œ")
        
        for i in range(0, len(X) - self.window_size + 1, self.stride):
            # ìœˆë„ìš° ì¶”ì¶œ
            window_X = X[i:i + self.window_size]
            window_y = y[i:i + self.window_size]
            
            # ìŠ¤ë§ˆíŠ¸ ë¼ë²¨ë§
            fall_ratio = np.sum(window_y == 1) / len(window_y)
            
            if fall_ratio >= self.overlap_threshold:
                # ì¶©ë¶„í•œ ë‚™ìƒ ë¹„ìœ¨
                sequence_label = 1
            elif fall_ratio > 0:
                # ì¼ë¶€ ë‚™ìƒì´ ìˆëŠ” ê²½ìš° - ìœ„ì¹˜ ê³ ë ¤
                fall_indices = np.where(window_y == 1)[0]
                
                # ë‚™ìƒì´ ìœˆë„ìš° í›„ë°˜ë¶€(70% ì´í›„) ë˜ëŠ” ì „ë°˜ë¶€(30% ì´ì „)ì— ìˆìœ¼ë©´ ì¤‘ìš”
                if (len(fall_indices) > 0 and 
                    (fall_indices[-1] >= len(window_y) * 0.7 or fall_indices[0] <= len(window_y) * 0.3)):
                    sequence_label = 1
                else:
                    sequence_label = 0
            else:
                sequence_label = 0
            
            sequences.append(window_X)
            labels.append(sequence_label)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if len(sequences) % 5000 == 0:
                self.logger.info(f"   ì§„í–‰: {len(sequences):,}ê°œ ì‹œí€€ìŠ¤ ìƒì„±ë¨")
        
        X_seq = np.array(sequences)
        y_seq = np.array(labels)
        
        # ë¼ë²¨ ë¶„í¬
        unique_labels, counts = np.unique(y_seq, return_counts=True)
        self.logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X_seq.shape[0]:,}ê°œ")
        
        for label, count in zip(unique_labels, counts):
            label_name = "ë‚™ìƒ" if label == 1 else "ì •ìƒ"
            percentage = (count / len(y_seq)) * 100
            self.logger.info(f"   {label_name}: {count:,}ê°œ ({percentage:.1f}%)")
        
        return X_seq, y_seq
    
    def preprocess_data(self, X):
        """ë°ì´í„° ì „ì²˜ë¦¬ (ì •ê·œí™”)"""
        self.logger.info("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬...")
        
        # ì›ë³¸ í˜•íƒœ ì €ì¥
        original_shape = X.shape
        
        # 2Dë¡œ ë³€í™˜í•˜ì—¬ ì •ê·œí™”
        X_2d = X.reshape(-1, X.shape[-1])
        X_normalized = self.scaler.fit_transform(X_2d)
        
        # ì›ë³¸ í˜•íƒœë¡œ ë³µì›
        X_normalized = X_normalized.reshape(original_shape)
        
        self.logger.info(f"   âœ… ì •ê·œí™” ì™„ë£Œ")
        self.logger.info(f"   ë°ì´í„° ë²”ìœ„: [{X_normalized.min():.3f}, {X_normalized.max():.3f}]")
        
        return X_normalized
    
    def build_model(self, input_shape):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        self.logger.info(f"ğŸ—ï¸ ëª¨ë¸ êµ¬ì¶•...")
        self.logger.info(f"   ì…ë ¥ í˜•íƒœ: {input_shape}")
        
        n_features = input_shape[1]
        
        # íŠ¹ì§• ìˆ˜ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        if n_features > 64:
            # ê³ ì°¨ì›ìš© ëª¨ë¸
            self.logger.info("   ğŸ“¦ ê³ ì°¨ì› íŠ¹ì§•ìš© ëª¨ë¸")
            model = Sequential([
                Dense(64, activation='relu', input_shape=input_shape, name='feature_reduction'),
                Dropout(0.3),
                BatchNormalization(),
                
                LSTM(64, return_sequences=True, name='lstm_1'),
                Dropout(0.4),
                
                LSTM(32, return_sequences=False, name='lstm_2'),
                Dropout(0.4),
                
                Dense(16, activation='relu', name='dense_1'),
                BatchNormalization(),
                Dropout(0.3),
                
                Dense(1, activation='sigmoid', name='output')
            ])
        else:
            # í‘œì¤€ ëª¨ë¸
            self.logger.info("   ğŸ›ï¸ í‘œì¤€ LSTM ëª¨ë¸")
            model = Sequential([
                LSTM(64, return_sequences=True, input_shape=input_shape, name='lstm_1'),
                Dropout(0.4),
                
                LSTM(32, return_sequences=False, name='lstm_2'),
                Dropout(0.4),
                
                Dense(16, activation='relu', name='dense_1'),
                BatchNormalization(),
                Dropout(0.3),
                
                Dense(1, activation='sigmoid', name='output')
            ])
        
        # ì»´íŒŒì¼ (metrics ë¬¸ì œ í•´ê²°)
        try:
            # TensorFlow 2.x í˜¸í™˜ì„±ì„ ìœ„í•œ metrics ì„¤ì •
            from tensorflow.keras import metrics
            
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=[
                    metrics.BinaryAccuracy(name='accuracy'),
                    metrics.Precision(name='precision'),
                    metrics.Recall(name='recall')
                ]
            )
        except Exception as e:
            # ë°±ì—… ë°©ë²•: ê¸°ë³¸ metricsë§Œ ì‚¬ìš©
            self.logger.warning(f"   âš ï¸ ê³ ê¸‰ metrics ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}")
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
        
        self.model = model
        self.logger.info(f"âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        self.logger.info(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {model.count_params():,}ê°œ")
        
        # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
        model.summary()
        
        return model
    
    def train_model(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=32):
        """ëª¨ë¸ í•™ìŠµ"""
        self.logger.info("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        classes = np.unique(y_train)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weight_dict = dict(zip(classes, class_weights))
        
        self.logger.info(f"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}")
        self.logger.info(f"   í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
        self.logger.info(f"   ì—í¬í¬: {epochs}, ë°°ì¹˜: {batch_size}")
        
        # í•™ìŠµ ì‹¤í–‰
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val) if X_val is not None else None,
            validation_split=0.2 if X_val is None else None,
            callbacks=callbacks,
            class_weight=class_weight_dict,
            verbose=1
        )
        
        self.logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        return history
    
    def evaluate_model(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        self.logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€...")
        
        try:
            # í‰ê°€ ì‹¤í–‰
            results = self.model.evaluate(X_test, y_test, verbose=0)
            
            # ê²°ê³¼ ì •ë¦¬ (metrics ì´ë¦„ í™•ì¸)
            metrics_names = self.model.metrics_names
            performance = {}
            
            for i, metric_name in enumerate(metrics_names):
                if i < len(results):
                    performance[metric_name] = float(results[i])
            
            # ë¡œê·¸ ì¶œë ¥
            for metric, value in performance.items():
                if metric == 'loss':
                    self.logger.info(f"   {metric}: {value:.4f}")
                else:
                    self.logger.info(f"   {metric}: {value:.4f} ({value*100:.1f}%)")
            
            return performance
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            
            # ìˆ˜ë™ í‰ê°€ (ë°±ì—…)
            try:
                y_pred = self.model.predict(X_test, verbose=0)
                y_pred_binary = (y_pred > 0.5).astype(int).flatten()
                
                from sklearn.metrics import accuracy_score, precision_score, recall_score
                
                accuracy = accuracy_score(y_test, y_pred_binary)
                
                # precisionê³¼ recall ê³„ì‚° (zero_division ì²˜ë¦¬)
                try:
                    precision = precision_score(y_test, y_pred_binary, zero_division=0)
                    recall = recall_score(y_test, y_pred_binary, zero_division=0)
                except:
                    precision = 0.0
                    recall = 0.0
                
                performance = {
                    'loss': 0.0,  # ê³„ì‚°í•˜ì§€ ì•ŠìŒ
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall
                }
                
                self.logger.info("   ğŸ“Š ìˆ˜ë™ í‰ê°€ ê²°ê³¼:")
                for metric, value in performance.items():
                    if metric == 'loss':
                        self.logger.info(f"   {metric}: N/A")
                    else:
                        self.logger.info(f"   {metric}: {value:.4f} ({value*100:.1f}%)")
                
                return performance
                
            except Exception as e2:
                self.logger.error(f"âŒ ìˆ˜ë™ í‰ê°€ë„ ì‹¤íŒ¨: {e2}")
                return {'loss': 0.0, 'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0}
    
    def save_model(self, model_path):
        """ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥ - JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°"""
        self.logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
        
        try:
            # 1. ëª¨ë¸ ì €ì¥ (Keras ë„¤ì´í‹°ë¸Œ í˜•ì‹ ì‚¬ìš©)
            if model_path.endswith('.h5'):
                # .keras í™•ì¥ìë¡œ ë³€ê²½ (ê¶Œì¥ í˜•ì‹)
                keras_model_path = model_path.replace('.h5', '.keras')
                self.model.save(keras_model_path)
                self.logger.info(f"   ğŸ“¦ ëª¨ë¸ ì €ì¥: {keras_model_path} (Keras ë„¤ì´í‹°ë¸Œ í˜•ì‹)")
                model_path = keras_model_path
            else:
                self.model.save(model_path)
            
            # 2. ì „ì²˜ë¦¬ê¸° ì €ì¥
            scaler_path = model_path.replace('.keras', '_scaler.pkl').replace('.h5', '_scaler.pkl')
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # 3. íŠ¹ì§• ì„ íƒê¸° ì €ì¥
            selector_path = None
            if self.feature_selector:
                selector_path = model_path.replace('.keras', '_selector.pkl').replace('.h5', '_selector.pkl')
                with open(selector_path, 'wb') as f:
                    pickle.dump(self.feature_selector, f)
            
            # 4. ë©”íƒ€ë°ì´í„° ì €ì¥ - JSON ì§ë ¬í™” ì•ˆì „ ì²˜ë¦¬
            def make_json_safe(obj):
                """ëª¨ë“  ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
                import numpy as np
                
                if obj is None:
                    return None
                elif isinstance(obj, (bool, str)):
                    return obj
                elif isinstance(obj, (np.integer, int)):
                    return int(obj)
                elif isinstance(obj, (np.floating, float)):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {str(k): make_json_safe(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [make_json_safe(item) for item in obj]
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                    return str(obj)
            
            # ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'window_size': int(self.window_size),
                'stride': int(self.stride), 
                'overlap_threshold': float(self.overlap_threshold),
                'model_architecture': 'lstm_v2',
                'training_date': datetime.now().isoformat(),
                'model_format': 'keras_native' if model_path.endswith('.keras') else 'h5'
            }
            
            # training_statsê°€ ìˆìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¶”ê°€
            if hasattr(self, 'training_stats') and self.training_stats:
                metadata['training_stats'] = make_json_safe(self.training_stats)
            
            # JSON ì €ì¥
            metadata_path = model_path.replace('.keras', '_metadata.json').replace('.h5', '_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self.logger.info("âœ… ì €ì¥ ì™„ë£Œ")
            
            # ì €ì¥ëœ íŒŒì¼ë“¤ ì •ë³´
            saved_files = {
                'model': model_path,
                'scaler': scaler_path,
                'metadata': metadata_path
            }
            
            if selector_path:
                saved_files['selector'] = selector_path
            
            # íŒŒì¼ í¬ê¸° ì •ë³´
            for file_type, file_path in saved_files.items():
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path) / 1024  # KB
                    self.logger.info(f"   ğŸ“„ {file_type}: {os.path.basename(file_path)} ({file_size:.1f} KB)")
            
            return saved_files
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # ìµœì†Œí•œì˜ ì €ì¥ ì‹œë„
            try:
                # ê¸°ë³¸ H5 í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ë§Œ ì €ì¥
                basic_model_path = model_path.replace('.keras', '.h5')
                self.model.save(basic_model_path)
                self.logger.info(f"   ğŸ“¦ ê¸°ë³¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {basic_model_path}")
                
                return {'model': basic_model_path}
                
            except Exception as e2:
                self.logger.error(f"âŒ ê¸°ë³¸ ì €ì¥ë„ ì‹¤íŒ¨: {e2}")
                return {}
    
        
    def auto_detect_files(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ìë™ íƒì§€"""
        print("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼/ë””ë ‰í† ë¦¬ ìë™ íƒì§€...")
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ íŒŒì¼ë“¤ í™•ì¸
        current_files = os.listdir('.')
        print(f"ğŸ“ í˜„ì¬ ë””ë ‰í† ë¦¬ íŒŒì¼ë“¤: {len(current_files)}ê°œ")
        
        # CSV íŒŒì¼ë“¤ ì°¾ê¸°
        csv_files = [f for f in current_files if f.endswith('.csv')]
        if csv_files:
            print(f"   ğŸ“„ ë°œê²¬ëœ CSV íŒŒì¼ë“¤:")
            for i, csv_file in enumerate(csv_files[:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                file_size = os.path.getsize(csv_file) / 1024  # KB
                print(f"      {i+1}. {csv_file} ({file_size:.1f} KB)")
            if len(csv_files) > 5:
                print(f"      ... ì™¸ {len(csv_files)-5}ê°œ")
        
        # ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        dirs = [d for d in current_files if os.path.isdir(d)]
        if dirs:
            print(f"   ğŸ“‚ ë°œê²¬ëœ ë””ë ‰í† ë¦¬ë“¤:")
            for i, dir_name in enumerate(dirs[:5]):
                csv_count = len(glob.glob(os.path.join(dir_name, "**", "*.csv"), recursive=True))
                print(f"      {i+1}. {dir_name}/ (CSV: {csv_count}ê°œ)")
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ í™•ì¸
        parent_dir = '..'
        if os.path.exists(parent_dir):
            parent_files = os.listdir(parent_dir)
            parent_dirs = [d for d in parent_files if os.path.isdir(os.path.join(parent_dir, d))]
            if parent_dirs:
                print(f"   ğŸ“‚ ìƒìœ„ ë””ë ‰í† ë¦¬ë“¤:")
                for dir_name in parent_dirs[:5]:
                    full_path = os.path.join(parent_dir, dir_name)
                    csv_count = len(glob.glob(os.path.join(full_path, "**", "*.csv"), recursive=True))
                    if csv_count > 0:
                        print(f"      {dir_name}/ (CSV: {csv_count}ê°œ)")
        
        return csv_files, dirs
    
    def find_best_test_file(self):
        """ê°€ì¥ ì í•©í•œ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°"""
        # 1. ë¯¸ë¦¬ ì •ì˜ëœ íŒŒì¼ë“¤ í™•ì¸
        for test_file in POSSIBLE_TEST_FILES:
            if os.path.exists(test_file):
                return test_file
        
        # 2. í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ë“¤ í™•ì¸
        csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]
        if csv_files:
            # ê°€ì¥ í° íŒŒì¼ ì„ íƒ (ë°ì´í„°ê°€ ë§ì„ ê°€ëŠ¥ì„±)
            csv_files_with_size = [(f, os.path.getsize(f)) for f in csv_files]
            largest_file = max(csv_files_with_size, key=lambda x: x[1])[0]
            return largest_file
        
        return None
    
    def find_best_data_dir(self):
        """ê°€ì¥ ì í•©í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        # 1. ë¯¸ë¦¬ ì •ì˜ëœ ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        for data_dir in POSSIBLE_DATA_DIRS:
            if os.path.exists(data_dir):
                csv_count = len(glob.glob(os.path.join(data_dir, "**", "*.csv"), recursive=True))
                if csv_count > 0:
                    return data_dir
        
        # 2. í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ í´ë”ë“¤ í™•ì¸
        dirs = [d for d in os.listdir('.') if os.path.isdir(d)]
        for dir_name in dirs:
            csv_count = len(glob.glob(os.path.join(dir_name, "**", "*.csv"), recursive=True))
            if csv_count > 0:
                return dir_name
        
        # 3. ìƒìœ„ ë””ë ‰í† ë¦¬ í™•ì¸
        parent_dirs = [d for d in os.listdir('..') if os.path.isdir(os.path.join('..', d))]
        for dir_name in parent_dirs:
            full_path = os.path.join('..', dir_name)
            csv_count = len(glob.glob(os.path.join(full_path, "**", "*.csv"), recursive=True))
            if csv_count > 0:
                return full_path
        
        return None

        return {
            'model': model_path,
            'scaler': scaler_path,
            'selector': selector_path if self.feature_selector else None,
            'metadata': metadata_path
        }

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def test_trainer(test_csv=None, data_dir=None):
    """í•™ìŠµ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª MultiFileCSITrainer í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # 1. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = MultiFileCSITrainer(
        window_size=DEFAULT_WINDOW_SIZE,
        stride=DEFAULT_STRIDE,
        overlap_threshold=DEFAULT_OVERLAP_THRESHOLD
    )
    
    # 2. íŒŒì¼/ë””ë ‰í† ë¦¬ ìë™ íƒì§€
    if test_csv is None and data_dir is None:
        print("ğŸ” ìë™ íƒì§€ ëª¨ë“œ...")
        trainer.auto_detect_files()
        
        # ìµœì  íŒŒì¼/ë””ë ‰í† ë¦¬ ì°¾ê¸°
        test_csv = trainer.find_best_test_file()
        data_dir = trainer.find_best_data_dir()
        
        print(f"\nğŸ¯ ìë™ íƒì§€ ê²°ê³¼:")
        print(f"   ğŸ“„ ì¶”ì²œ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_csv}")
        print(f"   ğŸ“‚ ì¶”ì²œ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if test_csv is None:
        test_csv = DEFAULT_TEST_CSV
    if data_dir is None:
        data_dir = DEFAULT_DATA_DIR
    
    print(f"\nğŸ“‹ ìµœì¢… ì„¤ì •:")
    print(f"   ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_csv}")
    print(f"   ğŸ“‚ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ìƒì„¸ í™•ì¸
    print(f"\nğŸ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸:")
    test_csv_exists = os.path.exists(test_csv)
    data_dir_exists = os.path.exists(data_dir)
    
    print(f"   ğŸ“„ {test_csv}: {'âœ… ì¡´ì¬' if test_csv_exists else 'âŒ ì—†ìŒ'}")
    if test_csv_exists:
        file_size = os.path.getsize(test_csv) / 1024
        print(f"      í¬ê¸°: {file_size:.1f} KB")
    
    print(f"   ğŸ“‚ {data_dir}: {'âœ… ì¡´ì¬' if data_dir_exists else 'âŒ ì—†ìŒ'}")
    if data_dir_exists:
        csv_count = len(glob.glob(os.path.join(data_dir, "**", "*.csv"), recursive=True))
        print(f"      CSV íŒŒì¼: {csv_count}ê°œ")
        
        # ëª‡ ê°œ íŒŒì¼ ì˜ˆì‹œ ë³´ì—¬ì£¼ê¸°
        if csv_count > 0:
            csv_files = glob.glob(os.path.join(data_dir, "**", "*.csv"), recursive=True)
            print(f"      ì˜ˆì‹œ íŒŒì¼ë“¤:")
            for i, csv_file in enumerate(csv_files[:3]):
                rel_path = os.path.relpath(csv_file, data_dir)
                file_size = os.path.getsize(csv_file) / 1024
                print(f"         {i+1}. {rel_path} ({file_size:.1f} KB)")
            if csv_count > 3:
                print(f"         ... ì™¸ {csv_count-3}ê°œ")
    
    # 3. ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
    if test_csv_exists:
        print(f"\nğŸ“„ ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸: {test_csv}")
        trainer.analyze_single_file(test_csv)
        
        try:
            X, y, file_info = trainer.load_multiple_csv_files(test_csv)
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {X.shape}")
            
            # íŠ¹ì§• ì„ íƒ
            X_selected = trainer.smart_feature_selection(X, y, target_features=DEFAULT_TARGET_FEATURES)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            X_seq, y_seq = trainer.create_sequences(X_selected, y)
            
            # ì „ì²˜ë¦¬
            X_processed = trainer.preprocess_data(X_seq)
            
            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y_seq, test_size=0.2, random_state=42
            )
            
            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
            print(f"   í›ˆë ¨: {X_train.shape}, í…ŒìŠ¤íŠ¸: {X_test.shape}")
            
            # ëª¨ë¸ êµ¬ì¶•
            input_shape = (X_train.shape[1], X_train.shape[2])
            trainer.build_model(input_shape)
            
            # ê°„ë‹¨í•œ í•™ìŠµ (í…ŒìŠ¤íŠ¸ìš©)
            print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ í•™ìŠµ ({DEFAULT_EPOCHS} ì—í¬í¬)...")
            history = trainer.train_model(X_train, y_train, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE)
            
            # í‰ê°€
            performance = trainer.evaluate_model(X_test, y_test)
            
            # ì €ì¥
            timestamp = datetime.now().strftime('%H%M%S')
            model_path = f"test_model_{timestamp}.h5"
            saved_files = trainer.save_model(model_path)
            
            print(f"\nâœ… ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"   ì„±ëŠ¥: {performance}")
            print(f"   ì €ì¥ëœ íŒŒì¼: {list(saved_files.keys())}")
            
            return trainer, saved_files
            
        except Exception as e:
            print(f"âŒ ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. ë‹¤ì¤‘ íŒŒì¼ í…ŒìŠ¤íŠ¸
    elif data_dir_exists:
        print(f"\nğŸ“‚ ë‹¤ì¤‘ íŒŒì¼ í…ŒìŠ¤íŠ¸: {data_dir}")
        try:
            csv_files = glob.glob(os.path.join(data_dir, "**", "*.csv"), recursive=True)
            if len(csv_files) == 0:
                print(f"âŒ {data_dir}ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
            
            print(f"   ë°œê²¬ëœ íŒŒì¼: {len(csv_files)}ê°œ")
            
            # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ë‹¨ì¼ ë¶„ì„
            first_file = csv_files[0]
            print(f"   ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„: {os.path.relpath(first_file, data_dir)}")
            trainer.analyze_single_file(first_file)
            
            # ë‹¤ì¤‘ íŒŒì¼ í•™ìŠµ (ìµœëŒ€ 5ê°œ íŒŒì¼ë¡œ ì œí•œ)
            print(f"\nğŸ“ ë‹¤ì¤‘ íŒŒì¼ í•™ìŠµ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ íŒŒì¼)...")
            X, y, file_info = trainer.load_multiple_csv_files(data_dir, max_files=5)
            
            print(f"âœ… ë‹¤ì¤‘ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {X.shape}")
            print(f"   ì‚¬ìš©ëœ íŒŒì¼: {len(file_info)}ê°œ")
            
            # ë‚˜ë¨¸ì§€ ê³¼ì •ì€ ë‹¨ì¼ íŒŒì¼ê³¼ ë™ì¼
            X_selected = trainer.smart_feature_selection(X, y, target_features=DEFAULT_TARGET_FEATURES)
            X_seq, y_seq = trainer.create_sequences(X_selected, y)
            X_processed = trainer.preprocess_data(X_seq)
            
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y_seq, test_size=0.2, random_state=42
            )
            
            input_shape = (X_train.shape[1], X_train.shape[2])
            trainer.build_model(input_shape)
            
            history = trainer.train_model(X_train, y_train, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE)
            performance = trainer.evaluate_model(X_test, y_test)
            
            timestamp = datetime.now().strftime('%H%M%S')
            model_path = f"multi_test_model_{timestamp}.h5"
            saved_files = trainer.save_model(model_path)
            
            print(f"\nâœ… ë‹¤ì¤‘ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"   ì„±ëŠ¥: {performance}")
            print(f"   ì €ì¥ëœ íŒŒì¼: {list(saved_files.keys())}")
            
            return trainer, saved_files
            
        except Exception as e:
            print(f"âŒ ë‹¤ì¤‘ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    else:
        print(f"\nâŒ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ì´ë‚˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print(f"\nğŸ“‹ í•´ê²° ë°©ë²•:")
        print(f"   1. CSV íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë³µì‚¬")
        print(f"   2. ë°ì´í„° í´ë”ë¥¼ ìƒì„±í•˜ê³  CSV íŒŒì¼ë“¤ ë„£ê¸°")
        print(f"   3. ì§ì ‘ ê²½ë¡œ ì§€ì •:")
        print(f"      test_trainer('ì‹¤ì œíŒŒì¼.csv')")
        print(f"      test_trainer(data_dir='ì‹¤ì œí´ë”ê²½ë¡œ')")
        
        # ë„ì›€ë§ ì •ë³´
        print(f"\nğŸ’¡ í˜„ì¬ ë””ë ‰í† ë¦¬ ì •ë³´:")
        print(f"   ê²½ë¡œ: {os.getcwd()}")
        current_files = os.listdir('.')
        csv_files = [f for f in current_files if f.endswith('.csv')]
        dirs = [d for d in current_files if os.path.isdir(d)]
        
        if csv_files:
            print(f"   CSV íŒŒì¼ë“¤: {csv_files}")
        if dirs:
            print(f"   ë””ë ‰í† ë¦¬ë“¤: {dirs}")
        
        return None, None

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê¸°ë³¸ ì„¤ì •)"""
    return test_trainer()

def custom_test(csv_file, window_size=DEFAULT_WINDOW_SIZE, target_features=DEFAULT_TARGET_FEATURES):
    """ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ”§ ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸: ìœˆë„ìš°={window_size}, íŠ¹ì§•={target_features}")
    
    # ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ
    trainer = MultiFileCSITrainer(
        window_size=window_size,
        stride=max(1, window_size // 10),  # ìœˆë„ìš° í¬ê¸°ì— ë¹„ë¡€í•œ ìŠ¤íŠ¸ë¼ì´ë“œ
        overlap_threshold=DEFAULT_OVERLAP_THRESHOLD
    )
    
    if os.path.exists(csv_file):
        trainer.analyze_single_file(csv_file)
        
        try:
            X, y, file_info = trainer.load_multiple_csv_files(csv_file)
            X_selected = trainer.smart_feature_selection(X, y, target_features=target_features)
            X_seq, y_seq = trainer.create_sequences(X_selected, y)
            X_processed = trainer.preprocess_data(X_seq)
            
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y_seq, test_size=0.2, random_state=42
            )
            
            input_shape = (X_train.shape[1], X_train.shape[2])
            trainer.build_model(input_shape)
            
            history = trainer.train_model(X_train, y_train, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE)
            performance = trainer.evaluate_model(X_test, y_test)
            
            timestamp = datetime.now().strftime('%H%M%S')
            model_path = f"custom_model_w{window_size}_f{target_features}_{timestamp}.h5"
            saved_files = trainer.save_model(model_path)
            
            print(f"\nâœ… ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"   ì„±ëŠ¥: {performance}")
            
            return trainer, saved_files
            
        except Exception as e:
            print(f"âŒ ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None, None
    else:
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
        return None, None

if __name__ == "__main__":
    import sys
    
    print("ğŸš€ CSI í•™ìŠµ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜:")
    print("   1. ê¸°ë³¸ í…ŒìŠ¤íŠ¸: python multi_file_trainer.py")
    print("   2. ì»¤ìŠ¤í…€ íŒŒì¼: python multi_file_trainer.py your_file.csv")
    print("   3. ë””ë ‰í† ë¦¬: python multi_file_trainer.py --dir ./your_data")
    print("")
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) == 1:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        print("ğŸ”„ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        test_trainer()
        
    elif len(sys.argv) == 2:
        arg = sys.argv[1]
        if arg.endswith('.csv'):
            # CSV íŒŒì¼ ì§€ì •
            print(f"ğŸ“„ ì§€ì •ëœ CSV íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸: {arg}")
            test_trainer(test_csv=arg)
        else:
            # ë””ë ‰í† ë¦¬ ì§€ì •
            print(f"ğŸ“‚ ì§€ì •ëœ ë””ë ‰í† ë¦¬ë¡œ í…ŒìŠ¤íŠ¸: {arg}")
            test_trainer(data_dir=arg)
            
    elif len(sys.argv) == 3 and sys.argv[1] == '--dir':
        # --dir ì˜µì…˜
        data_dir = sys.argv[2]
        print(f"ğŸ“‚ ë””ë ‰í† ë¦¬ í…ŒìŠ¤íŠ¸: {data_dir}")
        test_trainer(data_dir=data_dir)
        
    else:
        print("âŒ ì˜ëª»ëœ ì¸ìì…ë‹ˆë‹¤.")
        print("ğŸ’¡ ì‚¬ìš©ë²•:")
        print("   python multi_file_trainer.py")
        print("   python multi_file_trainer.py case32.csv")
        print("   python multi_file_trainer.py --dir ./csi_data")