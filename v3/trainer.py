# trainer.py
"""
CSI ë‚™ìƒ ê°ì§€ ì‹œìŠ¤í…œ ë©”ì¸ í›ˆë ¨ í´ë˜ìŠ¤
ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ê´€ë¦¬
"""

import os
import glob
import numpy as np
import pandas as pd
import pickle
import json
import logging
from datetime import datetime
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

from config import CSIConfig
from data_generator import CSIDataGenerator
from model_builder import CSIModelBuilder

class CSITrainer:
    """CSI ë‚™ìƒ ê°ì§€ ì‹œìŠ¤í…œ ë©”ì¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, data_directory=None, model_type='cnn_lstm_hybrid'):
        """
        Args:
            data_directory: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
        """
        self.data_directory = data_directory or CSIConfig.DEFAULT_DATA_DIR
        self.model_type = model_type
        
        # ì „ì²˜ë¦¬ê¸°
        self.scaler = RobustScaler()
        self.model_builder = None
        self.model = None
        
        # ì„¤ì •ê°’
        self.window_size = CSIConfig.WINDOW_SIZE
        self.stride = CSIConfig.STRIDE
        self.active_range = CSIConfig.ACTIVE_FEATURE_RANGE
        self.overlap_threshold = CSIConfig.OVERLAP_THRESHOLD
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {}
        self.data_stats = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        self.logger.info("ğŸš€ CSI íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_directory}")
        self.logger.info(f"   ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(CSIConfig.LOG_DIR, f'csi_training_{timestamp}.log')
        
        # ë¡œê±° ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸ“‹ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    def discover_csv_files(self, max_files=None):
        """CSV íŒŒì¼ ìë™ íƒì§€"""
        self.logger.info("ğŸ” CSV íŒŒì¼ íƒì§€ ì¤‘...")
        
        patterns = [
            os.path.join(self.data_directory, "*.csv"),
            os.path.join(self.data_directory, "**", "*.csv"),
            "*.csv",  # í˜„ì¬ ë””ë ‰í† ë¦¬
        ]
        
        csv_files = []
        for pattern in patterns:
            found_files = glob.glob(pattern, recursive=True)
            csv_files.extend(found_files)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        csv_files = sorted(list(set(csv_files)))
        
        if max_files:
            csv_files = csv_files[:max_files]
        
        self.logger.info(f"ğŸ“ ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
        
        # íŒŒì¼ ì •ë³´ ë¯¸ë¦¬ë³´ê¸°
        for i, file_path in enumerate(csv_files[:5]):
            try:
                file_size = os.path.getsize(file_path) / 1024  # KB
                self.logger.info(f"   {i+1}. {os.path.basename(file_path)} ({file_size:.1f} KB)")
            except:
                self.logger.info(f"   {i+1}. {os.path.basename(file_path)}")
        
        if len(csv_files) > 5:
            self.logger.info(f"   ... ì™¸ {len(csv_files)-5}ê°œ íŒŒì¼")
        
        return csv_files
    
    def analyze_data_characteristics(self, sample_files=10):
        """ë°ì´í„° íŠ¹ì„± ë¶„ì„"""
        self.logger.info("ğŸ”¬ ë°ì´í„° íŠ¹ì„± ë¶„ì„ ì‹œì‘...")
        
        csv_files = self.discover_csv_files()[:sample_files]
        
        if not csv_files:
            raise ValueError("âŒ ë¶„ì„í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        total_samples = 0
        total_fall_samples = 0
        sampling_rates = []
        active_features_counts = []
        file_durations = []
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                
                # ê¸°ë³¸ í†µê³„
                total_samples += len(df)
                fall_samples = np.sum(df['label'] == 1) if 'label' in df.columns else 0
                total_fall_samples += fall_samples
                
                # ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ ê³„ì‚°
                if 'timestamp' in df.columns:
                    try:
                        timestamps = pd.to_datetime(df['timestamp'])
                        duration = (timestamps.iloc[-1] - timestamps.iloc[0]).total_seconds()
                        if duration > 0:
                            sampling_rate = len(df) / duration
                            sampling_rates.append(sampling_rate)
                            file_durations.append(duration)
                    except:
                        pass
                
                # í™œì„± íŠ¹ì„± ìˆ˜ ê³„ì‚°
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                if feature_cols:
                    X = df[feature_cols].values
                    # ë¶„ì‚°ì´ 0ì´ ì•„ë‹Œ íŠ¹ì„± ê°œìˆ˜
                    active_count = np.sum(np.var(X, axis=0) > 1e-10)
                    active_features_counts.append(active_count)
                
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path} - {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        avg_sampling_rate = np.mean(sampling_rates) if sampling_rates else CSIConfig.SAMPLING_RATE
        avg_active_features = np.mean(active_features_counts) if active_features_counts else CSIConfig.ACTIVE_FEATURE_COUNT
        fall_ratio = total_fall_samples / total_samples if total_samples > 0 else 0
        total_duration = np.sum(file_durations) if file_durations else 0
        
        # ê²°ê³¼ ì €ì¥
        self.data_stats = {
            'total_files_analyzed': len(csv_files),
            'total_samples': total_samples,
            'total_fall_samples': total_fall_samples,
            'fall_ratio': fall_ratio,
            'avg_sampling_rate': avg_sampling_rate,
            'avg_active_features': avg_active_features,
            'total_duration_seconds': total_duration,
            'avg_file_duration': total_duration / len(file_durations) if file_durations else 0
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self.logger.info(f"ğŸ“Š ë°ì´í„° íŠ¹ì„± ë¶„ì„ ê²°ê³¼:")
        self.logger.info(f"   ë¶„ì„ íŒŒì¼: {len(csv_files)}ê°œ")
        self.logger.info(f"   ì´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
        self.logger.info(f"   ë‚™ìƒ ë¹„ìœ¨: {fall_ratio:.1%}")
        self.logger.info(f"   í‰ê·  ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜: {avg_sampling_rate:.0f}Hz")
        self.logger.info(f"   í‰ê·  í™œì„± íŠ¹ì„±: {avg_active_features:.0f}ê°œ")
        self.logger.info(f"   ì´ ì¸¡ì • ì‹œê°„: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)")
        
        # ì„¤ì • ìë™ ì¡°ì •
        if avg_sampling_rate > 200:
            suggested_window = int(0.5 * avg_sampling_rate)
            suggested_stride = int(0.05 * avg_sampling_rate)
            
            if abs(suggested_window - self.window_size) > 10:
                self.logger.info(f"ğŸ’¡ ê¶Œì¥ ìœˆë„ìš° í¬ê¸°: {suggested_window}ê°œ (í˜„ì¬: {self.window_size})")
            if abs(suggested_stride - self.stride) > 5:
                self.logger.info(f"ğŸ’¡ ê¶Œì¥ ìŠ¤íŠ¸ë¼ì´ë“œ: {suggested_stride}ê°œ (í˜„ì¬: {self.stride})")
        
        return self.data_stats
    
    def prepare_scaler(self, sample_files=15):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ì „ í•™ìŠµ"""
        self.logger.info("ğŸ”§ ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„ ì¤‘...")
        
        csv_files = self.discover_csv_files()[:sample_files]
        sample_data = []
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                feature_cols = [col for col in df.columns if col.startswith('feat_')]
                
                if not feature_cols:
                    continue
                
                X = df[feature_cols].values
                
                # í™œì„± íŠ¹ì„±ë§Œ ì„ íƒ
                start_feat, end_feat = self.active_range
                if X.shape[1] > end_feat:
                    X_active = X[:, start_feat:end_feat+1]
                else:
                    X_active = X
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìƒ˜í”Œë§
                sample_size = min(len(X_active), 2000)
                if len(X_active) > sample_size:
                    indices = np.random.choice(len(X_active), sample_size, replace=False)
                    X_sampled = X_active[indices]
                else:
                    X_sampled = X_active
                
                sample_data.append(X_sampled)
                
            except Exception as e:
                self.logger.warning(f"ìŠ¤ì¼€ì¼ëŸ¬ ìƒ˜í”Œ íŒŒì¼ ìŠ¤í‚µ: {file_path} - {e}")
                continue
        
        if not sample_data:
            raise ValueError("âŒ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        X_combined = np.vstack(sample_data)
        self.scaler.fit(X_combined)
        
        self.logger.info(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ: {X_combined.shape} ìƒ˜í”Œ")
        self.logger.info(f"   íŠ¹ì„± ë²”ìœ„: [{X_combined.min():.3f}, {X_combined.max():.3f}]")
        
        return self.scaler
    
    def create_data_generators(self, validation_split=None, test_split=None):
        """í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
        validation_split = validation_split or CSIConfig.VALIDATION_SPLIT
        test_split = test_split or CSIConfig.TEST_SPLIT
        
        self.logger.info("ğŸ“Š ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±...")
        
        csv_files = self.discover_csv_files()
        
        if len(csv_files) < 3:
            raise ValueError("âŒ ìµœì†Œ 3ê°œ ì´ìƒì˜ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
        
        # íŒŒì¼ ë ˆë²¨ì—ì„œ ë¶„í•  (ë°ì´í„° ë¦¬í¬ ë°©ì§€)
        np.random.shuffle(csv_files)
        
        test_size = max(1, int(len(csv_files) * test_split))
        val_size = max(1, int(len(csv_files) * validation_split))
        train_size = len(csv_files) - test_size - val_size
        
        if train_size < 1:
            raise ValueError("âŒ í›ˆë ¨ íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
        
        train_files = csv_files[:train_size]
        val_files = csv_files[train_size:train_size + val_size]
        test_files = csv_files[train_size + val_size:]
        
        self.logger.info(f"ğŸ“ˆ ë°ì´í„° ë¶„í• :")
        self.logger.info(f"   í›ˆë ¨ íŒŒì¼: {len(train_files)}ê°œ")
        self.logger.info(f"   ê²€ì¦ íŒŒì¼: {len(val_files)}ê°œ")
        self.logger.info(f"   í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(test_files)}ê°œ")
        
        # ì œë„ˆë ˆì´í„° ìƒì„±
        train_generator = CSIDataGenerator(
            file_list=train_files,
            batch_size=CSIConfig.BATCH_SIZE,
            window_size=self.window_size,
            stride=self.stride,
            scaler=self.scaler,
            active_range=self.active_range,
            shuffle=True,
            logger=self.logger
        )
        
        val_generator = CSIDataGenerator(
            file_list=val_files,
            batch_size=CSIConfig.BATCH_SIZE,
            window_size=self.window_size,
            stride=self.stride,
            scaler=self.scaler,
            active_range=self.active_range,
            shuffle=False,
            logger=self.logger
        )
        
        test_generator = CSIDataGenerator(
            file_list=test_files,
            batch_size=CSIConfig.BATCH_SIZE,
            window_size=self.window_size,
            stride=self.stride,
            scaler=self.scaler,
            active_range=self.active_range,
            shuffle=False,
            logger=self.logger
        ) if test_files else None
        
        # í†µê³„ ì¶œë ¥
        self.logger.info(f"ğŸ”„ ì˜ˆìƒ ì‹œí€€ìŠ¤:")
        self.logger.info(f"   í›ˆë ¨: {train_generator.total_sequences:,}ê°œ")
        self.logger.info(f"   ê²€ì¦: {val_generator.total_sequences:,}ê°œ")
        if test_generator:
            self.logger.info(f"   í…ŒìŠ¤íŠ¸: {test_generator.total_sequences:,}ê°œ")
        
        return train_generator, val_generator, test_generator
    
    def build_model(self):
        """ëª¨ë¸ êµ¬ì¶•"""
        self.logger.info(f"ğŸ—ï¸ {self.model_type} ëª¨ë¸ êµ¬ì¶•...")
        
        # ì…ë ¥ í˜•íƒœ ê³„ì‚°
        feature_count = self.active_range[1] - self.active_range[0] + 1
        input_shape = (self.window_size, feature_count)
        
        # ëª¨ë¸ ë¹Œë” ìƒì„±
        self.model_builder = CSIModelBuilder(input_shape, self.logger)
        
        # ëª¨ë¸ êµ¬ì¶•
        self.model = self.model_builder.build_model(self.model_type)
        
        return self.model
    
    def train_model(self, epochs=None):
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
        epochs = epochs or CSIConfig.EPOCHS
        
        self.logger.info("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        self.logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„° íŠ¹ì„± ë¶„ì„
            self.analyze_data_characteristics()
            
            # 2. ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„
            self.prepare_scaler()
            
            # 3. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
            train_gen, val_gen, test_gen = self.create_data_generators()
            
            # 4. ëª¨ë¸ êµ¬ì¶•
            self.build_model()
            
            # 5. ì½œë°± ì„¤ì •
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_save_path = os.path.join(
                CSIConfig.MODEL_SAVE_DIR, 
                f'best_csi_{self.model_type}_{timestamp}.keras'
            )
            
            callbacks = self.model_builder.create_callbacks(model_save_path)
            
            # 6. í•™ìŠµ ì‹¤í–‰
            self.logger.info(f"ğŸ“š í•™ìŠµ ì‹œì‘: {epochs} ì—í¬í¬")
            
            history = self.model.fit(
                train_gen,
                validation_data=val_gen,
                epochs=epochs,
                callbacks=callbacks,
                verbose=1
            )
            
            # 7. í…ŒìŠ¤íŠ¸ í‰ê°€ (ìˆëŠ” ê²½ìš°)
            test_results = None
            if test_gen:
                self.logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ í‰ê°€ ì¤‘...")
                test_results = self.model.evaluate(test_gen, verbose=0)
                test_metrics = dict(zip(self.model.metrics_names, test_results))
                
                self.logger.info("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
                for metric, value in test_metrics.items():
                    if metric != 'loss':
                        self.logger.info(f"   {metric}: {value:.1%}")
                    else:
                        self.logger.info(f"   {metric}: {value:.4f}")
            
            # 8. í›ˆë ¨ í†µê³„ ì €ì¥
            self.training_stats = {
                'model_type': self.model_type,
                'model_params': self.model.count_params(),
                'data_stats': self.data_stats,
                'train_sequences': train_gen.total_sequences,
                'val_sequences': val_gen.total_sequences,
                'test_sequences': test_gen.total_sequences if test_gen else 0,
                'epochs_trained': len(history.history['loss']),
                'best_val_loss': min(history.history['val_loss']),
                'final_train_loss': history.history['loss'][-1],
                'final_val_loss': history.history['val_loss'][-1],
                'test_results': test_metrics if test_gen else None,
                'training_time': datetime.now().isoformat()
            }
            
            # 9. ì™„ì „í•œ ì‹œìŠ¤í…œ ì €ì¥
            complete_save_path = model_save_path.replace('.keras', '_complete.keras')
            self.save_complete_system(complete_save_path)
            
            self.logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
            
            return history
            
        except Exception as e:
            self.logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def save_complete_system(self, model_path):
        """ì™„ì „í•œ ì‹œìŠ¤í…œ ì €ì¥ (ëª¨ë¸ + ì „ì²˜ë¦¬ê¸° + ë©”íƒ€ë°ì´í„°)"""
        self.logger.info(f"ğŸ’¾ ì™„ì „í•œ ì‹œìŠ¤í…œ ì €ì¥: {model_path}")
        
        try:
            # 1. ëª¨ë¸ ì €ì¥
            self.model.save(model_path)
            
            # 2. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            scaler_path = model_path.replace('.keras', '_scaler.pkl')
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'system_info': {
                    'version': '2.0',
                    'created_at': datetime.now().isoformat(),
                    'model_type': self.model_type
                },
                'model_config': {
                    'window_size': self.window_size,
                    'stride': self.stride,
                    'active_range': self.active_range,
                    'overlap_threshold': self.overlap_threshold,
                    'input_shape': self.model.input_shape[1:] if self.model else None
                },
                'training_config': CSIConfig.get_data_config(),
                'training_stats': self.training_stats,
                'data_stats': self.data_stats
            }
            
            metadata_path = model_path.replace('.keras', '_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
            
            # 4. ì €ì¥ëœ íŒŒì¼ë“¤ ì •ë³´
            saved_files = {
                'model': model_path,
                'scaler': scaler_path,
                'metadata': metadata_path
            }
            
            # íŒŒì¼ í¬ê¸° ì •ë³´
            total_size = 0
            for file_type, file_path in saved_files.items():
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                    total_size += file_size
                    self.logger.info(f"   ğŸ“„ {file_type}: {os.path.basename(file_path)} ({file_size:.1f} MB)")
            
            self.logger.info(f"   ğŸ“¦ ì´ í¬ê¸°: {total_size:.1f} MB")
            self.logger.info("âœ… ì™„ì „í•œ ì‹œìŠ¤í…œ ì €ì¥ ì™„ë£Œ")
            
            return saved_files
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return {}
    
    def quick_train(self, csv_file, epochs=10):
        """ë‹¨ì¼ íŒŒì¼ë¡œ ë¹ ë¥¸ í•™ìŠµ (í…ŒìŠ¤íŠ¸ìš©)"""
        self.logger.info(f"ğŸ§ª ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ: {csv_file}")
        
        if not os.path.exists(csv_file):
            raise FileNotFoundError(f"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(csv_file)
            feature_cols = [col for col in df.columns if col.startswith('feat_')]
            
            if not feature_cols:
                raise ValueError("íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            X = df[feature_cols].values
            y = df['label'].values if 'label' in df.columns else np.zeros(len(df))
            
            # 2. í™œì„± íŠ¹ì„±ë§Œ ì„ íƒ
            start_feat, end_feat = self.active_range
            if X.shape[1] > end_feat:
                X_active = X[:, start_feat:end_feat+1]
            else:
                X_active = X
            
            # 3. ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            self.scaler.fit(X_active)
            X_normalized = self.scaler.transform(X_active)
            
            # 4. ì‹œí€€ìŠ¤ ìƒì„±
            sequences = []
            labels = []
            
            for i in range(0, len(X_normalized) - self.window_size + 1, self.stride):
                window_X = X_normalized[i:i + self.window_size]
                window_y = y[i:i + self.window_size]
                
                # ë¼ë²¨ë§
                fall_ratio = np.mean(window_y == 1)
                sequence_label = 1 if fall_ratio >= self.overlap_threshold else 0
                
                sequences.append(window_X)
                labels.append(sequence_label)
            
            X_seq = np.array(sequences)
            y_seq = np.array(labels)
            
            self.logger.info(f"   ìƒì„±ëœ ì‹œí€€ìŠ¤: {len(X_seq)}ê°œ")
            self.logger.info(f"   ë¼ë²¨ ë¶„í¬: ì •ìƒ {np.sum(y_seq==0)}, ë‚™ìƒ {np.sum(y_seq==1)}")
            
            # 5. ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq
            )
            
            # 6. ëª¨ë¸ êµ¬ì¶•
            self.build_model()
            
            # 7. í•™ìŠµ
            self.logger.info(f"ğŸ“ ë¹ ë¥¸ í•™ìŠµ ì‹œì‘ ({epochs} ì—í¬í¬)...")
            
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=epochs,
                batch_size=min(16, len(X_train)),
                verbose=1
            )
            
            # 8. í‰ê°€
            test_results = self.model.evaluate(X_test, y_test, verbose=0)
            test_metrics = dict(zip(self.model.metrics_names, test_results))
            
            self.logger.info("ğŸ¯ ë¹ ë¥¸ í•™ìŠµ ê²°ê³¼:")
            for metric, value in test_metrics.items():
                if metric != 'loss':
                    self.logger.info(f"   {metric}: {value:.1%}")
                else:
                    self.logger.info(f"   {metric}: {value:.4f}")
            
            # 9. ì €ì¥
            timestamp = datetime.now().strftime('%H%M%S')
            quick_model_path = f"quick_model_{timestamp}.keras"
            self.save_complete_system(quick_model_path)
            
            return history, test_metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹ ë¥¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise
    
    def resume_training(self, model_path, additional_epochs=10):
        """ê¸°ì¡´ ëª¨ë¸ë¡œë¶€í„° í•™ìŠµ ì¬ê°œ"""
        self.logger.info(f"ğŸ”„ í•™ìŠµ ì¬ê°œ: {model_path}")
        
        try:
            from tensorflow.keras.models import load_model
            
            # 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            self.model = load_model(model_path)
            self.logger.info("   âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # 2. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = model_path.replace('.keras', '_scaler.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.logger.info("   âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning("   âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì—†ìŒ, ìƒˆë¡œ í•™ìŠµ")
                self.prepare_scaler()
            
            # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = model_path.replace('.keras', '_metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                model_config = metadata.get('model_config', {})
                self.window_size = model_config.get('window_size', self.window_size)
                self.stride = model_config.get('stride', self.stride)
                self.active_range = tuple(model_config.get('active_range', self.active_range))
                
                self.logger.info(f"   ğŸ“‹ ì„¤ì • ë¡œë“œ: ìœˆë„ìš°={self.window_size}, ìŠ¤íŠ¸ë¼ì´ë“œ={self.stride}")
            
            # 4. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
            train_gen, val_gen, test_gen = self.create_data_generators()
            
            # 5. ì¶”ê°€ í•™ìŠµ
            self.logger.info(f"ğŸ“ ì¶”ê°€ í•™ìŠµ: {additional_epochs} ì—í¬í¬")
            
            # ì½œë°± ì„¤ì •
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            new_model_path = model_path.replace('.keras', f'_resumed_{timestamp}.keras')
            
            from model_builder import CSIModelBuilder
            temp_builder = CSIModelBuilder(logger=self.logger)
            callbacks = temp_builder.create_callbacks(new_model_path)
            
            # í•™ìŠµ ì‹¤í–‰
            history = self.model.fit(
                train_gen,
                validation_data=val_gen,
                epochs=additional_epochs,
                callbacks=callbacks,
                verbose=1
            )
            
            # 6. ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥
            self.save_complete_system(new_model_path)
            
            self.logger.info("âœ… í•™ìŠµ ì¬ê°œ ì™„ë£Œ!")
            
            return history
            
        except Exception as e:
            self.logger.error(f"âŒ í•™ìŠµ ì¬ê°œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_training_summary(self):
        """í›ˆë ¨ ìš”ì•½ ì •ë³´"""
        if not self.training_stats:
            return "í›ˆë ¨ í†µê³„ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary = []
        summary.append("ğŸ“Š í›ˆë ¨ ìš”ì•½")
        summary.append("=" * 40)
        
        # ëª¨ë¸ ì •ë³´
        summary.append(f"ğŸ—ï¸ ëª¨ë¸ ì •ë³´:")
        summary.append(f"   íƒ€ì…: {self.training_stats.get('model_type', 'Unknown')}")
        summary.append(f"   íŒŒë¼ë¯¸í„°: {self.training_stats.get('model_params', 0):,}ê°œ")
        
        # ë°ì´í„° ì •ë³´
        summary.append(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
        data_stats = self.training_stats.get('data_stats', {})
        summary.append(f"   ì´ ìƒ˜í”Œ: {data_stats.get('total_samples', 0):,}ê°œ")
        summary.append(f"   ë‚™ìƒ ë¹„ìœ¨: {data_stats.get('fall_ratio', 0):.1%}")
        summary.append(f"   ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜: {data_stats.get('avg_sampling_rate', 0):.0f}Hz")
        
        # í•™ìŠµ ì •ë³´
        summary.append(f"\nğŸ“ í•™ìŠµ ì •ë³´:")
        summary.append(f"   í›ˆë ¨ ì‹œí€€ìŠ¤: {self.training_stats.get('train_sequences', 0):,}ê°œ")
        summary.append(f"   ê²€ì¦ ì‹œí€€ìŠ¤: {self.training_stats.get('val_sequences', 0):,}ê°œ")
        summary.append(f"   í•™ìŠµ ì—í¬í¬: {self.training_stats.get('epochs_trained', 0)}ê°œ")
        
        # ì„±ëŠ¥ ì •ë³´
        summary.append(f"\nğŸ¯ ì„±ëŠ¥ ì •ë³´:")
        summary.append(f"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {self.training_stats.get('final_train_loss', 0):.4f}")
        summary.append(f"   ìµœì¢… ê²€ì¦ ì†ì‹¤: {self.training_stats.get('final_val_loss', 0):.4f}")
        summary.append(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {self.training_stats.get('best_val_loss', 0):.4f}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼
        test_results = self.training_stats.get('test_results')
        if test_results:
            summary.append(f"\nğŸ“ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            for metric, value in test_results.items():
                if metric != 'loss':
                    summary.append(f"   {metric}: {value:.1%}")
                else:
                    summary.append(f"   {metric}: {value:.4f}")
        
        return '\n'.join(summary)
    
    def print_training_summary(self):
        """í›ˆë ¨ ìš”ì•½ ì¶œë ¥"""
        print(self.get_training_summary())

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import sys
    
    print("ğŸ§ª CSI íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    if len(sys.argv) > 1:
        # ëª…ë ¹í–‰ ì¸ìê°€ ìˆëŠ” ê²½ìš°
        command = sys.argv[1]
        
        if command == "quick" and len(sys.argv) > 2:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            csv_file = sys.argv[2]
            epochs = int(sys.argv[3]) if len(sys.argv) > 3 else 5
            
            trainer = CSITrainer(model_type='cnn_lstm_hybrid')
            history, metrics = trainer.quick_train(csv_file, epochs)
            trainer.print_training_summary()
            
        elif command == "full":
            # ì „ì²´ í•™ìŠµ
            data_dir = sys.argv[2] if len(sys.argv) > 2 else CSIConfig.DEFAULT_DATA_DIR
            epochs = int(sys.argv[3]) if len(sys.argv) > 3 else CSIConfig.EPOCHS
            
            trainer = CSITrainer(data_directory=data_dir, model_type='cnn_lstm_hybrid')
            history = trainer.train_model(epochs)
            trainer.print_training_summary()
            
        else:
            print("âŒ ì˜ëª»ëœ ëª…ë ¹ì–´")
            print("ì‚¬ìš©ë²•:")
            print("  python trainer.py quick <csv_file> [epochs]")
            print("  python trainer.py full [data_dir] [epochs]")
    
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸: 35.csv íŒŒì¼ë¡œ ë¹ ë¥¸ í•™ìŠµ
        test_file = "35.csv"
        if os.path.exists(test_file):
            print(f"ğŸ¯ ê¸°ë³¸ í…ŒìŠ¤íŠ¸: {test_file}")
            
            trainer = CSITrainer(model_type='cnn_lstm_hybrid')
            history, metrics = trainer.quick_train(test_file, epochs=3)
            trainer.print_training_summary()
            
            print("âœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        else:
            print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_file}")
            print("ğŸ’¡ ì‚¬ìš©ë²•:")
            print("  python trainer.py quick <csv_file>")
            print("  python trainer.py full [data_directory]")