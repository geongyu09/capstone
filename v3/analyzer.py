# analyzer.py
"""
ë‚™ìƒ íƒ€ì„ë¼ì¸ ë¶„ì„ê¸°
í•™ìŠµëœ ëª¨ë¸ë¡œ CSI ë°ì´í„° ë¶„ì„ ë° ë‚™ìƒ ì´ë²¤íŠ¸ ê°ì§€
"""

import os
import glob
import numpy as np
import pandas as pd
import pickle
import json
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from tensorflow.keras.models import load_model

from config import CSIConfig

class FallTimelineAnalyzer:
    """ë‚™ìƒ íƒ€ì„ë¼ì¸ ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, model_path=None, confidence_threshold=None):
        """
        Args:
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
            confidence_threshold: ë‚™ìƒ ê°ì§€ ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        self.model = None
        self.scaler = None
        self.metadata = None
        
        # ì„¤ì •ê°’
        self.confidence_threshold = confidence_threshold or CSIConfig.CONFIDENCE_THRESHOLD
        self.window_size = CSIConfig.WINDOW_SIZE
        self.stride = CSIConfig.STRIDE
        self.active_range = CSIConfig.ACTIVE_FEATURE_RANGE
        
        # ë¶„ì„ ê²°ê³¼
        self.fall_events = []
        self.timeline_data = None
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ë¡œë“œ
        if model_path or self._find_latest_model():
            self.load_model_system(model_path)
    
    def _find_latest_model(self):
        """ìµœì‹  ëª¨ë¸ ìë™ íƒì§€"""
        patterns = [
            os.path.join(CSIConfig.MODEL_SAVE_DIR, "*complete*.keras"),
            os.path.join(CSIConfig.MODEL_SAVE_DIR, "*model*.keras"),
            "*complete*.keras",
            "*model*.keras"
        ]
        
        model_files = []
        for pattern in patterns:
            model_files.extend(glob.glob(pattern))
        
        if model_files:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ë°˜í™˜
            return max(model_files, key=os.path.getctime)
        
        return None
    
    def load_model_system(self, model_path=None):
        """ì™„ì „í•œ ëª¨ë¸ ì‹œìŠ¤í…œ ë¡œë“œ"""
        if model_path is None:
            model_path = self._find_latest_model()
        
        if not model_path:
            raise ValueError("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        self.logger.info(f"ğŸ“¥ ëª¨ë¸ ì‹œìŠ¤í…œ ë¡œë”©: {os.path.basename(model_path)}")
        
        # 1. ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„± ê°œì„ )
        try:
            # ì²« ë²ˆì§¸ ì‹œë„: ê¸°ë³¸ ë¡œë“œ
            self.model = load_model(model_path)
            self.logger.info("   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e1:
            self.logger.warning(f"   âš ï¸ ê¸°ë³¸ ë¡œë“œ ì‹¤íŒ¨, í˜¸í™˜ì„± ëª¨ë“œ ì‹œë„: {e1}")
            try:
                # ë‘ ë²ˆì§¸ ì‹œë„: compile=Falseë¡œ ë¡œë“œ
                self.model = load_model(model_path, compile=False)
                
                # ìˆ˜ë™ìœ¼ë¡œ ì»´íŒŒì¼ (ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜ ë‹¤ì‹œ ì„¤ì •)
                from tensorflow.keras.optimizers import Adam
                self.model.compile(
                    optimizer=Adam(learning_rate=0.001),
                    loss='binary_crossentropy',
                    metrics=['accuracy']
                )
                self.logger.info("   âœ… í˜¸í™˜ì„± ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e2:
                self.logger.error(f"   âŒ ëª¨ë“  ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {e2}")
                return False
            
            # 2. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = model_path.replace('.keras', '_scaler.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.logger.info("   âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning("   âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì—†ìŒ")
            
            # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = model_path.replace('.keras', '_metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                
                # ì„¤ì •ê°’ ì—…ë°ì´íŠ¸
                model_config = self.metadata.get('model_config', {})
                self.window_size = model_config.get('window_size', self.window_size)
                self.stride = model_config.get('stride', self.stride)
                self.active_range = tuple(model_config.get('active_range', self.active_range))
                
                self.logger.info(f"   ğŸ“‹ ì„¤ì • ë¡œë“œ: ìœˆë„ìš°={self.window_size}, ìŠ¤íŠ¸ë¼ì´ë“œ={self.stride}")
                
                # í›ˆë ¨ í†µê³„ ì¶œë ¥
                training_stats = self.metadata.get('training_stats', {})
                if training_stats:
                    self.logger.info(f"   ğŸ“Š ëª¨ë¸ ì„±ëŠ¥: {training_stats.get('model_params', 0):,} íŒŒë¼ë¯¸í„°")
                    
                    test_results = training_stats.get('test_results')
                    if test_results and 'accuracy' in test_results:
                        self.logger.info(f"   ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_results['accuracy']:.1%}")
            
            self.logger.info("âœ… ëª¨ë¸ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def preprocess_data(self, X):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # í™œì„± íŠ¹ì„±ë§Œ ì„ íƒ
        start_feat, end_feat = self.active_range
        if X.shape[1] > end_feat:
            X_active = X[:, start_feat:end_feat+1]
        else:
            X_active = X
        
        # ì •ê·œí™”
        if self.scaler:
            X_normalized = self.scaler.transform(X_active)
        else:
            # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì •ê·œí™”
            X_normalized = (X_active - X_active.mean()) / (X_active.std() + 1e-8)
        
        return X_normalized
    
    def create_sequences_with_timestamps(self, X, timestamps):
        """íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ ì‹œí€€ìŠ¤ ìƒì„±"""
        sequences = []
        sequence_timestamps = []
        sequence_start_times = []
        sequence_end_times = []
        
        for i in range(0, len(X) - self.window_size + 1, self.stride):
            window_X = X[i:i + self.window_size]
            window_timestamps = timestamps[i:i + self.window_size]
            
            sequences.append(window_X)
            sequence_timestamps.append(window_timestamps)
            sequence_start_times.append(window_timestamps[0])
            sequence_end_times.append(window_timestamps[-1])
        
        return (np.array(sequences), sequence_timestamps, 
                sequence_start_times, sequence_end_times)
    
    def detect_fall_events(self, probabilities, start_times, end_times, 
                          fall_duration_threshold=None):
        """ë‚™ìƒ ì´ë²¤íŠ¸ ê°ì§€ ë° êµ¬ê°„ ë¶„ì„"""
        fall_duration_threshold = fall_duration_threshold or CSIConfig.FALL_DURATION_THRESHOLD
        
        self.logger.info("ğŸ” ë‚™ìƒ ì´ë²¤íŠ¸ êµ¬ê°„ ë¶„ì„...")
        
        # ì„ê³„ê°’ ì´ìƒì¸ ì‹œí€€ìŠ¤ë“¤ ì°¾ê¸°
        fall_mask = probabilities >= self.confidence_threshold
        fall_indices = np.where(fall_mask)[0]
        
        if len(fall_indices) == 0:
            self.logger.info("   âœ… ë‚™ìƒ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì—°ì†ëœ êµ¬ê°„ë“¤ë¡œ ê·¸ë£¹í™”
        fall_events = []
        current_event = None
        
        for i, idx in enumerate(fall_indices):
            if current_event is None:
                # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì‹œì‘
                current_event = {
                    'start_index': idx,
                    'end_index': idx,
                    'start_time': start_times[idx],
                    'end_time': end_times[idx],
                    'max_probability': probabilities[idx],
                    'sequence_count': 1,
                    'probabilities': [probabilities[idx]]
                }
            else:
                # ì—°ì†ì„± í™•ì¸ (5ê°œ ê°„ê²© ì´í•˜ë©´ ì—°ì†ìœ¼ë¡œ ê°„ì£¼)
                if idx - current_event['end_index'] <= 5:
                    # ê¸°ì¡´ ì´ë²¤íŠ¸ í™•ì¥
                    current_event['end_index'] = idx
                    current_event['end_time'] = end_times[idx]
                    current_event['max_probability'] = max(current_event['max_probability'], 
                                                          probabilities[idx])
                    current_event['probabilities'].append(probabilities[idx])
                    current_event['sequence_count'] += 1
                else:
                    # ì´ì „ ì´ë²¤íŠ¸ ì™„ë£Œ ë° ìƒˆ ì´ë²¤íŠ¸ ì‹œì‘
                    if current_event['sequence_count'] >= fall_duration_threshold:
                        current_event['avg_probability'] = np.mean(current_event['probabilities'])
                        current_event['duration_seconds'] = self._calculate_duration(
                            current_event['start_time'], current_event['end_time']
                        )
                        fall_events.append(current_event)
                    
                    current_event = {
                        'start_index': idx,
                        'end_index': idx,
                        'start_time': start_times[idx],
                        'end_time': end_times[idx],
                        'max_probability': probabilities[idx],
                        'sequence_count': 1,
                        'probabilities': [probabilities[idx]]
                    }
        
        # ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì²˜ë¦¬
        if current_event and current_event['sequence_count'] >= fall_duration_threshold:
            current_event['avg_probability'] = np.mean(current_event['probabilities'])
            current_event['duration_seconds'] = self._calculate_duration(
                current_event['start_time'], current_event['end_time']
            )
            fall_events.append(current_event)
        
        # ê²°ê³¼ ì •ë¦¬
        for i, event in enumerate(fall_events):
            event['event_id'] = i + 1
            
            # ì‹ ë¢°ë„ ë ˆë²¨ ì¶”ê°€
            if event['max_probability'] > 0.8:
                event['confidence_level'] = 'high'
            elif event['max_probability'] > 0.6:
                event['confidence_level'] = 'medium'
            else:
                event['confidence_level'] = 'low'
        
        self.logger.info(f"   ğŸ“Š ê°ì§€ëœ ë‚™ìƒ ì´ë²¤íŠ¸: {len(fall_events)}ê°œ")
        
        return fall_events
    
    def _calculate_duration(self, start_time, end_time):
        """ì‹œê°„ ì°¨ì´ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)"""
        try:
            if isinstance(start_time, str):
                start_dt = pd.to_datetime(start_time)
                end_dt = pd.to_datetime(end_time)
            else:
                start_dt = start_time
                end_dt = end_time
            
            duration = (end_dt - start_dt).total_seconds()
            return max(duration, 0)  # ìŒìˆ˜ ë°©ì§€
        except:
            return 0
    
    def _process_timestamps(self, df):
        """ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì²˜ë¦¬"""
        try:
            # 1. í‘œì¤€ datetime í˜•ì‹ ì‹œë„
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            return True
        except:
            pass
        
        try:
            # 2. ìˆ«ì í˜•íƒœ (ì´ˆ ë‹¨ìœ„) ì²˜ë¦¬
            timestamp_numeric = pd.to_numeric(df['timestamp'], errors='coerce')
            if not timestamp_numeric.isna().all():
                base_time = pd.Timestamp.now().normalize()
                df['timestamp'] = base_time + pd.to_timedelta(timestamp_numeric, unit='s')
                return True
        except:
            pass
        
        try:
            # 3. MM:SS.f í˜•ì‹ ì²˜ë¦¬
            def parse_mmss(time_str):
                if isinstance(time_str, str) and ':' in time_str:
                    parts = time_str.split(':')
                    if len(parts) == 2:
                        minutes = float(parts[0])
                        seconds = float(parts[1])
                        return minutes * 60 + seconds
                return None
            
            timestamp_seconds = df['timestamp'].apply(parse_mmss)
            if not timestamp_seconds.isna().all():
                base_time = pd.Timestamp.now().normalize()
                df['timestamp'] = base_time + pd.to_timedelta(timestamp_seconds, unit='s')
                return True
        except:
            pass
        
        # 4. ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒì„± (ìµœí›„ ìˆ˜ë‹¨)
        base_time = pd.Timestamp.now().normalize()
        time_intervals = pd.to_timedelta(df.index * 0.1, unit='s')
        df['timestamp'] = base_time + time_intervals
        return False
    
    def analyze_csv_timeline(self, csv_path):
        """CSV íŒŒì¼ì˜ ì „ì²´ íƒ€ì„ë¼ì¸ ë¶„ì„"""
        self.logger.info(f"ğŸ“Š íƒ€ì„ë¼ì¸ ë¶„ì„: {os.path.basename(csv_path)}")
        
        if not self.model:
            raise ValueError("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(csv_path)
            self.logger.info(f"   ğŸ“„ ë°ì´í„° í¬ê¸°: {df.shape}")
            
            # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            self.logger.info("   ğŸ” íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬...")
            timestamp_parsed = self._process_timestamps(df)
            
            if timestamp_parsed:
                self.logger.info("   âœ… íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì„±ê³µ")
            else:
                self.logger.info("   âš ï¸ ì¸ë±ìŠ¤ ê¸°ë°˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±")
            
            # ì‹œê°„ ë²”ìœ„ ì¶œë ¥
            time_range = df['timestamp'].max() - df['timestamp'].min()
            self.logger.info(f"   â° ì¸¡ì • ì‹œê°„: {time_range.total_seconds():.1f}ì´ˆ")
            
            # 3. íŠ¹ì„± ì¶”ì¶œ
            feature_cols = [col for col in df.columns if col.startswith('feat_')]
            if not feature_cols:
                raise ValueError("íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            X = df[feature_cols].values
            timestamps = df['timestamp'].values
            
            self.logger.info(f"   ğŸ“ˆ íŠ¹ì„± ì»¬ëŸ¼: {len(feature_cols)}ê°œ")
            
            # 4. ì „ì²˜ë¦¬
            X_processed = self.preprocess_data(X)
            self.logger.info(f"   ğŸ”§ ì „ì²˜ë¦¬ ì™„ë£Œ: {X_processed.shape}")
            
            # 5. ì‹œí€€ìŠ¤ ìƒì„±
            X_seq, seq_timestamps, start_times, end_times = self.create_sequences_with_timestamps(
                X_processed, timestamps
            )
            
            self.logger.info(f"   ğŸ”„ ìƒì„±ëœ ì‹œí€€ìŠ¤: {len(X_seq)}ê°œ")
            
            # 6. ì˜ˆì¸¡ ìˆ˜í–‰
            self.logger.info("   ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
            probabilities = self.model.predict(X_seq, verbose=0).flatten()

            # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ê³¼ì‹  ë³´ì •
            probabilities = self.temperature_scaling(probabilities)
            
            self.logger.info(f"   ğŸ“Š ì˜ˆì¸¡ ì™„ë£Œ - í™•ë¥  ë²”ìœ„: {probabilities.min():.3f} ~ {probabilities.max():.3f}")
            
            # 7. ë‚™ìƒ ì´ë²¤íŠ¸ ê°ì§€
            fall_events = self.detect_fall_events(probabilities, start_times, end_times)
            
            # 8. íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„±
            self.timeline_data = {
                'timestamps': start_times,
                'end_times': end_times,
                'probabilities': probabilities,
                'fall_mask': probabilities >= self.confidence_threshold,
                'original_timestamps': timestamps,
                'original_labels': df['label'].values if 'label' in df.columns else None,
                'file_info': {
                    'filename': os.path.basename(csv_path),
                    'total_samples': len(df),
                    'total_sequences': len(X_seq),
                    'measurement_duration': time_range.total_seconds(),
                    'confidence_threshold': self.confidence_threshold
                }
            }
            
            self.fall_events = fall_events
            
            # 9. ê²°ê³¼ ìš”ì•½
            self.print_fall_summary()
            
            return fall_events
            
        except Exception as e:
            self.logger.error(f"âŒ íƒ€ì„ë¼ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def print_fall_summary(self):
        """ë‚™ìƒ ì´ë²¤íŠ¸ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“‹ ë‚™ìƒ ê°ì§€ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        
        # íŒŒì¼ ì •ë³´
        if self.timeline_data and 'file_info' in self.timeline_data:
            file_info = self.timeline_data['file_info']
            print(f"ğŸ“ íŒŒì¼: {file_info['filename']}")
            print(f"ğŸ“Š ë°ì´í„°: {file_info['total_samples']:,}ê°œ ìƒ˜í”Œ â†’ {file_info['total_sequences']:,}ê°œ ì‹œí€€ìŠ¤")
            print(f"â° ì¸¡ì • ì‹œê°„: {file_info['measurement_duration']:.1f}ì´ˆ")
            print(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {file_info['confidence_threshold']:.1%}")
        
        if not self.fall_events:
            print("\nâœ… ë‚™ìƒ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if self.timeline_data:
                max_prob = np.max(self.timeline_data['probabilities'])
                avg_prob = np.mean(self.timeline_data['probabilities'])
                high_prob_count = np.sum(self.timeline_data['probabilities'] > 0.3)
                
                print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
                print(f"   ìµœëŒ€ í™•ë¥ : {max_prob:.1%}")
                print(f"   í‰ê·  í™•ë¥ : {avg_prob:.1%}")
                print(f"   30% ì´ìƒ í™•ë¥ : {high_prob_count}ê°œ ì‹œí€€ìŠ¤")
            return
        
        print(f"\nğŸš¨ ì´ {len(self.fall_events)}ê°œì˜ ë‚™ìƒ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        for event in self.fall_events:
            print(f"\nğŸ“… ë‚™ìƒ ì´ë²¤íŠ¸ #{event['event_id']}")
            print(f"   â° ì‹œì‘: {event['start_time']}")
            print(f"   â° ì¢…ë£Œ: {event['end_time']}")
            print(f"   â±ï¸  ì§€ì†: {event['duration_seconds']:.1f}ì´ˆ")
            print(f"   ğŸ“Š ìµœëŒ€ í™•ë¥ : {event['max_probability']:.1%}")
            print(f"   ğŸ“Š í‰ê·  í™•ë¥ : {event['avg_probability']:.1%}")
            print(f"   ğŸ”¢ ì‹œí€€ìŠ¤: {event['sequence_count']}ê°œ")
            
            # ì‹ ë¢°ë„ í‘œì‹œ
            confidence_icons = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
            confidence_names = {'high': 'ë†’ìŒ', 'medium': 'ì¤‘ê°„', 'low': 'ë‚®ìŒ'}
            
            confidence = event['confidence_level']
            icon = confidence_icons.get(confidence, 'âšª')
            name = confidence_names.get(confidence, 'ì•Œ ìˆ˜ ì—†ìŒ')
            
            print(f"   ğŸ¯ ì‹ ë¢°ë„: {icon} {name}")
        
        # ì „ì²´ í†µê³„
        total_fall_time = sum(event['duration_seconds'] for event in self.fall_events)
        max_prob_overall = max(event['max_probability'] for event in self.fall_events)
        avg_prob_overall = np.mean([event['avg_probability'] for event in self.fall_events])
        
        high_confidence_count = sum(1 for event in self.fall_events if event['confidence_level'] == 'high')
        medium_confidence_count = sum(1 for event in self.fall_events if event['confidence_level'] == 'medium')
        low_confidence_count = sum(1 for event in self.fall_events if event['confidence_level'] == 'low')
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   ì´ ë‚™ìƒ ì‹œê°„: {total_fall_time:.1f}ì´ˆ")
        print(f"   ìµœê³  í™•ë¥ : {max_prob_overall:.1%}")
        print(f"   í‰ê·  í™•ë¥ : {avg_prob_overall:.1%}")
        print(f"   ì‹ ë¢°ë„ ë¶„í¬: ğŸ”´{high_confidence_count}ê°œ, ğŸŸ¡{medium_confidence_count}ê°œ, ğŸŸ¢{low_confidence_count}ê°œ")
    
    def visualize_timeline(self, save_plot=True, figsize=(15, 12)):
        """íƒ€ì„ë¼ì¸ ì‹œê°í™”"""
        if not self.timeline_data:
            print("âŒ íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        print("ğŸ“Š íƒ€ì„ë¼ì¸ ì‹œê°í™” ì¤‘...")
        
        fig, axes = plt.subplots(3, 1, figsize=figsize)
        
        timestamps = pd.to_datetime(self.timeline_data['timestamps'])
        probabilities = self.timeline_data['probabilities']
        
        # 1. í™•ë¥  íƒ€ì„ë¼ì¸
        axes[0].plot(timestamps, probabilities, linewidth=1.5, color='blue', alpha=0.7, label='ë‚™ìƒ í™•ë¥ ')
        axes[0].axhline(y=self.confidence_threshold, color='red', linestyle='--', 
                       alpha=0.8, label=f'ì„ê³„ê°’ ({self.confidence_threshold:.1%})')
        
        # ë‚™ìƒ êµ¬ê°„ ê°•ì¡°
        if self.fall_events:
            for i, event in enumerate(self.fall_events):
                start_time = pd.to_datetime(event['start_time'])
                end_time = pd.to_datetime(event['end_time'])
                
                # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                color_map = {'high': 'red', 'medium': 'orange', 'low': 'yellow'}
                color = color_map.get(event['confidence_level'], 'red')
                
                axes[0].axvspan(start_time, end_time, alpha=0.3, color=color,
                               label=f'ë‚™ìƒ êµ¬ê°„ ({event["confidence_level"]})' if i == 0 else "")
        
        axes[0].set_title('ë‚™ìƒ í™•ë¥  íƒ€ì„ë¼ì¸', fontsize=14, fontweight='bold')
        axes[0].set_ylabel('ë‚™ìƒ í™•ë¥ ')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
        
        # 2. ë‚™ìƒ ê°ì§€ ìƒíƒœ
        fall_binary = (probabilities >= self.confidence_threshold).astype(int)
        axes[1].fill_between(timestamps, fall_binary, alpha=0.6, color='red', 
                            step='pre', label='ë‚™ìƒ ê°ì§€')
        
        axes[1].set_title('ë‚™ìƒ ê°ì§€ ìƒíƒœ', fontsize=14, fontweight='bold')
        axes[1].set_ylabel('ê°ì§€ ìƒíƒœ (1=ë‚™ìƒ, 0=ì •ìƒ)')
        axes[1].set_ylim(-0.1, 1.1)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
        
        # 3. ì‹¤ì œ ë¼ë²¨ ë¹„êµ ë˜ëŠ” ì´ë²¤íŠ¸ ìƒì„¸
        if self.timeline_data['original_labels'] is not None:
            original_timestamps = pd.to_datetime(self.timeline_data['original_timestamps'])
            original_labels = self.timeline_data['original_labels']
            
            axes[2].fill_between(original_timestamps, original_labels, alpha=0.6, 
                               color='green', step='pre', label='ì‹¤ì œ ë¼ë²¨')
            axes[2].fill_between(timestamps, fall_binary*0.5, alpha=0.6, 
                               color='red', step='pre', label='ì˜ˆì¸¡ ê²°ê³¼')
            
            axes[2].set_title('ì‹¤ì œ ë¼ë²¨ vs ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ', fontsize=14, fontweight='bold')
            axes[2].set_ylabel('ë¼ë²¨ ê°’')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)
            axes[2].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
        else:
            # ë‚™ìƒ ì´ë²¤íŠ¸ ìƒì„¸ ì •ë³´
            if self.fall_events:
                event_probs = [event['max_probability'] for event in self.fall_events]
                event_names = [f"#{event['event_id']}\n({event['confidence_level']})" 
                              for event in self.fall_events]
                
                # ì‹ ë¢°ë„ë³„ ìƒ‰ìƒ
                colors = []
                for event in self.fall_events:
                    if event['confidence_level'] == 'high':
                        colors.append('red')
                    elif event['confidence_level'] == 'medium':
                        colors.append('orange')
                    else:
                        colors.append('gold')
                
                bars = axes[2].bar(range(len(self.fall_events)), event_probs, 
                                  alpha=0.7, color=colors)
                
                axes[2].set_title('ë‚™ìƒ ì´ë²¤íŠ¸ë³„ ìµœëŒ€ í™•ë¥ ', fontsize=14, fontweight='bold')
                axes[2].set_ylabel('ìµœëŒ€ í™•ë¥ ')
                axes[2].set_xlabel('ì´ë²¤íŠ¸')
                axes[2].set_xticks(range(len(self.fall_events)))
                axes[2].set_xticklabels(event_names)
                
                # ê°’ í‘œì‹œ
                for bar, prob in zip(bars, event_probs):
                    height = bar.get_height()
                    axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                               f'{prob:.1%}', ha='center', va='bottom', fontsize=10)
            else:
                axes[2].text(0.5, 0.5, 'ê°ì§€ëœ ë‚™ìƒ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤', 
                           ha='center', va='center', transform=axes[2].transAxes,
                           fontsize=16, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                axes[2].set_title('ë‚™ìƒ ì´ë²¤íŠ¸ ì—†ìŒ', fontsize=14, fontweight='bold')
            
            axes[2].grid(True, alpha=0.3)
        
        # ì „ì²´ ì œëª©
        file_info = self.timeline_data.get('file_info', {})
        filename = file_info.get('filename', 'Unknown')
        fig.suptitle(f'CSI ë‚™ìƒ ê°ì§€ ë¶„ì„ ê²°ê³¼: {filename}', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        
        if save_plot:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_dir = CSIConfig.RESULTS_DIR
            filename = f'fall_timeline_analysis_{timestamp}.png'
            filepath = os.path.join(results_dir, filename)
            
            plt.savefig(filepath, dpi=CSIConfig.VISUALIZATION_DPI, bbox_inches='tight')
            print(f"   ğŸ’¾ íƒ€ì„ë¼ì¸ ì €ì¥: {filepath}")
        
        plt.show()
    
    def export_fall_events(self, output_file=None):
        """ë‚™ìƒ ì´ë²¤íŠ¸ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        if not self.fall_events:
            print("ğŸ“‹ ë‚´ë³´ë‚¼ ë‚™ìƒ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_dir = CSIConfig.RESULTS_DIR
            output_file = os.path.join(results_dir, f'fall_events_{timestamp}.json')
        
        # ë¶„ì„ ì •ë³´
        analysis_info = {
            'analysis_time': datetime.now().isoformat(),
            'model_info': {
                'confidence_threshold': self.confidence_threshold,
                'window_size': self.window_size,
                'stride': self.stride,
                'active_range': self.active_range
            },
            'file_info': self.timeline_data.get('file_info', {}) if self.timeline_data else {}
        }
        
        # ìš”ì•½ ì •ë³´
        summary = {
            'total_events': len(self.fall_events),
            'total_fall_time': sum(event['duration_seconds'] for event in self.fall_events),
            'max_probability': max(event['max_probability'] for event in self.fall_events),
            'avg_probability': np.mean([event['avg_probability'] for event in self.fall_events]),
            'confidence_distribution': {
                'high': sum(1 for e in self.fall_events if e['confidence_level'] == 'high'),
                'medium': sum(1 for e in self.fall_events if e['confidence_level'] == 'medium'),
                'low': sum(1 for e in self.fall_events if e['confidence_level'] == 'low')
            }
        }
        
        # ì´ë²¤íŠ¸ ìƒì„¸ ì •ë³´
        events_detail = []
        for event in self.fall_events:
            event_data = {
                'event_id': event['event_id'],
                'start_time': event['start_time'].isoformat() if hasattr(event['start_time'], 'isoformat') else str(event['start_time']),
                'end_time': event['end_time'].isoformat() if hasattr(event['end_time'], 'isoformat') else str(event['end_time']),
                'duration_seconds': event['duration_seconds'],
                'max_probability': float(event['max_probability']),
                'avg_probability': float(event['avg_probability']),
                'sequence_count': int(event['sequence_count']),
                'confidence_level': event['confidence_level'],
                'start_index': int(event['start_index']),
                'end_index': int(event['end_index'])
            }
            events_detail.append(event_data)
        
        # ì „ì²´ ë°ì´í„° êµ¬ì„±
        export_data = {
            'analysis_info': analysis_info,
            'summary': summary,
            'events': events_detail
        }
        
        # JSON ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ ë‚™ìƒ ì´ë²¤íŠ¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        return output_file
    
    def analyze_and_visualize(self, csv_path, save_results=True):
        """ë¶„ì„ + ì‹œê°í™” + ê²°ê³¼ ì €ì¥ í†µí•© í•¨ìˆ˜"""
        print(f"ğŸ” ì¢…í•© ë¶„ì„ ì‹œì‘: {os.path.basename(csv_path)}")
        print("=" * 60)
        
        try:
            # 1. íƒ€ì„ë¼ì¸ ë¶„ì„
            fall_events = self.analyze_csv_timeline(csv_path)
            
            # 2. ì‹œê°í™”
            if self.timeline_data:
                self.visualize_timeline(save_plot=save_results)
            
            # 3. ê²°ê³¼ ì €ì¥
            if save_results and fall_events:
                self.export_fall_events()
            
            print(f"\nğŸ‰ ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
            print(f"   ê°ì§€ëœ ì´ë²¤íŠ¸: {len(fall_events)}ê°œ")
            
            return fall_events
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def temperature_scaling(self, probabilities, temperature=7):
        """ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ê³¼ì‹ ëœ í™•ë¥  ë³´ì •"""
        
        # ê³¼ì‹  ì—¬ë¶€ ì²´í¬
        high_conf_ratio = np.mean(probabilities > 0.95)
        
        if high_conf_ratio > 0.3:  # 30% ì´ìƒì´ 95% ë„˜ìœ¼ë©´ ê³¼ì‹ 
            self.logger.warning(f"   âš ï¸ ëª¨ë¸ ê³¼ì‹  ê°ì§€: {high_conf_ratio:.1%}ê°€ 95% ì´ìƒ")
            self.logger.info(f"   ğŸŒ¡ï¸ ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ ì ìš© (T={temperature})")
            
            # í™•ë¥ ì„ ë¡œì§“ìœ¼ë¡œ ë³€í™˜
            logits = np.log(probabilities / (1 - probabilities + 1e-8))
            
            # ì˜¨ë„ë¡œ ë‚˜ëˆ„ê¸° (ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê¸°)
            scaled_logits = logits / temperature
            
            # ë‹¤ì‹œ í™•ë¥ ë¡œ ë³€í™˜
            calibrated_probs = 1 / (1 + np.exp(-scaled_logits))
            
            # ê²°ê³¼ ì¶œë ¥
            new_high_conf_ratio = np.mean(calibrated_probs > 0.95)
            self.logger.info(f"   ğŸ“Š ë³´ì • ê²°ê³¼: 95% ì´ìƒ {high_conf_ratio:.1%} â†’ {new_high_conf_ratio:.1%}")
            self.logger.info(f"   ğŸ“Š í‰ê·  í™•ë¥ : {np.mean(probabilities):.3f} â†’ {np.mean(calibrated_probs):.3f}")
            
            return calibrated_probs
        
        else:
            self.logger.info(f"   âœ… ì •ìƒì ì¸ í™•ë¥  ë¶„í¬ (99% ì´ìƒ: {high_conf_ratio:.1%})")
            return probabilities

if __name__ == "__main__":
    import sys
    
    print("ğŸ” CSI ë‚™ìƒ íƒ€ì„ë¼ì¸ ë¶„ì„ê¸°")
    print("=" * 40)
    
    if len(sys.argv) > 1:
        # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
        csv_file = sys.argv[1]
        confidence = float(sys.argv[2]) if len(sys.argv) > 2 else CSIConfig.CONFIDENCE_THRESHOLD
        model_path = sys.argv[3] if len(sys.argv) > 3 else None
        
        print(f"ğŸ“ ë¶„ì„ íŒŒì¼: {csv_file}")
        print(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {confidence:.1%}")
        
        try:
            analyzer = FallTimelineAnalyzer(model_path, confidence)
            analyzer.analyze_and_visualize(csv_file)
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        test_file = "35.csv"
        if os.path.exists(test_file):
            print(f"ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸: {test_file}")
            
            try:
                analyzer = FallTimelineAnalyzer(confidence_threshold=0.3)
                analyzer.analyze_and_visualize(test_file)
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_file}")
            print("ğŸ’¡ ì‚¬ìš©ë²•:")
            print("  python analyzer.py <csv_file> [confidence_threshold] [model_path]")