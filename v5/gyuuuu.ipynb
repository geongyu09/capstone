{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d26fe1",
   "metadata": {},
   "source": [
    "### 1. Google Drive 마운트 및 라이브러리 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard ### <--- TensorBoard 콜백 임포트\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import datetime ### <--- 로그 디렉토리 생성을 위한 datetime 임포트\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a5cf",
   "metadata": {},
   "source": [
    "### 2. 데이터 로드 및 전처리\n",
    "지정한 디렉터리 구조를 탐색하여 모든 CSV 파일을 읽고, 시계열 데이터를 모델 학습에 적합한 형태(시계열 윈도우)로 가공합니다.\n",
    "\n",
    "- SEQUENCE_LENGTH: 모델이 한 번에 입력으로 받을 시간 스텝의 길이입니다. 이 값을 조절하여 성능을 튜닝할 수 있습니다.\n",
    "- STEP: 윈도우를 얼마만큼씩 이동하며 만들지 결정합니다. 1이면 한 스텝씩 이동합니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정 (본인의 구글 드라이브 경로에 맞게 수정하세요)\n",
    "BASE_PATH = '/content/drive/MyDrive/낙상'\n",
    "\n",
    "# 시계열 윈도우 생성을 위한 하이퍼파라미터\n",
    "SEQUENCE_LENGTH = 100  # 100개의 타임스텝을 하나의 시퀀스로 사용\n",
    "STEP = 1               # 윈도우를 한 스텝씩 이동\n",
    "\n",
    "def load_and_preprocess_data(base_path):\n",
    "    \"\"\"\n",
    "    지정된 경로에서 모든 CSV 파일을 로드하고, 시계열 윈도우로 변환합니다.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(base_path, 'case*', '*', '*.csv'), recursive=True)\n",
    "    print(f\"총 {len(all_files)}개의 파일을 찾았습니다.\")\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # CSV 파일 로드 (첫 번째 행을 헤더로 사용하지 않음)\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            \n",
    "            # 컬럼명 설정: timestamp, label, feature_0, ..., feature_255\n",
    "            columns = ['timestamp', 'label'] + [f'feature_{i}' for i in range(256)]\n",
    "            df.columns = columns\n",
    "            \n",
    "            # feature 데이터와 label 분리\n",
    "            features = df.drop(['timestamp', 'label'], axis=1).values\n",
    "            label_data = df['label'].values\n",
    "\n",
    "            # 데이터 정규화 (StandardScaler 사용)\n",
    "            scaler = StandardScaler()\n",
    "            features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "            # 시계열 윈도우 생성\n",
    "            for i in range(0, len(df) - SEQUENCE_LENGTH, STEP):\n",
    "                sequences.append(features_scaled[i: i + SEQUENCE_LENGTH])\n",
    "                \n",
    "                # 윈도우 내에 낙상(1)이 한 번이라도 포함되면 해당 윈도우의 레이블을 1로 지정\n",
    "                window_labels = label_data[i: i + SEQUENCE_LENGTH]\n",
    "                if 1 in window_labels:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"파일 처리 중 오류 발생: {file_path}, 오류: {e}\")\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# 데이터 로드 및 전처리 실행\n",
    "X, y = load_and_preprocess_data(BASE_PATH)\n",
    "\n",
    "print(f\"\\n생성된 총 시퀀스 수: {X.shape[0]}\")\n",
    "print(f\"시퀀스 형태 (데이터 수, 시퀀스 길이, 특징 수): {X.shape}\")\n",
    "print(f\"레이블 형태: {y.shape}\")\n",
    "print(f\"낙상(1) 데이터 수: {np.sum(y)} / 비낙상(0) 데이터 수: {len(y) - np.sum(y)}\")\n",
    "\n",
    "\n",
    "# 학습 데이터와 검증 데이터 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\n--- 데이터 분할 결과 ---\")\n",
    "print(f\"학습 데이터 형태: {X_train.shape}\")\n",
    "print(f\"검증 데이터 형태: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacd50d",
   "metadata": {},
   "source": [
    "### 3. CNN-LSTM 모델 구축\n",
    "CNN으로 시계열 데이터의 지역적 특징을 추출하고, LSTM으로 시간적 패턴을 학습하는 결합 모델을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    CNN-LSTM 결합 모델을 생성합니다.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN 계층: 시계열의 지역적 특징 추출\n",
    "    # TimeDistributed는 각 타임스텝에 동일한 Conv1D를 적용하기 위함이지만,\n",
    "    # 여기서는 전체 시퀀스에서 특징을 뽑는 Conv1D를 먼저 적용합니다.\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # LSTM 계층: 추출된 특징의 시간적 순서 학습\n",
    "    model.add(LSTM(100, return_sequences=False)) # 마지막 출력만 사용\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # 완전 연결 계층 (분류기)\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')) # 이진 분류를 위한 시그모이드 활성화 함수\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "input_shape = (SEQUENCE_LENGTH, X_train.shape[2]) # (시퀀스 길이, 특징 수)\n",
    "model = build_cnn_lstm_model(input_shape)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 구조 출력\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f706015",
   "metadata": {},
   "source": [
    "### 4. TensorBoard 실행 및 모델 학습 (핵심 수정 부분)\n",
    "이 부분이 TensorBoard를 연동하는 핵심입니다.\n",
    "\n",
    "1. 실행할 때마다 로그를 구분하기 위해 현재 시간을 이용해 로그 디렉터리 경로를 만듭니다.\n",
    "2. Colab의 매직 커맨드 %load_ext tensorboard와 %tensorboard --logdir logs/fit을 사용해 TensorBoard를 실행합니다. 이 코드를 model.fit 보다 먼저 실행하면, 학습이 시작됨과 동시에 모니터링 UI가 나타납니다.\n",
    "3. model.fit의 callbacks 리스트에 TensorBoard 콜백을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- TensorBoard 연동 부분 --- ###\n",
    "\n",
    "# 1. TensorBoard 로그를 저장할 고유한 디렉토리 설정\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 2. TensorBoard 콜백 생성\n",
    "# histogram_freq=1은 매 에포크마다 가중치와 편향의 분포를 시각화합니다.\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 3. Colab에서 TensorBoard 확장 프로그램 로드 및 실행\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n",
    "\n",
    "### --------------------------- ###\n",
    "\n",
    "\n",
    "# 조기 종료 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# 모델 학습 (callbacks 리스트에 tensorboard_callback 추가)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, tensorboard_callback] ### <--- 수정된 부분\n",
    ")\n",
    "\n",
    "print(\"\\n모델 학습이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2513c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0fb4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fff49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
